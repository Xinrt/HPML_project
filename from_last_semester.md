# Benchmarking IBM Analog Hardware KIT performance for different networks

intend to use and contribute to the AIHWKIT with the initial simulation library for experimenting and testing and its performance Vs that of the PyTorch performance.



**对比AIHWKIT和PyTorch**





# Distributed Training of GANs

implement different training strategies for creating a GAN to build fake abstract art images

implement training strategies to help improve these bottlenecks to **improve the training speed while maintaining the quality of our GANs**





# Performance measurement of clustering applications in ML with GPU acceleration using CUDA

**Accelerate clustering algorithms on large-scale datasets.**



PyTorch-CPU

PyTorch-GPU

Scikit





# Gradient Compression with Delayed Gradient Updates



**通过Gradient Compression比较准确性和速度**



Gradient Compression

no Gradient Compression





# Improving efficiency of inference computation



**Using Apache TVM to reduce the inference time of Convolutional neural networks**







# Attacking Compressed NLP



Investigate the transferability of adversarial samples across the SOTA NLP models and their compressed versions and **infer the effects different compression techniques have** on adversarial attacks







# YOLOv5 Virus Detection

Use transfer learning on YOLOv5 pretrain model

**With limited data, we are still able to achieve good result from YOLOv5**, which can expedite the labeling process







# Distributed Self-Supervised Learning for the generation of Image Embeddings

**Experiment with Self-supervised learning using low resolution images**

High resolution: not memory friendly, increased complexity

Producing a dataset with clean labels is expensive







# Performances of Highly Scalable Deep Learning Training System with Different Precision

**figure out how different precisions improve the performance of deep learning models.**

explore three different precisions in Single-GPU and 4-GPUs Distributed Data Parallel two methods, including FP32, FP16, AMP.







# Transformer & Linformer Based Chatbot



whether Linformer, known to be very powerful for sentiment detection used by Meta (former Facebook), could be used to train and **reduce the inference time** in the case of a conversational Chatbot, where training input sequences’ lengths are varied and mostly short.



the training technique consists of a combination of Transformer and Linformer.



# 3D Generative Adversarial Networks



**Examine the problem of 3D object generation in innovating, designing and creating effectively.**



Analyse the code, its behaviour by changing optimizers,  loss functions, activations, parallel computing, and pipelining







# Reinforcement Learning: obtaining an intelligent agent in pong pixel game with QN



**Explore how batch size affect the performance of agent and whether the Linear Scaling Rule applied in RL**

改变网络结构

Add more nodes in hidden state

Add more dense Layers





# Performance Comparison of Pytorch Python and C++ API

**compare the performance of the two API and provide a quantitative comparison that can be used to make choices on when and where to use the different API’s.**





# SCALABLE VISION TRANSFORMERS WITH STRATIFIED POOLING

**improve the model capacity and hence the performance.**

propose a hierarchical pooling regime that gradually reduces the sequence length as the layer goes deeper, which significantly improves the scalability and the pyramidal feature hierarchy of Visual Transformers.





# Voice Separation and Optimization in Recurrent Neural Networks

**compare performances and find the configureations yield good performance**

Run different configurations and collect outputs



















