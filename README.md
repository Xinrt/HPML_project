# HPML_project
## reference
1. ATCON: Attention Consistency for Vision Models
link: https://github.com/alimirzazadeh/semisupervisedattention
注意力机制是指在神经网络中，模型可以选择性地将其关注点放在输入数据的某些部分，以此来提高模型的准确性和性能。

2. Batch Norm层
link: https://github.com/mr-eggplant/SAR
batch 无关的 Norm 层（Group 和 Layer Norm）一定程度上规避了 Batch Norm 局限性，更适合在动态开放场景中执行 TTA，其稳定性也更高

3. Dense QR
使用QR分解权重矩阵的主要目的是减少神经网络中的参数数量和计算复杂度，从而提高神经网络的训练效率和泛化能力
