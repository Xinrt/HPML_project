05/14/2023 12:50:33 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 12:50:33 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 12:50:37 - INFO - train -   Model resnet18 created, param count:37904976
05/14/2023 12:51:19 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 12:51:19 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 12:51:30 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 12:51:39 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 8.293 (8.29)  Time: 8.195s,   31.24/s  (8.195s,   31.24/s)  LR: 5.500e-06  Data: 1.192 (1.192)
05/14/2023 12:51:55 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 8.011 (8.15)  Time: 0.308s,  831.41/s  (0.478s,  535.77/s)  LR: 5.500e-06  Data: 0.012 (0.036)
05/14/2023 12:52:10 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 8.065 (8.12)  Time: 0.358s,  715.33/s  (0.391s,  655.01/s)  LR: 5.500e-06  Data: 0.012 (0.024)
05/14/2023 12:52:11 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 7.867 (8.06)  Time: 0.257s,  994.38/s  (0.386s,  663.15/s)  LR: 5.500e-06  Data: 0.000 (0.024)
05/14/2023 12:52:11 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:52:14 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 8.310 (8.31)  Time: 1.105s,  231.78/s  (1.105s,  231.78/s)  LR: 5.504e-03  Data: 0.813 (0.813)
05/14/2023 12:52:29 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 5.545 (6.93)  Time: 0.296s,  863.44/s  (0.318s,  804.09/s)  LR: 5.504e-03  Data: 0.015 (0.028)
05/14/2023 12:52:44 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 4.544 (6.13)  Time: 0.303s,  844.01/s  (0.311s,  823.86/s)  LR: 5.504e-03  Data: 0.013 (0.021)
05/14/2023 12:52:45 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 4.772 (5.79)  Time: 0.293s,  874.75/s  (0.310s,  826.00/s)  LR: 5.504e-03  Data: 0.000 (0.020)
05/14/2023 12:52:45 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:52:48 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 4.577 (4.58)  Time: 1.117s,  229.20/s  (1.117s,  229.20/s)  LR: 1.100e-02  Data: 0.814 (0.814)
05/14/2023 12:53:03 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.414 (4.50)  Time: 0.351s,  730.23/s  (0.322s,  795.77/s)  LR: 1.100e-02  Data: 0.011 (0.028)
05/14/2023 12:53:18 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.310 (4.43)  Time: 0.360s,  711.20/s  (0.308s,  830.70/s)  LR: 1.100e-02  Data: 0.012 (0.020)
05/14/2023 12:53:19 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.266 (4.39)  Time: 0.288s,  888.09/s  (0.308s,  830.26/s)  LR: 1.100e-02  Data: 0.000 (0.020)
05/14/2023 12:53:19 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:53:22 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 4.261 (4.26)  Time: 1.099s,  232.97/s  (1.099s,  232.97/s)  LR: 1.650e-02  Data: 0.804 (0.804)
05/14/2023 12:53:37 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 4.069 (4.16)  Time: 0.271s,  944.37/s  (0.324s,  791.33/s)  LR: 1.650e-02  Data: 0.014 (0.028)
05/14/2023 12:53:52 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 3.966 (4.10)  Time: 0.298s,  859.19/s  (0.311s,  823.87/s)  LR: 1.650e-02  Data: 0.014 (0.021)
05/14/2023 12:53:53 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 3.947 (4.06)  Time: 0.294s,  871.99/s  (0.310s,  826.78/s)  LR: 1.650e-02  Data: 0.000 (0.020)
05/14/2023 12:53:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:53:56 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 4.081 (4.08)  Time: 1.095s,  233.81/s  (1.095s,  233.81/s)  LR: 2.200e-02  Data: 0.801 (0.801)
05/14/2023 12:54:12 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 3.913 (4.00)  Time: 0.272s,  940.68/s  (0.326s,  786.41/s)  LR: 2.200e-02  Data: 0.012 (0.028)
05/14/2023 12:54:27 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 3.872 (3.96)  Time: 0.303s,  845.19/s  (0.316s,  810.11/s)  LR: 2.200e-02  Data: 0.011 (0.020)
05/14/2023 12:54:28 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 3.786 (3.91)  Time: 0.259s,  986.60/s  (0.315s,  812.61/s)  LR: 2.200e-02  Data: 0.000 (0.020)
05/14/2023 12:54:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:54:31 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 3.889 (3.89)  Time: 1.089s,  235.00/s  (1.089s,  235.00/s)  LR: 2.566e-02  Data: 0.829 (0.829)
05/14/2023 12:54:46 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 3.734 (3.81)  Time: 0.273s,  937.53/s  (0.314s,  815.51/s)  LR: 2.566e-02  Data: 0.012 (0.028)
05/14/2023 12:55:01 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 3.758 (3.79)  Time: 0.296s,  865.49/s  (0.309s,  828.07/s)  LR: 2.566e-02  Data: 0.013 (0.020)
05/14/2023 12:55:02 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 3.767 (3.79)  Time: 0.342s,  747.83/s  (0.309s,  828.54/s)  LR: 2.566e-02  Data: 0.000 (0.020)
05/14/2023 12:55:02 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:55:05 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 3.789 (3.79)  Time: 1.152s,  222.26/s  (1.152s,  222.26/s)  LR: 2.487e-02  Data: 0.844 (0.844)
05/14/2023 12:55:20 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 3.737 (3.76)  Time: 0.357s,  716.88/s  (0.310s,  824.72/s)  LR: 2.487e-02  Data: 0.012 (0.029)
05/14/2023 12:55:35 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 3.696 (3.74)  Time: 0.295s,  868.02/s  (0.309s,  827.77/s)  LR: 2.487e-02  Data: 0.012 (0.021)
05/14/2023 12:55:37 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 3.477 (3.67)  Time: 0.260s,  984.95/s  (0.310s,  826.61/s)  LR: 2.487e-02  Data: 0.000 (0.021)
05/14/2023 12:55:37 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:55:40 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 3.617 (3.62)  Time: 1.139s,  224.78/s  (1.139s,  224.78/s)  LR: 2.397e-02  Data: 0.840 (0.840)
05/14/2023 12:55:55 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 3.586 (3.60)  Time: 0.354s,  722.58/s  (0.319s,  802.20/s)  LR: 2.397e-02  Data: 0.014 (0.029)
05/14/2023 12:56:10 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 3.613 (3.61)  Time: 0.280s,  915.19/s  (0.308s,  831.95/s)  LR: 2.397e-02  Data: 0.012 (0.021)
05/14/2023 12:56:11 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 3.514 (3.58)  Time: 0.257s,  994.90/s  (0.307s,  834.30/s)  LR: 2.397e-02  Data: 0.000 (0.020)
05/14/2023 12:56:11 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:56:13 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 3.483 (3.48)  Time: 1.144s,  223.69/s  (1.144s,  223.69/s)  LR: 2.295e-02  Data: 0.850 (0.850)
05/14/2023 12:56:29 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 3.451 (3.47)  Time: 0.365s,  701.81/s  (0.323s,  791.94/s)  LR: 2.295e-02  Data: 0.012 (0.029)
05/14/2023 12:56:44 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 3.504 (3.48)  Time: 0.357s,  716.11/s  (0.313s,  817.16/s)  LR: 2.295e-02  Data: 0.012 (0.021)
05/14/2023 12:56:45 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 3.391 (3.46)  Time: 0.295s,  867.48/s  (0.313s,  817.88/s)  LR: 2.295e-02  Data: 0.000 (0.021)
05/14/2023 12:56:45 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:56:48 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 3.552 (3.55)  Time: 1.074s,  238.45/s  (1.074s,  238.45/s)  LR: 2.183e-02  Data: 0.788 (0.788)
05/14/2023 12:57:03 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 3.300 (3.43)  Time: 0.257s,  997.92/s  (0.312s,  820.59/s)  LR: 2.183e-02  Data: 0.012 (0.028)
05/14/2023 12:57:18 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 3.448 (3.43)  Time: 0.304s,  843.31/s  (0.304s,  841.82/s)  LR: 2.183e-02  Data: 0.012 (0.020)
05/14/2023 12:57:19 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 3.478 (3.44)  Time: 0.345s,  740.97/s  (0.305s,  838.69/s)  LR: 2.183e-02  Data: 0.000 (0.020)
05/14/2023 12:57:19 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:57:22 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 3.334 (3.33)  Time: 1.146s,  223.47/s  (1.146s,  223.47/s)  LR: 2.063e-02  Data: 0.846 (0.846)
05/14/2023 12:57:38 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 3.297 (3.32)  Time: 0.273s,  937.33/s  (0.326s,  785.60/s)  LR: 2.063e-02  Data: 0.012 (0.029)
05/14/2023 12:57:53 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 3.268 (3.30)  Time: 0.303s,  845.13/s  (0.315s,  813.25/s)  LR: 2.063e-02  Data: 0.012 (0.021)
05/14/2023 12:57:54 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 3.270 (3.29)  Time: 0.261s,  979.47/s  (0.314s,  814.91/s)  LR: 2.063e-02  Data: 0.000 (0.020)
05/14/2023 12:57:54 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:57:57 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 3.239 (3.24)  Time: 1.052s,  243.44/s  (1.052s,  243.44/s)  LR: 1.934e-02  Data: 0.786 (0.786)
05/14/2023 12:58:11 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 3.326 (3.28)  Time: 0.306s,  836.31/s  (0.311s,  823.54/s)  LR: 1.934e-02  Data: 0.012 (0.028)
05/14/2023 12:58:27 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 3.276 (3.28)  Time: 0.291s,  879.23/s  (0.306s,  837.29/s)  LR: 1.934e-02  Data: 0.013 (0.020)
05/14/2023 12:58:28 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 3.332 (3.29)  Time: 0.287s,  890.53/s  (0.305s,  838.85/s)  LR: 1.934e-02  Data: 0.000 (0.020)
05/14/2023 12:58:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:58:31 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 3.238 (3.24)  Time: 1.126s,  227.41/s  (1.126s,  227.41/s)  LR: 1.800e-02  Data: 0.831 (0.831)
05/14/2023 12:58:45 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 3.370 (3.30)  Time: 0.270s,  949.48/s  (0.313s,  818.13/s)  LR: 1.800e-02  Data: 0.012 (0.028)
05/14/2023 12:59:00 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 3.194 (3.27)  Time: 0.254s, 1008.31/s  (0.306s,  836.01/s)  LR: 1.800e-02  Data: 0.013 (0.021)
05/14/2023 12:59:02 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 3.195 (3.25)  Time: 0.293s,  874.04/s  (0.306s,  835.96/s)  LR: 1.800e-02  Data: 0.000 (0.020)
05/14/2023 12:59:02 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:59:05 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 3.142 (3.14)  Time: 1.189s,  215.26/s  (1.189s,  215.26/s)  LR: 1.661e-02  Data: 0.831 (0.831)
05/14/2023 12:59:20 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 3.292 (3.22)  Time: 0.369s,  693.23/s  (0.317s,  807.03/s)  LR: 1.661e-02  Data: 0.012 (0.029)
05/14/2023 12:59:35 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 3.291 (3.24)  Time: 0.356s,  719.55/s  (0.310s,  826.54/s)  LR: 1.661e-02  Data: 0.013 (0.021)
05/14/2023 12:59:36 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 3.292 (3.25)  Time: 0.291s,  878.67/s  (0.309s,  828.37/s)  LR: 1.661e-02  Data: 0.000 (0.020)
05/14/2023 12:59:36 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:59:39 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 3.280 (3.28)  Time: 1.058s,  241.89/s  (1.058s,  241.89/s)  LR: 1.519e-02  Data: 0.776 (0.776)
05/14/2023 12:59:54 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 3.176 (3.23)  Time: 0.276s,  927.28/s  (0.318s,  804.50/s)  LR: 1.519e-02  Data: 0.016 (0.028)
05/14/2023 13:00:09 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 3.295 (3.25)  Time: 0.291s,  880.32/s  (0.307s,  834.30/s)  LR: 1.519e-02  Data: 0.012 (0.020)
05/14/2023 13:00:10 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 3.383 (3.28)  Time: 0.345s,  741.25/s  (0.307s,  834.56/s)  LR: 1.519e-02  Data: 0.000 (0.020)
05/14/2023 13:00:10 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:00:13 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 3.160 (3.16)  Time: 1.193s,  214.55/s  (1.193s,  214.55/s)  LR: 1.375e-02  Data: 0.923 (0.923)
05/14/2023 13:00:28 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 3.229 (3.19)  Time: 0.314s,  814.59/s  (0.321s,  797.60/s)  LR: 1.375e-02  Data: 0.014 (0.031)
05/14/2023 13:00:43 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 3.270 (3.22)  Time: 0.276s,  927.07/s  (0.314s,  814.68/s)  LR: 1.375e-02  Data: 0.012 (0.022)
05/14/2023 13:00:44 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 3.201 (3.21)  Time: 0.294s,  871.16/s  (0.313s,  817.79/s)  LR: 1.375e-02  Data: 0.000 (0.021)
05/14/2023 13:00:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:00:47 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 3.230 (3.23)  Time: 1.167s,  219.34/s  (1.167s,  219.34/s)  LR: 1.231e-02  Data: 0.818 (0.818)
05/14/2023 13:01:02 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 3.181 (3.21)  Time: 0.267s,  957.21/s  (0.313s,  816.61/s)  LR: 1.231e-02  Data: 0.013 (0.028)
05/14/2023 13:01:17 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 3.110 (3.17)  Time: 0.278s,  921.41/s  (0.305s,  837.98/s)  LR: 1.231e-02  Data: 0.019 (0.021)
05/14/2023 13:01:18 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 3.286 (3.20)  Time: 0.280s,  913.76/s  (0.306s,  837.62/s)  LR: 1.231e-02  Data: 0.000 (0.020)
05/14/2023 13:01:18 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:01:21 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 3.212 (3.21)  Time: 1.157s,  221.32/s  (1.157s,  221.32/s)  LR: 1.089e-02  Data: 0.813 (0.813)
05/14/2023 13:01:37 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.980 (3.10)  Time: 0.288s,  888.07/s  (0.318s,  806.06/s)  LR: 1.089e-02  Data: 0.013 (0.028)
05/14/2023 13:01:51 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 3.163 (3.12)  Time: 0.258s,  992.00/s  (0.309s,  829.68/s)  LR: 1.089e-02  Data: 0.012 (0.020)
05/14/2023 13:01:53 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 3.254 (3.15)  Time: 0.253s, 1011.82/s  (0.307s,  833.21/s)  LR: 1.089e-02  Data: 0.000 (0.020)
05/14/2023 13:01:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:01:56 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 3.264 (3.26)  Time: 1.244s,  205.71/s  (1.244s,  205.71/s)  LR: 9.501e-03  Data: 0.895 (0.895)
05/14/2023 13:02:11 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 3.132 (3.20)  Time: 0.285s,  897.82/s  (0.319s,  802.55/s)  LR: 9.501e-03  Data: 0.012 (0.030)
05/14/2023 13:02:26 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 3.084 (3.16)  Time: 0.252s, 1014.69/s  (0.306s,  835.28/s)  LR: 9.501e-03  Data: 0.012 (0.021)
05/14/2023 13:02:27 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 3.280 (3.19)  Time: 0.338s,  758.27/s  (0.306s,  837.21/s)  LR: 9.501e-03  Data: 0.000 (0.021)
05/14/2023 13:02:27 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:02:30 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 3.120 (3.12)  Time: 1.160s,  220.69/s  (1.160s,  220.69/s)  LR: 8.157e-03  Data: 0.850 (0.850)
05/14/2023 13:02:45 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 3.151 (3.14)  Time: 0.299s,  857.32/s  (0.318s,  804.57/s)  LR: 8.157e-03  Data: 0.012 (0.029)
05/14/2023 13:03:00 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 3.073 (3.11)  Time: 0.296s,  864.67/s  (0.309s,  827.34/s)  LR: 8.157e-03  Data: 0.012 (0.021)
05/14/2023 13:03:01 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 3.116 (3.12)  Time: 0.247s, 1036.69/s  (0.309s,  829.77/s)  LR: 8.157e-03  Data: 0.000 (0.020)
05/14/2023 13:03:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:03:04 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 3.103 (3.10)  Time: 1.187s,  215.69/s  (1.187s,  215.69/s)  LR: 6.875e-03  Data: 0.897 (0.897)
05/14/2023 13:03:19 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 3.120 (3.11)  Time: 0.298s,  859.88/s  (0.317s,  806.53/s)  LR: 6.875e-03  Data: 0.012 (0.029)
05/14/2023 13:03:34 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 3.095 (3.11)  Time: 0.266s,  961.08/s  (0.309s,  829.20/s)  LR: 6.875e-03  Data: 0.012 (0.021)
05/14/2023 13:03:35 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 3.055 (3.09)  Time: 0.262s,  975.31/s  (0.308s,  829.87/s)  LR: 6.875e-03  Data: 0.000 (0.020)
05/14/2023 13:03:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:03:38 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 3.139 (3.14)  Time: 1.111s,  230.37/s  (1.111s,  230.37/s)  LR: 5.668e-03  Data: 0.826 (0.826)
05/14/2023 13:03:53 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 3.116 (3.13)  Time: 0.294s,  871.33/s  (0.310s,  824.85/s)  LR: 5.668e-03  Data: 0.014 (0.028)
05/14/2023 13:04:08 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 3.036 (3.10)  Time: 0.262s,  977.48/s  (0.301s,  850.13/s)  LR: 5.668e-03  Data: 0.012 (0.020)
05/14/2023 13:04:09 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 3.125 (3.10)  Time: 0.247s, 1034.58/s  (0.300s,  852.98/s)  LR: 5.668e-03  Data: 0.000 (0.020)
05/14/2023 13:04:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:04:12 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.971 (2.97)  Time: 1.067s,  239.97/s  (1.067s,  239.97/s)  LR: 4.549e-03  Data: 0.802 (0.802)
05/14/2023 13:04:27 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 3.064 (3.02)  Time: 0.293s,  873.21/s  (0.314s,  816.22/s)  LR: 4.549e-03  Data: 0.012 (0.028)
05/14/2023 13:04:41 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 3.092 (3.04)  Time: 0.298s,  857.82/s  (0.305s,  838.46/s)  LR: 4.549e-03  Data: 0.012 (0.020)
05/14/2023 13:04:43 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 3.119 (3.06)  Time: 0.289s,  884.46/s  (0.305s,  839.81/s)  LR: 4.549e-03  Data: 0.000 (0.019)
05/14/2023 13:04:43 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:04:45 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 3.166 (3.17)  Time: 1.160s,  220.77/s  (1.160s,  220.77/s)  LR: 3.532e-03  Data: 0.869 (0.869)
05/14/2023 13:05:01 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 3.094 (3.13)  Time: 0.256s,  999.10/s  (0.318s,  804.28/s)  LR: 3.532e-03  Data: 0.018 (0.030)
05/14/2023 13:05:15 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 3.068 (3.11)  Time: 0.262s,  976.50/s  (0.308s,  831.27/s)  LR: 3.532e-03  Data: 0.011 (0.021)
05/14/2023 13:05:17 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 3.074 (3.10)  Time: 0.288s,  889.14/s  (0.307s,  834.23/s)  LR: 3.532e-03  Data: 0.000 (0.021)
05/14/2023 13:05:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:05:20 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 3.176 (3.18)  Time: 1.052s,  243.26/s  (1.052s,  243.26/s)  LR: 2.626e-03  Data: 0.771 (0.771)
05/14/2023 13:05:34 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 3.030 (3.10)  Time: 0.288s,  888.44/s  (0.306s,  837.07/s)  LR: 2.626e-03  Data: 0.012 (0.027)
05/14/2023 13:05:49 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 3.116 (3.11)  Time: 0.299s,  855.00/s  (0.300s,  853.92/s)  LR: 2.626e-03  Data: 0.012 (0.020)
05/14/2023 13:05:50 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 3.015 (3.08)  Time: 0.284s,  902.44/s  (0.299s,  856.18/s)  LR: 2.626e-03  Data: 0.000 (0.019)
05/14/2023 13:05:50 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:05:53 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 3.125 (3.13)  Time: 1.125s,  227.54/s  (1.125s,  227.54/s)  LR: 1.842e-03  Data: 0.836 (0.836)
05/14/2023 13:06:08 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 2.955 (3.04)  Time: 0.300s,  852.15/s  (0.311s,  823.19/s)  LR: 1.842e-03  Data: 0.012 (0.028)
05/14/2023 13:06:22 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 3.073 (3.05)  Time: 0.309s,  827.56/s  (0.304s,  841.86/s)  LR: 1.842e-03  Data: 0.018 (0.021)
05/14/2023 13:06:23 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 3.075 (3.06)  Time: 0.249s, 1028.07/s  (0.303s,  846.20/s)  LR: 1.842e-03  Data: 0.000 (0.020)
05/14/2023 13:06:23 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:06:26 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 3.039 (3.04)  Time: 0.994s,  257.45/s  (0.994s,  257.45/s)  LR: 1.189e-03  Data: 0.768 (0.768)
05/14/2023 13:06:41 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 3.019 (3.03)  Time: 0.268s,  955.06/s  (0.313s,  816.88/s)  LR: 1.189e-03  Data: 0.013 (0.027)
05/14/2023 13:06:56 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 3.204 (3.09)  Time: 0.350s,  730.64/s  (0.306s,  837.32/s)  LR: 1.189e-03  Data: 0.011 (0.020)
05/14/2023 13:06:57 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 3.184 (3.11)  Time: 0.272s,  941.21/s  (0.305s,  838.88/s)  LR: 1.189e-03  Data: 0.000 (0.020)
05/14/2023 13:06:57 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:07:00 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 3.048 (3.05)  Time: 1.053s,  243.13/s  (1.053s,  243.13/s)  LR: 6.730e-04  Data: 0.793 (0.793)
05/14/2023 13:07:15 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 3.148 (3.10)  Time: 0.353s,  725.40/s  (0.313s,  818.88/s)  LR: 6.730e-04  Data: 0.013 (0.028)
05/14/2023 13:07:29 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 3.038 (3.08)  Time: 0.297s,  860.87/s  (0.302s,  846.31/s)  LR: 6.730e-04  Data: 0.012 (0.020)
05/14/2023 13:07:31 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 3.078 (3.08)  Time: 0.276s,  926.99/s  (0.303s,  844.63/s)  LR: 6.730e-04  Data: 0.000 (0.020)
05/14/2023 13:07:31 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:07:34 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 2.913 (2.91)  Time: 0.979s,  261.58/s  (0.979s,  261.58/s)  LR: 3.005e-04  Data: 0.721 (0.721)
05/14/2023 13:07:49 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 3.079 (3.00)  Time: 0.298s,  859.75/s  (0.314s,  814.44/s)  LR: 3.005e-04  Data: 0.012 (0.026)
05/14/2023 13:08:03 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 3.128 (3.04)  Time: 0.285s,  896.92/s  (0.304s,  840.82/s)  LR: 3.005e-04  Data: 0.012 (0.020)
05/14/2023 13:08:05 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 3.161 (3.07)  Time: 0.337s,  759.94/s  (0.304s,  840.88/s)  LR: 3.005e-04  Data: 0.000 (0.019)
05/14/2023 13:08:05 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:08:07 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 3.145 (3.15)  Time: 1.129s,  226.70/s  (1.129s,  226.70/s)  LR: 7.532e-05  Data: 0.781 (0.781)
05/14/2023 13:08:22 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 3.115 (3.13)  Time: 0.284s,  899.83/s  (0.316s,  810.20/s)  LR: 7.532e-05  Data: 0.012 (0.028)
05/14/2023 13:08:37 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 3.165 (3.14)  Time: 0.297s,  862.42/s  (0.304s,  842.65/s)  LR: 7.532e-05  Data: 0.012 (0.020)
05/14/2023 13:08:38 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 3.072 (3.12)  Time: 0.285s,  899.14/s  (0.302s,  846.32/s)  LR: 7.532e-05  Data: 0.000 (0.020)
05/14/2023 13:08:38 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:08:38 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/14/2023 13:08:41 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/14/2023 13:08:43 - INFO - train -   Test: [   0/39]  Time: 1.473 (1.473)  Loss:  0.9805 (0.9805)  Acc@1: 79.6875 (79.6875)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:46 - INFO - train -   Test: [  39/39]  Time: 0.060 (0.113)  Loss:  1.8516 (0.9917)  Acc@1: 56.2500 (79.7700)  Acc@5: 100.0000 (99.8200)
05/14/2023 13:08:46 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/14/2023 13:08:46 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/14/2023 13:08:47 - INFO - train -   Test: [   0/39]  Time: 0.676 (0.676)  Loss:  0.9014 (0.9014)  Acc@1: 84.7656 (84.7656)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:49 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/14/2023 13:08:49 - INFO - train -   Test: [  39/39]  Time: 0.286 (0.084)  Loss:  0.9751 (0.8419)  Acc@1: 81.2500 (88.2300)  Acc@5: 100.0000 (99.8900)
05/14/2023 13:08:49 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/14/2023 13:08:50 - INFO - train -   Test: [   0/39]  Time: 0.627 (0.627)  Loss:  1.0000 (1.0000)  Acc@1: 77.7344 (77.7344)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:52 - INFO - train -   Test: [  39/39]  Time: 0.012 (0.071)  Loss:  0.8926 (0.9544)  Acc@1: 81.2500 (78.7300)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:52 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/14/2023 13:08:52 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/14/2023 13:08:53 - INFO - train -   Test: [   0/39]  Time: 0.631 (0.631)  Loss:  0.8896 (0.8896)  Acc@1: 86.7188 (86.7188)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:55 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/14/2023 13:08:55 - INFO - train -   Test: [  39/39]  Time: 0.012 (0.068)  Loss:  0.7446 (0.8641)  Acc@1: 93.7500 (86.4600)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:55 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/14/2023 13:08:56 - INFO - train -   Test: [   0/39]  Time: 0.627 (0.627)  Loss:  1.0859 (1.0859)  Acc@1: 79.2969 (79.2969)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:58 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/14/2023 13:08:58 - INFO - train -   Test: [  39/39]  Time: 0.012 (0.071)  Loss:  0.8545 (1.0065)  Acc@1: 81.2500 (81.5900)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:08:58 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/14/2023 13:08:59 - INFO - train -   Test: [   0/39]  Time: 0.615 (0.615)  Loss:  1.1094 (1.1094)  Acc@1: 79.6875 (79.6875)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:01 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/14/2023 13:09:01 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.065)  Loss:  0.9116 (1.0478)  Acc@1: 87.5000 (82.2600)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:01 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/14/2023 13:09:02 - INFO - train -   Test: [   0/39]  Time: 0.659 (0.659)  Loss:  0.9932 (0.9932)  Acc@1: 80.4688 (80.4688)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:04 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/14/2023 13:09:04 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.068)  Loss:  0.7637 (0.9455)  Acc@1: 93.7500 (83.6800)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:04 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/14/2023 13:09:05 - INFO - train -   Test: [   0/39]  Time: 0.629 (0.629)  Loss:  0.9229 (0.9229)  Acc@1: 86.3281 (86.3281)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:07 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/14/2023 13:09:07 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.066)  Loss:  0.9102 (0.8764)  Acc@1: 87.5000 (86.0100)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:07 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/14/2023 13:09:08 - INFO - train -   Test: [   0/39]  Time: 0.618 (0.618)  Loss:  1.0000 (1.0000)  Acc@1: 88.2812 (88.2812)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:10 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/14/2023 13:09:10 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.064)  Loss:  1.1191 (0.9685)  Acc@1: 68.7500 (86.2400)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:10 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/14/2023 13:09:10 - INFO - train -   Test: [   0/39]  Time: 0.612 (0.612)  Loss:  0.8350 (0.8350)  Acc@1: 93.3594 (93.3594)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:09:12 - INFO - train -   Test: [  39/39]  Time: 0.009 (0.065)  Loss:  0.7744 (0.8057)  Acc@1: 87.5000 (92.5500)  Acc@5: 100.0000 (100.0000)
