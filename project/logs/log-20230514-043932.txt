05/14/2023 04:39:32 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 04:39:32 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 04:39:38 - INFO - train -   Model resnet18 created, param count:112977488
05/14/2023 04:41:24 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 04:41:24 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 04:41:34 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 04:41:41 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 6.700 (6.70)  Time: 7.801s,   32.82/s  (7.801s,   32.82/s)  LR: 5.500e-06  Data: 1.687 (1.687)
05/14/2023 04:42:07 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 6.673 (6.69)  Time: 0.504s,  508.42/s  (0.661s,  387.39/s)  LR: 5.500e-06  Data: 0.012 (0.044)
05/14/2023 04:42:33 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 6.684 (6.69)  Time: 0.501s,  511.42/s  (0.585s,  437.71/s)  LR: 5.500e-06  Data: 0.011 (0.028)
05/14/2023 04:42:35 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.719 (6.69)  Time: 0.504s,  507.85/s  (0.582s,  440.19/s)  LR: 5.500e-06  Data: 0.000 (0.027)
05/14/2023 04:42:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:42:41 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.577 (6.58)  Time: 1.139s,  224.84/s  (1.139s,  224.84/s)  LR: 5.504e-03  Data: 0.631 (0.631)
05/14/2023 04:43:06 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 6.162 (6.37)  Time: 0.496s,  516.39/s  (0.519s,  493.71/s)  LR: 5.504e-03  Data: 0.011 (0.023)
05/14/2023 04:43:31 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 5.638 (6.13)  Time: 0.498s,  514.38/s  (0.512s,  499.59/s)  LR: 5.504e-03  Data: 0.010 (0.017)
05/14/2023 04:43:33 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 5.695 (6.02)  Time: 0.506s,  505.74/s  (0.512s,  499.97/s)  LR: 5.504e-03  Data: 0.000 (0.017)
05/14/2023 04:43:33 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:43:40 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 5.745 (5.75)  Time: 1.067s,  240.00/s  (1.067s,  240.00/s)  LR: 1.100e-02  Data: 0.547 (0.547)
05/14/2023 04:44:05 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.618 (5.18)  Time: 0.507s,  505.08/s  (0.515s,  497.13/s)  LR: 1.100e-02  Data: 0.011 (0.022)
05/14/2023 04:44:30 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.174 (4.85)  Time: 0.508s,  504.26/s  (0.509s,  502.53/s)  LR: 1.100e-02  Data: 0.011 (0.016)
05/14/2023 04:44:32 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.245 (4.70)  Time: 0.493s,  519.72/s  (0.509s,  502.89/s)  LR: 1.100e-02  Data: 0.000 (0.016)
05/14/2023 04:44:32 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:44:38 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 3.980 (3.98)  Time: 1.095s,  233.85/s  (1.095s,  233.85/s)  LR: 1.650e-02  Data: 0.561 (0.561)
05/14/2023 04:45:03 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 3.855 (3.92)  Time: 0.505s,  506.52/s  (0.516s,  496.17/s)  LR: 1.650e-02  Data: 0.010 (0.022)
05/14/2023 04:45:28 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 3.612 (3.82)  Time: 0.498s,  514.52/s  (0.510s,  501.50/s)  LR: 1.650e-02  Data: 0.011 (0.017)
05/14/2023 04:45:30 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 3.952 (3.85)  Time: 0.499s,  513.01/s  (0.510s,  501.86/s)  LR: 1.650e-02  Data: 0.000 (0.016)
05/14/2023 04:45:30 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:45:37 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 3.708 (3.71)  Time: 1.153s,  222.01/s  (1.153s,  222.01/s)  LR: 2.200e-02  Data: 0.594 (0.594)
05/14/2023 04:46:02 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 3.474 (3.59)  Time: 0.492s,  520.25/s  (0.516s,  496.00/s)  LR: 2.200e-02  Data: 0.010 (0.022)
05/14/2023 04:46:27 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 5.169 (4.12)  Time: 0.503s,  509.28/s  (0.511s,  500.96/s)  LR: 2.200e-02  Data: 0.011 (0.017)
05/14/2023 04:46:29 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 3.736 (4.02)  Time: 0.487s,  525.61/s  (0.510s,  501.56/s)  LR: 2.200e-02  Data: 0.000 (0.017)
05/14/2023 04:46:29 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:46:36 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 3.708 (3.71)  Time: 1.074s,  238.30/s  (1.074s,  238.30/s)  LR: 2.566e-02  Data: 0.555 (0.555)
05/14/2023 04:47:01 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 3.590 (3.65)  Time: 0.496s,  515.99/s  (0.514s,  498.40/s)  LR: 2.566e-02  Data: 0.010 (0.022)
05/14/2023 04:47:26 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 3.459 (3.59)  Time: 0.492s,  519.89/s  (0.509s,  502.64/s)  LR: 2.566e-02  Data: 0.011 (0.016)
05/14/2023 04:47:28 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 3.719 (3.62)  Time: 0.496s,  515.92/s  (0.509s,  503.05/s)  LR: 2.566e-02  Data: 0.000 (0.016)
05/14/2023 04:47:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:47:35 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 3.695 (3.70)  Time: 1.034s,  247.63/s  (1.034s,  247.63/s)  LR: 2.487e-02  Data: 0.515 (0.515)
05/14/2023 04:48:00 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 3.656 (3.68)  Time: 0.515s,  496.91/s  (0.515s,  496.91/s)  LR: 2.487e-02  Data: 0.010 (0.021)
05/14/2023 04:48:25 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 3.497 (3.62)  Time: 0.491s,  521.49/s  (0.510s,  502.38/s)  LR: 2.487e-02  Data: 0.011 (0.016)
05/14/2023 04:48:27 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 3.352 (3.55)  Time: 0.481s,  532.45/s  (0.509s,  502.99/s)  LR: 2.487e-02  Data: 0.000 (0.016)
05/14/2023 04:48:27 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:48:34 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 3.300 (3.30)  Time: 1.126s,  227.45/s  (1.126s,  227.45/s)  LR: 2.397e-02  Data: 0.614 (0.614)
05/14/2023 04:48:59 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 3.238 (3.27)  Time: 0.503s,  509.45/s  (0.516s,  496.16/s)  LR: 2.397e-02  Data: 0.011 (0.023)
05/14/2023 04:49:24 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 3.367 (3.30)  Time: 0.493s,  519.38/s  (0.509s,  503.41/s)  LR: 2.397e-02  Data: 0.011 (0.017)
05/14/2023 04:49:26 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 3.298 (3.30)  Time: 0.481s,  532.18/s  (0.508s,  504.04/s)  LR: 2.397e-02  Data: 0.000 (0.017)
05/14/2023 04:49:26 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:49:32 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 3.404 (3.40)  Time: 1.072s,  238.75/s  (1.072s,  238.75/s)  LR: 2.295e-02  Data: 0.567 (0.567)
05/14/2023 04:49:58 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 3.082 (3.24)  Time: 0.503s,  509.26/s  (0.514s,  497.86/s)  LR: 2.295e-02  Data: 0.012 (0.022)
05/14/2023 04:50:23 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 3.088 (3.19)  Time: 0.503s,  509.29/s  (0.509s,  502.80/s)  LR: 2.295e-02  Data: 0.011 (0.017)
05/14/2023 04:50:25 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 3.248 (3.21)  Time: 0.500s,  512.10/s  (0.509s,  503.12/s)  LR: 2.295e-02  Data: 0.000 (0.017)
05/14/2023 04:50:25 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:50:32 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 3.419 (3.42)  Time: 1.057s,  242.16/s  (1.057s,  242.16/s)  LR: 2.183e-02  Data: 0.547 (0.547)
05/14/2023 04:50:57 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 3.056 (3.24)  Time: 0.504s,  507.52/s  (0.514s,  497.90/s)  LR: 2.183e-02  Data: 0.012 (0.022)
05/14/2023 04:51:22 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 3.104 (3.19)  Time: 0.499s,  512.66/s  (0.509s,  502.70/s)  LR: 2.183e-02  Data: 0.011 (0.017)
05/14/2023 04:51:24 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 3.022 (3.15)  Time: 0.489s,  523.39/s  (0.509s,  503.04/s)  LR: 2.183e-02  Data: 0.000 (0.016)
05/14/2023 04:51:24 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:51:30 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 2.948 (2.95)  Time: 1.086s,  235.76/s  (1.086s,  235.76/s)  LR: 2.063e-02  Data: 0.588 (0.588)
05/14/2023 04:51:55 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 3.073 (3.01)  Time: 0.524s,  488.91/s  (0.517s,  495.17/s)  LR: 2.063e-02  Data: 0.011 (0.023)
05/14/2023 04:52:20 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 3.199 (3.07)  Time: 0.504s,  508.27/s  (0.510s,  501.52/s)  LR: 2.063e-02  Data: 0.011 (0.017)
05/14/2023 04:52:22 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 3.741 (3.24)  Time: 0.489s,  523.18/s  (0.510s,  501.98/s)  LR: 2.063e-02  Data: 0.000 (0.017)
05/14/2023 04:52:22 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:52:29 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 3.049 (3.05)  Time: 1.050s,  243.71/s  (1.050s,  243.71/s)  LR: 1.934e-02  Data: 0.531 (0.531)
05/14/2023 04:52:54 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 2.966 (3.01)  Time: 0.499s,  512.99/s  (0.515s,  497.06/s)  LR: 1.934e-02  Data: 0.011 (0.022)
05/14/2023 04:53:19 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 3.462 (3.16)  Time: 0.504s,  507.83/s  (0.509s,  503.43/s)  LR: 1.934e-02  Data: 0.011 (0.017)
05/14/2023 04:53:21 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 3.498 (3.24)  Time: 0.483s,  529.57/s  (0.508s,  503.80/s)  LR: 1.934e-02  Data: 0.000 (0.016)
05/14/2023 04:53:21 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:53:27 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 3.198 (3.20)  Time: 1.098s,  233.20/s  (1.098s,  233.20/s)  LR: 1.800e-02  Data: 0.580 (0.580)
05/14/2023 04:53:53 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 2.981 (3.09)  Time: 0.498s,  513.79/s  (0.515s,  497.18/s)  LR: 1.800e-02  Data: 0.011 (0.023)
05/14/2023 04:54:18 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 2.939 (3.04)  Time: 0.497s,  514.83/s  (0.509s,  502.59/s)  LR: 1.800e-02  Data: 0.012 (0.017)
05/14/2023 04:54:20 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 2.962 (3.02)  Time: 0.500s,  512.38/s  (0.509s,  503.00/s)  LR: 1.800e-02  Data: 0.000 (0.017)
05/14/2023 04:54:20 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:54:26 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 2.926 (2.93)  Time: 1.174s,  218.01/s  (1.174s,  218.01/s)  LR: 1.661e-02  Data: 0.656 (0.656)
05/14/2023 04:54:51 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 2.773 (2.85)  Time: 0.512s,  500.36/s  (0.516s,  496.59/s)  LR: 1.661e-02  Data: 0.011 (0.024)
05/14/2023 04:55:16 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 2.827 (2.84)  Time: 0.498s,  513.86/s  (0.510s,  502.37/s)  LR: 1.661e-02  Data: 0.011 (0.018)
05/14/2023 04:55:18 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 2.923 (2.86)  Time: 0.480s,  533.19/s  (0.509s,  502.80/s)  LR: 1.661e-02  Data: 0.000 (0.017)
05/14/2023 04:55:18 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:55:24 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 2.857 (2.86)  Time: 1.077s,  237.78/s  (1.077s,  237.78/s)  LR: 1.519e-02  Data: 0.569 (0.569)
05/14/2023 04:55:49 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 2.868 (2.86)  Time: 0.499s,  512.52/s  (0.515s,  497.47/s)  LR: 1.519e-02  Data: 0.011 (0.022)
05/14/2023 04:56:14 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 2.943 (2.89)  Time: 0.513s,  499.28/s  (0.509s,  502.46/s)  LR: 1.519e-02  Data: 0.012 (0.017)
05/14/2023 04:56:16 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 2.843 (2.88)  Time: 0.485s,  528.29/s  (0.509s,  502.66/s)  LR: 1.519e-02  Data: 0.000 (0.017)
05/14/2023 04:56:16 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:56:23 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 3.230 (3.23)  Time: 1.170s,  218.77/s  (1.170s,  218.77/s)  LR: 1.375e-02  Data: 0.651 (0.651)
05/14/2023 04:56:48 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 2.721 (2.98)  Time: 0.495s,  517.62/s  (0.520s,  492.61/s)  LR: 1.375e-02  Data: 0.012 (0.024)
05/14/2023 04:57:14 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 2.967 (2.97)  Time: 0.506s,  506.06/s  (0.512s,  500.19/s)  LR: 1.375e-02  Data: 0.011 (0.018)
05/14/2023 04:57:16 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 2.836 (2.94)  Time: 0.483s,  529.98/s  (0.511s,  500.80/s)  LR: 1.375e-02  Data: 0.000 (0.018)
05/14/2023 04:57:16 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:57:21 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 2.784 (2.78)  Time: 1.052s,  243.45/s  (1.052s,  243.45/s)  LR: 1.231e-02  Data: 0.532 (0.532)
05/14/2023 04:57:47 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 2.964 (2.87)  Time: 0.506s,  506.27/s  (0.513s,  498.67/s)  LR: 1.231e-02  Data: 0.011 (0.022)
05/14/2023 04:58:12 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 2.936 (2.89)  Time: 0.497s,  515.31/s  (0.508s,  503.47/s)  LR: 1.231e-02  Data: 0.010 (0.017)
05/14/2023 04:58:14 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 3.301 (3.00)  Time: 0.485s,  527.52/s  (0.508s,  503.78/s)  LR: 1.231e-02  Data: 0.000 (0.016)
05/14/2023 04:58:14 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:58:19 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 2.980 (2.98)  Time: 1.076s,  237.97/s  (1.076s,  237.97/s)  LR: 1.089e-02  Data: 0.566 (0.566)
05/14/2023 04:58:44 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.769 (2.87)  Time: 0.498s,  514.37/s  (0.512s,  499.62/s)  LR: 1.089e-02  Data: 0.012 (0.022)
05/14/2023 04:59:10 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 3.182 (2.98)  Time: 0.500s,  512.11/s  (0.507s,  504.81/s)  LR: 1.089e-02  Data: 0.011 (0.017)
05/14/2023 04:59:12 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 2.990 (2.98)  Time: 0.482s,  530.80/s  (0.507s,  505.31/s)  LR: 1.089e-02  Data: 0.000 (0.017)
05/14/2023 04:59:12 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:59:17 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 2.903 (2.90)  Time: 1.133s,  226.01/s  (1.133s,  226.01/s)  LR: 9.501e-03  Data: 0.606 (0.606)
05/14/2023 04:59:43 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 2.919 (2.91)  Time: 0.505s,  507.28/s  (0.515s,  496.92/s)  LR: 9.501e-03  Data: 0.011 (0.023)
05/14/2023 05:00:08 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 2.708 (2.84)  Time: 0.491s,  521.16/s  (0.510s,  501.98/s)  LR: 9.501e-03  Data: 0.011 (0.017)
05/14/2023 05:00:10 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 2.712 (2.81)  Time: 0.495s,  517.25/s  (0.510s,  502.35/s)  LR: 9.501e-03  Data: 0.000 (0.017)
05/14/2023 05:00:10 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:00:16 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 2.742 (2.74)  Time: 1.151s,  222.49/s  (1.151s,  222.49/s)  LR: 8.157e-03  Data: 0.637 (0.637)
05/14/2023 05:00:41 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 2.803 (2.77)  Time: 0.521s,  491.50/s  (0.517s,  495.39/s)  LR: 8.157e-03  Data: 0.011 (0.024)
05/14/2023 05:01:07 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 2.718 (2.75)  Time: 0.493s,  519.08/s  (0.510s,  502.38/s)  LR: 8.157e-03  Data: 0.011 (0.018)
05/14/2023 05:01:09 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 2.738 (2.75)  Time: 0.487s,  526.07/s  (0.509s,  502.93/s)  LR: 8.157e-03  Data: 0.000 (0.017)
05/14/2023 05:01:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:01:14 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 2.677 (2.68)  Time: 1.129s,  226.78/s  (1.129s,  226.78/s)  LR: 6.875e-03  Data: 0.619 (0.619)
05/14/2023 05:01:40 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 2.748 (2.71)  Time: 0.501s,  511.05/s  (0.514s,  498.23/s)  LR: 6.875e-03  Data: 0.012 (0.024)
05/14/2023 05:02:05 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 2.856 (2.76)  Time: 0.519s,  492.87/s  (0.508s,  503.49/s)  LR: 6.875e-03  Data: 0.011 (0.018)
05/14/2023 05:02:07 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 2.763 (2.76)  Time: 0.487s,  525.66/s  (0.508s,  503.76/s)  LR: 6.875e-03  Data: 0.000 (0.017)
05/14/2023 05:02:07 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:02:13 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 2.782 (2.78)  Time: 1.068s,  239.62/s  (1.068s,  239.62/s)  LR: 5.668e-03  Data: 0.558 (0.558)
05/14/2023 05:02:38 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 3.032 (2.91)  Time: 0.502s,  510.12/s  (0.512s,  500.02/s)  LR: 5.668e-03  Data: 0.011 (0.022)
05/14/2023 05:03:03 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 3.209 (3.01)  Time: 0.501s,  510.55/s  (0.507s,  505.36/s)  LR: 5.668e-03  Data: 0.011 (0.017)
05/14/2023 05:03:05 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 3.123 (3.04)  Time: 0.485s,  527.61/s  (0.506s,  505.50/s)  LR: 5.668e-03  Data: 0.000 (0.016)
05/14/2023 05:03:05 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:03:11 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.643 (2.64)  Time: 1.102s,  232.39/s  (1.102s,  232.39/s)  LR: 4.549e-03  Data: 0.595 (0.595)
05/14/2023 05:03:36 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 2.701 (2.67)  Time: 0.500s,  511.66/s  (0.515s,  497.04/s)  LR: 4.549e-03  Data: 0.011 (0.023)
05/14/2023 05:04:01 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 2.737 (2.69)  Time: 0.496s,  516.07/s  (0.509s,  503.05/s)  LR: 4.549e-03  Data: 0.011 (0.017)
05/14/2023 05:04:03 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 3.257 (2.83)  Time: 0.490s,  522.39/s  (0.509s,  503.29/s)  LR: 4.549e-03  Data: 0.000 (0.017)
05/14/2023 05:04:03 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:04:09 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 2.686 (2.69)  Time: 1.096s,  233.52/s  (1.096s,  233.52/s)  LR: 3.532e-03  Data: 0.596 (0.596)
05/14/2023 05:04:34 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 2.733 (2.71)  Time: 0.496s,  516.38/s  (0.515s,  496.93/s)  LR: 3.532e-03  Data: 0.011 (0.023)
05/14/2023 05:05:00 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 2.775 (2.73)  Time: 0.498s,  514.33/s  (0.509s,  502.74/s)  LR: 3.532e-03  Data: 0.011 (0.017)
05/14/2023 05:05:02 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 3.183 (2.84)  Time: 0.493s,  519.76/s  (0.509s,  503.15/s)  LR: 3.532e-03  Data: 0.000 (0.017)
05/14/2023 05:05:02 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:05:07 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 2.689 (2.69)  Time: 1.139s,  224.74/s  (1.139s,  224.74/s)  LR: 2.626e-03  Data: 0.623 (0.623)
05/14/2023 05:05:32 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 2.934 (2.81)  Time: 0.527s,  485.92/s  (0.515s,  497.25/s)  LR: 2.626e-03  Data: 0.011 (0.024)
05/14/2023 05:05:57 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 2.954 (2.86)  Time: 0.494s,  518.00/s  (0.508s,  504.05/s)  LR: 2.626e-03  Data: 0.011 (0.018)
05/14/2023 05:05:59 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 3.262 (2.96)  Time: 0.509s,  502.71/s  (0.508s,  504.25/s)  LR: 2.626e-03  Data: 0.000 (0.017)
05/14/2023 05:05:59 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:06:05 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 2.831 (2.83)  Time: 0.994s,  257.51/s  (0.994s,  257.51/s)  LR: 1.842e-03  Data: 0.484 (0.484)
05/14/2023 05:06:30 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 2.655 (2.74)  Time: 0.524s,  488.36/s  (0.514s,  498.23/s)  LR: 1.842e-03  Data: 0.011 (0.021)
05/14/2023 05:06:55 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 2.702 (2.73)  Time: 0.498s,  513.72/s  (0.508s,  503.64/s)  LR: 1.842e-03  Data: 0.011 (0.016)
05/14/2023 05:06:57 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 3.022 (2.80)  Time: 0.527s,  486.13/s  (0.508s,  503.67/s)  LR: 1.842e-03  Data: 0.000 (0.016)
05/14/2023 05:06:57 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:07:03 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 2.718 (2.72)  Time: 1.143s,  223.88/s  (1.143s,  223.88/s)  LR: 1.189e-03  Data: 0.625 (0.625)
05/14/2023 05:07:28 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 2.608 (2.66)  Time: 0.501s,  511.05/s  (0.512s,  499.73/s)  LR: 1.189e-03  Data: 0.011 (0.023)
05/14/2023 05:07:53 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 2.737 (2.69)  Time: 0.500s,  511.61/s  (0.508s,  503.98/s)  LR: 1.189e-03  Data: 0.011 (0.017)
05/14/2023 05:07:55 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 3.428 (2.87)  Time: 0.481s,  532.21/s  (0.507s,  504.47/s)  LR: 1.189e-03  Data: 0.000 (0.017)
05/14/2023 05:07:55 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:08:01 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 2.666 (2.67)  Time: 1.075s,  238.11/s  (1.075s,  238.11/s)  LR: 6.730e-04  Data: 0.569 (0.569)
05/14/2023 05:08:26 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 2.776 (2.72)  Time: 0.516s,  496.05/s  (0.515s,  497.55/s)  LR: 6.730e-04  Data: 0.010 (0.022)
05/14/2023 05:08:51 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 2.695 (2.71)  Time: 0.518s,  494.24/s  (0.510s,  501.92/s)  LR: 6.730e-04  Data: 0.010 (0.017)
05/14/2023 05:08:53 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 2.654 (2.70)  Time: 0.489s,  523.46/s  (0.510s,  502.34/s)  LR: 6.730e-04  Data: 0.000 (0.016)
05/14/2023 05:08:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:08:59 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 2.601 (2.60)  Time: 1.108s,  231.00/s  (1.108s,  231.00/s)  LR: 3.005e-04  Data: 0.601 (0.601)
05/14/2023 05:09:24 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 2.743 (2.67)  Time: 0.494s,  518.41/s  (0.515s,  497.37/s)  LR: 3.005e-04  Data: 0.011 (0.023)
05/14/2023 05:09:49 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 2.711 (2.69)  Time: 0.495s,  516.96/s  (0.508s,  503.80/s)  LR: 3.005e-04  Data: 0.011 (0.017)
05/14/2023 05:09:51 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 2.712 (2.69)  Time: 0.488s,  524.57/s  (0.508s,  504.13/s)  LR: 3.005e-04  Data: 0.000 (0.017)
05/14/2023 05:09:51 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:09:57 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 3.092 (3.09)  Time: 1.152s,  222.26/s  (1.152s,  222.26/s)  LR: 7.532e-05  Data: 0.648 (0.648)
05/14/2023 05:10:22 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 2.826 (2.96)  Time: 0.515s,  497.22/s  (0.517s,  494.72/s)  LR: 7.532e-05  Data: 0.011 (0.024)
05/14/2023 05:10:48 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 2.622 (2.85)  Time: 0.497s,  515.49/s  (0.510s,  502.20/s)  LR: 7.532e-05  Data: 0.012 (0.018)
05/14/2023 05:10:50 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 2.841 (2.85)  Time: 0.487s,  526.19/s  (0.509s,  502.81/s)  LR: 7.532e-05  Data: 0.000 (0.017)
05/14/2023 05:10:50 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 05:10:50 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 05:10:54 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 05:10:56 - INFO - train -   Test: [   0/39]  Time: 1.060 (1.060)  Loss:  2.2109 (2.2109)  Acc@1: 34.7656 (34.7656)  Acc@5: 99.6094 (99.6094)
05/14/2023 05:11:00 - INFO - train -   Test: [  39/39]  Time: 0.340 (0.120)  Loss:  2.0508 (2.1605)  Acc@1: 43.7500 (37.9500)  Acc@5: 93.7500 (99.5500)
05/14/2023 05:11:00 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 05:11:00 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 05:11:01 - INFO - train -   Test: [   0/39]  Time: 0.615 (0.615)  Loss:  0.0896 (0.0896)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:04 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.101)  Loss:  0.0947 (0.0935)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:04 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 05:11:04 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 05:11:06 - INFO - train -   Test: [   0/39]  Time: 0.702 (0.702)  Loss:  0.1940 (0.1940)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:09 - INFO - train -   Test: [  39/39]  Time: 0.052 (0.105)  Loss:  0.1945 (0.1931)  Acc@1: 100.0000 (99.6200)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:09 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 05:11:09 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 05:11:10 - INFO - train -   Test: [   0/39]  Time: 0.614 (0.614)  Loss:  0.2318 (0.2318)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:14 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 05:11:14 - INFO - train -   Test: [  39/39]  Time: 0.027 (0.103)  Loss:  0.2236 (0.2297)  Acc@1: 100.0000 (99.9700)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:14 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 05:11:15 - INFO - train -   Test: [   0/39]  Time: 0.612 (0.612)  Loss:  0.3320 (0.3320)  Acc@1: 97.2656 (97.2656)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:18 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.103)  Loss:  0.2100 (0.2956)  Acc@1: 100.0000 (98.7700)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:18 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 05:11:18 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 05:11:19 - INFO - train -   Test: [   0/39]  Time: 0.638 (0.638)  Loss:  0.8071 (0.8071)  Acc@1: 78.5156 (78.5156)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:23 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 05:11:23 - INFO - train -   Test: [  39/39]  Time: 0.055 (0.104)  Loss:  0.7920 (0.8273)  Acc@1: 87.5000 (78.1800)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:23 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 05:11:24 - INFO - train -   Test: [   0/39]  Time: 0.632 (0.632)  Loss:  0.5708 (0.5708)  Acc@1: 87.1094 (87.1094)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:28 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 05:11:28 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.103)  Loss:  0.5820 (0.5323)  Acc@1: 81.2500 (88.9200)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:28 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 05:11:29 - INFO - train -   Test: [   0/39]  Time: 0.631 (0.631)  Loss:  0.6685 (0.6685)  Acc@1: 85.9375 (85.9375)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:32 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 05:11:32 - INFO - train -   Test: [  39/39]  Time: 0.027 (0.103)  Loss:  0.5786 (0.6325)  Acc@1: 87.5000 (87.1100)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:32 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 05:11:33 - INFO - train -   Test: [   0/39]  Time: 0.757 (0.757)  Loss:  0.6094 (0.6094)  Acc@1: 84.3750 (84.3750)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:37 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.105)  Loss:  0.7983 (0.5435)  Acc@1: 75.0000 (89.1900)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:37 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 05:11:37 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 05:11:38 - INFO - train -   Test: [   0/39]  Time: 0.628 (0.628)  Loss:  0.4456 (0.4456)  Acc@1: 91.4062 (91.4062)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:41 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 05:11:41 - INFO - train -   Test: [  39/39]  Time: 0.029 (0.104)  Loss:  0.3384 (0.4046)  Acc@1: 93.7500 (94.2700)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:41 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 05:11:42 - INFO - train -   Test: [   0/39]  Time: 0.624 (0.624)  Loss:  0.7510 (0.7510)  Acc@1: 83.2031 (83.2031)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:46 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.102)  Loss:  0.7036 (0.6802)  Acc@1: 81.2500 (83.6600)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:46 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 05:11:46 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 05:11:47 - INFO - train -   Test: [   0/39]  Time: 0.627 (0.627)  Loss:  0.6348 (0.6348)  Acc@1: 87.5000 (87.5000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:50 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.103)  Loss:  0.6113 (0.5977)  Acc@1: 81.2500 (88.3300)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:50 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 05:11:51 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 05:11:52 - INFO - train -   Test: [   0/39]  Time: 0.631 (0.631)  Loss:  0.6035 (0.6035)  Acc@1: 86.7188 (86.7188)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:55 - INFO - train -   Test: [  39/39]  Time: 0.026 (0.102)  Loss:  0.5801 (0.5684)  Acc@1: 87.5000 (88.6500)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:11:55 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 05:11:55 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 05:11:56 - INFO - train -   Test: [   0/39]  Time: 0.630 (0.630)  Loss:  0.9688 (0.9688)  Acc@1: 76.1719 (76.1719)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:00 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 05:12:00 - INFO - train -   Test: [  39/39]  Time: 0.028 (0.103)  Loss:  1.1123 (0.9239)  Acc@1: 68.7500 (75.3700)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:00 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 05:12:01 - INFO - train -   Test: [   0/39]  Time: 0.674 (0.674)  Loss:  1.0332 (1.0332)  Acc@1: 66.4062 (66.4062)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:04 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.104)  Loss:  1.0420 (1.0298)  Acc@1: 68.7500 (66.6200)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:04 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 05:12:04 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 05:12:05 - INFO - train -   Test: [   0/39]  Time: 0.642 (0.642)  Loss:  0.8872 (0.8872)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:09 - INFO - train -   Test: [  39/39]  Time: 0.027 (0.104)  Loss:  0.8682 (0.9166)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:09 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 05:12:09 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 05:12:10 - INFO - train -   Test: [   0/39]  Time: 0.607 (0.607)  Loss:  0.9463 (0.9463)  Acc@1: 82.0312 (82.0312)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:13 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.102)  Loss:  0.9180 (0.9170)  Acc@1: 75.0000 (85.2500)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:13 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 05:12:13 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 05:12:15 - INFO - train -   Test: [   0/39]  Time: 0.633 (0.633)  Loss:  1.3789 (1.3789)  Acc@1: 58.5938 (58.5938)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:18 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.102)  Loss:  1.6855 (1.3360)  Acc@1: 43.7500 (60.9700)  Acc@5: 100.0000 (99.9400)
05/14/2023 05:12:18 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 05:12:18 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 05:12:19 - INFO - train -   Test: [   0/39]  Time: 0.637 (0.637)  Loss:  1.2637 (1.2637)  Acc@1: 66.4062 (66.4062)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:23 - INFO - train -   Test: [  39/39]  Time: 0.026 (0.103)  Loss:  1.5098 (1.2686)  Acc@1: 62.5000 (65.3100)  Acc@5: 100.0000 (99.8600)
05/14/2023 05:12:23 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 05:12:23 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 05:12:24 - INFO - train -   Test: [   0/39]  Time: 0.590 (0.590)  Loss:  0.8242 (0.8242)  Acc@1: 83.9844 (83.9844)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:27 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.102)  Loss:  1.0947 (0.8071)  Acc@1: 56.2500 (84.7000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:27 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 05:12:27 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 05:12:28 - INFO - train -   Test: [   0/39]  Time: 0.710 (0.710)  Loss:  0.8818 (0.8818)  Acc@1: 88.2812 (88.2812)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:32 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.105)  Loss:  0.7461 (0.8680)  Acc@1: 87.5000 (89.6100)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:32 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 05:12:32 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 05:12:33 - INFO - train -   Test: [   0/39]  Time: 0.653 (0.653)  Loss:  0.9668 (0.9668)  Acc@1: 84.3750 (84.3750)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:37 - INFO - train -   Test: [  39/39]  Time: 0.026 (0.106)  Loss:  0.9824 (0.9096)  Acc@1: 75.0000 (86.3500)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:37 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 05:12:37 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 05:12:38 - INFO - train -   Test: [   0/39]  Time: 0.700 (0.700)  Loss:  0.6851 (0.6851)  Acc@1: 86.3281 (86.3281)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:41 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 05:12:41 - INFO - train -   Test: [  39/39]  Time: 0.027 (0.105)  Loss:  0.6963 (0.6598)  Acc@1: 81.2500 (89.3700)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:41 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 05:12:42 - INFO - train -   Test: [   0/39]  Time: 0.623 (0.623)  Loss:  0.6143 (0.6143)  Acc@1: 98.8281 (98.8281)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:46 - INFO - train -   Test: [  39/39]  Time: 0.027 (0.103)  Loss:  0.5605 (0.6045)  Acc@1: 100.0000 (98.8800)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:46 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 05:12:46 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 05:12:47 - INFO - train -   Test: [   0/39]  Time: 0.641 (0.641)  Loss:  0.2079 (0.2079)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:50 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 05:12:50 - INFO - train -   Test: [  39/39]  Time: 0.029 (0.104)  Loss:  0.1829 (0.1999)  Acc@1: 100.0000 (99.8000)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:50 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 05:12:52 - INFO - train -   Test: [   0/39]  Time: 0.623 (0.623)  Loss:  1.6406 (1.6406)  Acc@1: 67.9688 (67.9688)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:12:55 - INFO - train -   Test: [  39/39]  Time: 0.029 (0.102)  Loss:  1.8633 (1.6170)  Acc@1: 50.0000 (68.4400)  Acc@5: 100.0000 (99.8500)
05/14/2023 05:12:55 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 05:12:55 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 05:12:56 - INFO - train -   Test: [   0/39]  Time: 0.675 (0.675)  Loss:  0.8682 (0.8682)  Acc@1: 91.7969 (91.7969)  Acc@5: 100.0000 (100.0000)
05/14/2023 05:13:00 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.105)  Loss:  0.9502 (0.8818)  Acc@1: 93.7500 (89.4200)  Acc@5: 100.0000 (100.0000)
