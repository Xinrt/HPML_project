04/24/2023 13:24:52 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
04/24/2023 13:24:52 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
04/24/2023 13:24:57 - INFO - train -   Model resnet50 created, param count:37904976
04/24/2023 13:25:51 - INFO - train -   Using native Torch AMP. Training in mixed precision.
04/24/2023 13:25:51 - INFO - train -   Using native Torch DistributedDataParallel.
04/24/2023 13:26:10 - INFO - train -   Scheduled epochs: 3. LR stepped per epoch.
04/24/2023 13:26:19 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 8.293 (8.29)  Time: 8.396s,   30.49/s  (8.396s,   30.49/s)  LR: 5.500e-06  Data: 1.262 (1.262)
04/24/2023 13:26:35 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 8.011 (8.15)  Time: 0.302s,  846.73/s  (0.479s,  533.95/s)  LR: 5.500e-06  Data: 0.013 (0.038)
04/24/2023 13:26:50 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 8.066 (8.12)  Time: 0.352s,  728.16/s  (0.390s,  656.66/s)  LR: 5.500e-06  Data: 0.012 (0.026)
04/24/2023 13:26:51 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 7.867 (8.06)  Time: 0.252s, 1014.80/s  (0.385s,  665.06/s)  LR: 5.500e-06  Data: 0.000 (0.025)
04/24/2023 13:26:51 - INFO - train -   Distributing BatchNorm running means and vars
04/24/2023 13:26:54 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 8.310 (8.31)  Time: 1.028s,  249.09/s  (1.028s,  249.09/s)  LR: 5.504e-03  Data: 0.731 (0.731)
04/24/2023 13:27:09 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 5.606 (6.96)  Time: 0.290s,  883.19/s  (0.313s,  816.97/s)  LR: 5.504e-03  Data: 0.012 (0.027)
04/24/2023 13:27:24 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 4.632 (6.18)  Time: 0.298s,  858.71/s  (0.305s,  838.16/s)  LR: 5.504e-03  Data: 0.013 (0.020)
04/24/2023 13:27:25 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 4.760 (5.83)  Time: 0.290s,  883.25/s  (0.305s,  840.18/s)  LR: 5.504e-03  Data: 0.000 (0.019)
04/24/2023 13:27:25 - INFO - train -   Distributing BatchNorm running means and vars
04/24/2023 13:27:28 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 4.679 (4.68)  Time: 1.043s,  245.47/s  (1.043s,  245.47/s)  LR: 1.100e-02  Data: 0.746 (0.746)
04/24/2023 13:27:43 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.406 (4.54)  Time: 0.351s,  729.32/s  (0.317s,  806.73/s)  LR: 1.100e-02  Data: 0.011 (0.029)
04/24/2023 13:27:58 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.423 (4.50)  Time: 0.355s,  721.72/s  (0.305s,  840.09/s)  LR: 1.100e-02  Data: 0.012 (0.021)
04/24/2023 13:27:59 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.340 (4.46)  Time: 0.284s,  900.43/s  (0.305s,  839.77/s)  LR: 1.100e-02  Data: 0.000 (0.020)
04/24/2023 13:27:59 - INFO - train -   Distributing BatchNorm running means and vars
04/24/2023 13:28:02 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
