05/08/2023 22:29:43 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 3, total 4, device cuda:3.
05/08/2023 22:29:43 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 2, total 4, device cuda:2.
05/08/2023 22:29:43 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 4, device cuda:1.
05/08/2023 22:29:43 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 4, device cuda:0.
05/08/2023 22:29:46 - INFO - train -   Model resnet18 created, param count:37904976
05/08/2023 22:29:58 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/08/2023 22:29:58 - INFO - train -   Using native Torch DistributedDataParallel.
05/08/2023 22:30:07 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/08/2023 22:30:15 - INFO - train -   Train: 0 [   0/52 (  0%)]  Loss: 7.952 (7.95)  Time: 7.453s,   68.69/s  (7.453s,   68.69/s)  LR: 5.500e-06  Data: 2.493 (2.493)
05/08/2023 22:30:32 - INFO - train -   Train: 0 [  50/52 ( 98%)]  Loss: 8.098 (8.02)  Time: 0.308s, 1661.30/s  (0.493s, 1037.64/s)  LR: 5.500e-06  Data: 0.012 (0.062)
05/08/2023 22:30:33 - INFO - train -   Train: 0 [  51/52 (100%)]  Loss: 8.270 (8.11)  Time: 0.292s, 1752.91/s  (0.490s, 1045.85/s)  LR: 5.500e-06  Data: 0.000 (0.061)
05/08/2023 22:30:33 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:30:36 - INFO - train -   Train: 1 [   0/52 (  0%)]  Loss: 8.215 (8.22)  Time: 1.386s,  369.42/s  (1.386s,  369.42/s)  LR: 5.504e-03  Data: 1.082 (1.082)
05/08/2023 22:30:52 - INFO - train -   Train: 1 [  50/52 ( 98%)]  Loss: 5.921 (7.07)  Time: 0.361s, 1416.98/s  (0.339s, 1509.38/s)  LR: 5.504e-03  Data: 0.013 (0.034)
05/08/2023 22:30:52 - INFO - train -   Train: 1 [  51/52 (100%)]  Loss: 5.847 (6.66)  Time: 0.343s, 1490.82/s  (0.339s, 1509.02/s)  LR: 5.504e-03  Data: 0.000 (0.034)
05/08/2023 22:30:52 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:30:56 - INFO - train -   Train: 2 [   0/52 (  0%)]  Loss: 5.535 (5.54)  Time: 1.442s,  354.99/s  (1.442s,  354.99/s)  LR: 1.100e-02  Data: 1.137 (1.137)
05/08/2023 22:31:12 - INFO - train -   Train: 2 [  50/52 ( 98%)]  Loss: 4.372 (4.95)  Time: 0.295s, 1737.48/s  (0.350s, 1464.88/s)  LR: 1.100e-02  Data: 0.014 (0.035)
05/08/2023 22:31:13 - INFO - train -   Train: 2 [  51/52 (100%)]  Loss: 4.566 (4.82)  Time: 0.356s, 1437.53/s  (0.350s, 1464.35/s)  LR: 1.100e-02  Data: 0.000 (0.035)
05/08/2023 22:31:13 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:31:16 - INFO - train -   Train: 3 [   0/52 (  0%)]  Loss: 4.436 (4.44)  Time: 1.383s,  370.26/s  (1.383s,  370.26/s)  LR: 1.650e-02  Data: 1.080 (1.080)
05/08/2023 22:31:32 - INFO - train -   Train: 3 [  50/52 ( 98%)]  Loss: 4.243 (4.34)  Time: 0.305s, 1678.06/s  (0.342s, 1497.15/s)  LR: 1.650e-02  Data: 0.015 (0.034)
05/08/2023 22:31:32 - INFO - train -   Train: 3 [  51/52 (100%)]  Loss: 4.271 (4.32)  Time: 0.297s, 1722.96/s  (0.341s, 1500.93/s)  LR: 1.650e-02  Data: 0.000 (0.033)
05/08/2023 22:31:32 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:31:35 - INFO - train -   Train: 4 [   0/52 (  0%)]  Loss: 4.262 (4.26)  Time: 1.508s,  339.54/s  (1.508s,  339.54/s)  LR: 2.200e-02  Data: 1.158 (1.158)
05/08/2023 22:31:52 - INFO - train -   Train: 4 [  50/52 ( 98%)]  Loss: 4.122 (4.19)  Time: 0.267s, 1918.13/s  (0.347s, 1475.35/s)  LR: 2.200e-02  Data: 0.012 (0.035)
05/08/2023 22:31:52 - INFO - train -   Train: 4 [  51/52 (100%)]  Loss: 4.082 (4.16)  Time: 0.349s, 1467.09/s  (0.347s, 1475.19/s)  LR: 2.200e-02  Data: 0.000 (0.034)
05/08/2023 22:31:52 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:31:55 - INFO - train -   Train: 5 [   0/52 (  0%)]  Loss: 4.213 (4.21)  Time: 1.481s,  345.78/s  (1.481s,  345.78/s)  LR: 2.566e-02  Data: 1.111 (1.111)
05/08/2023 22:32:11 - INFO - train -   Train: 5 [  50/52 ( 98%)]  Loss: 4.075 (4.14)  Time: 0.362s, 1414.60/s  (0.344s, 1488.91/s)  LR: 2.566e-02  Data: 0.012 (0.034)
05/08/2023 22:32:12 - INFO - train -   Train: 5 [  51/52 (100%)]  Loss: 3.881 (4.06)  Time: 0.352s, 1454.64/s  (0.344s, 1488.24/s)  LR: 2.566e-02  Data: 0.000 (0.034)
05/08/2023 22:32:12 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:32:15 - INFO - train -   Train: 6 [   0/52 (  0%)]  Loss: 3.967 (3.97)  Time: 1.461s,  350.53/s  (1.461s,  350.53/s)  LR: 2.487e-02  Data: 1.153 (1.153)
05/08/2023 22:32:31 - INFO - train -   Train: 6 [  50/52 ( 98%)]  Loss: 3.867 (3.92)  Time: 0.359s, 1425.69/s  (0.346s, 1481.54/s)  LR: 2.487e-02  Data: 0.013 (0.036)
05/08/2023 22:32:31 - INFO - train -   Train: 6 [  51/52 (100%)]  Loss: 3.886 (3.91)  Time: 0.294s, 1742.11/s  (0.345s, 1485.82/s)  LR: 2.487e-02  Data: 0.000 (0.035)
05/08/2023 22:32:31 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:32:35 - INFO - train -   Train: 7 [   0/52 (  0%)]  Loss: 3.919 (3.92)  Time: 1.456s,  351.62/s  (1.456s,  351.62/s)  LR: 2.397e-02  Data: 1.096 (1.096)
05/08/2023 22:32:51 - INFO - train -   Train: 7 [  50/52 ( 98%)]  Loss: 3.736 (3.83)  Time: 0.306s, 1672.13/s  (0.341s, 1501.50/s)  LR: 2.397e-02  Data: 0.014 (0.034)
05/08/2023 22:32:51 - INFO - train -   Train: 7 [  51/52 (100%)]  Loss: 3.748 (3.80)  Time: 0.285s, 1797.48/s  (0.340s, 1506.27/s)  LR: 2.397e-02  Data: 0.000 (0.034)
05/08/2023 22:32:51 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:32:54 - INFO - train -   Train: 8 [   0/52 (  0%)]  Loss: 3.720 (3.72)  Time: 1.395s,  366.96/s  (1.395s,  366.96/s)  LR: 2.295e-02  Data: 1.108 (1.108)
05/08/2023 22:33:11 - INFO - train -   Train: 8 [  50/52 ( 98%)]  Loss: 3.624 (3.67)  Time: 0.307s, 1665.17/s  (0.348s, 1471.04/s)  LR: 2.295e-02  Data: 0.014 (0.034)
05/08/2023 22:33:11 - INFO - train -   Train: 8 [  51/52 (100%)]  Loss: 3.761 (3.70)  Time: 0.360s, 1422.63/s  (0.348s, 1470.07/s)  LR: 2.295e-02  Data: 0.000 (0.034)
05/08/2023 22:33:11 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:33:14 - INFO - train -   Train: 9 [   0/52 (  0%)]  Loss: 3.634 (3.63)  Time: 1.403s,  364.82/s  (1.403s,  364.82/s)  LR: 2.183e-02  Data: 1.099 (1.099)
05/08/2023 22:33:30 - INFO - train -   Train: 9 [  50/52 ( 98%)]  Loss: 3.760 (3.70)  Time: 0.370s, 1384.27/s  (0.344s, 1488.20/s)  LR: 2.183e-02  Data: 0.012 (0.034)
05/08/2023 22:33:31 - INFO - train -   Train: 9 [  51/52 (100%)]  Loss: 3.726 (3.71)  Time: 0.347s, 1475.23/s  (0.344s, 1487.95/s)  LR: 2.183e-02  Data: 0.000 (0.034)
05/08/2023 22:33:31 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:33:34 - INFO - train -   Train: 10 [   0/52 (  0%)]  Loss: 3.557 (3.56)  Time: 1.404s,  364.59/s  (1.404s,  364.59/s)  LR: 2.063e-02  Data: 1.104 (1.104)
05/08/2023 22:33:50 - INFO - train -   Train: 10 [  50/52 ( 98%)]  Loss: 3.586 (3.57)  Time: 0.361s, 1419.67/s  (0.344s, 1489.58/s)  LR: 2.063e-02  Data: 0.013 (0.034)
05/08/2023 22:33:50 - INFO - train -   Train: 10 [  51/52 (100%)]  Loss: 3.625 (3.59)  Time: 0.339s, 1511.17/s  (0.344s, 1489.99/s)  LR: 2.063e-02  Data: 0.000 (0.033)
05/08/2023 22:33:50 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:33:54 - INFO - train -   Train: 11 [   0/52 (  0%)]  Loss: 3.477 (3.48)  Time: 1.350s,  379.14/s  (1.350s,  379.14/s)  LR: 1.934e-02  Data: 1.045 (1.045)
05/08/2023 22:34:10 - INFO - train -   Train: 11 [  50/52 ( 98%)]  Loss: 3.511 (3.49)  Time: 0.309s, 1657.70/s  (0.341s, 1501.44/s)  LR: 1.934e-02  Data: 0.014 (0.033)
05/08/2023 22:34:10 - INFO - train -   Train: 11 [  51/52 (100%)]  Loss: 3.523 (3.50)  Time: 0.288s, 1776.77/s  (0.340s, 1505.93/s)  LR: 1.934e-02  Data: 0.000 (0.033)
05/08/2023 22:34:10 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:34:13 - INFO - train -   Train: 12 [   0/52 (  0%)]  Loss: 3.419 (3.42)  Time: 1.348s,  379.91/s  (1.348s,  379.91/s)  LR: 1.800e-02  Data: 1.044 (1.044)
05/08/2023 22:34:29 - INFO - train -   Train: 12 [  50/52 ( 98%)]  Loss: 3.450 (3.43)  Time: 0.309s, 1659.26/s  (0.338s, 1515.45/s)  LR: 1.800e-02  Data: 0.012 (0.033)
05/08/2023 22:34:29 - INFO - train -   Train: 12 [  51/52 (100%)]  Loss: 3.557 (3.48)  Time: 0.295s, 1733.76/s  (0.337s, 1519.13/s)  LR: 1.800e-02  Data: 0.000 (0.033)
05/08/2023 22:34:29 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:34:32 - INFO - train -   Train: 13 [   0/52 (  0%)]  Loss: 3.395 (3.39)  Time: 1.391s,  368.12/s  (1.391s,  368.12/s)  LR: 1.661e-02  Data: 1.089 (1.089)
05/08/2023 22:34:49 - INFO - train -   Train: 13 [  50/52 ( 98%)]  Loss: 3.467 (3.43)  Time: 0.292s, 1753.09/s  (0.347s, 1475.44/s)  LR: 1.661e-02  Data: 0.014 (0.034)
05/08/2023 22:34:49 - INFO - train -   Train: 13 [  51/52 (100%)]  Loss: 3.424 (3.43)  Time: 0.288s, 1780.30/s  (0.346s, 1480.31/s)  LR: 1.661e-02  Data: 0.000 (0.033)
05/08/2023 22:34:49 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:34:52 - INFO - train -   Train: 14 [   0/52 (  0%)]  Loss: 3.605 (3.61)  Time: 1.334s,  383.80/s  (1.334s,  383.80/s)  LR: 1.519e-02  Data: 0.963 (0.963)
05/08/2023 22:35:09 - INFO - train -   Train: 14 [  50/52 ( 98%)]  Loss: 3.481 (3.54)  Time: 0.360s, 1423.41/s  (0.347s, 1474.22/s)  LR: 1.519e-02  Data: 0.013 (0.032)
05/08/2023 22:35:09 - INFO - train -   Train: 14 [  51/52 (100%)]  Loss: 3.495 (3.53)  Time: 0.342s, 1496.27/s  (0.347s, 1474.64/s)  LR: 1.519e-02  Data: 0.000 (0.031)
05/08/2023 22:35:09 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:35:12 - INFO - train -   Train: 15 [   0/52 (  0%)]  Loss: 3.427 (3.43)  Time: 1.385s,  369.64/s  (1.385s,  369.64/s)  LR: 1.375e-02  Data: 1.091 (1.091)
05/08/2023 22:35:28 - INFO - train -   Train: 15 [  50/52 ( 98%)]  Loss: 3.347 (3.39)  Time: 0.271s, 1890.64/s  (0.342s, 1499.14/s)  LR: 1.375e-02  Data: 0.014 (0.034)
05/08/2023 22:35:28 - INFO - train -   Train: 15 [  51/52 (100%)]  Loss: 3.304 (3.36)  Time: 0.287s, 1781.25/s  (0.340s, 1503.72/s)  LR: 1.375e-02  Data: 0.000 (0.033)
05/08/2023 22:35:28 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:35:32 - INFO - train -   Train: 16 [   0/52 (  0%)]  Loss: 3.361 (3.36)  Time: 1.416s,  361.57/s  (1.416s,  361.57/s)  LR: 1.231e-02  Data: 1.110 (1.110)
05/08/2023 22:35:48 - INFO - train -   Train: 16 [  50/52 ( 98%)]  Loss: 3.494 (3.43)  Time: 0.362s, 1412.46/s  (0.345s, 1483.64/s)  LR: 1.231e-02  Data: 0.012 (0.034)
05/08/2023 22:35:48 - INFO - train -   Train: 16 [  51/52 (100%)]  Loss: 3.422 (3.43)  Time: 0.287s, 1781.39/s  (0.344s, 1488.42/s)  LR: 1.231e-02  Data: 0.000 (0.034)
05/08/2023 22:35:48 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:35:51 - INFO - train -   Train: 17 [   0/52 (  0%)]  Loss: 3.406 (3.41)  Time: 1.305s,  392.46/s  (1.305s,  392.46/s)  LR: 1.089e-02  Data: 1.035 (1.035)
05/08/2023 22:36:07 - INFO - train -   Train: 17 [  50/52 ( 98%)]  Loss: 3.365 (3.39)  Time: 0.277s, 1849.31/s  (0.335s, 1528.32/s)  LR: 1.089e-02  Data: 0.014 (0.033)
05/08/2023 22:36:08 - INFO - train -   Train: 17 [  51/52 (100%)]  Loss: 3.330 (3.37)  Time: 0.288s, 1780.25/s  (0.334s, 1532.49/s)  LR: 1.089e-02  Data: 0.000 (0.032)
05/08/2023 22:36:08 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:36:11 - INFO - train -   Train: 18 [   0/52 (  0%)]  Loss: 3.450 (3.45)  Time: 1.513s,  338.32/s  (1.513s,  338.32/s)  LR: 9.501e-03  Data: 1.163 (1.163)
05/08/2023 22:36:27 - INFO - train -   Train: 18 [  50/52 ( 98%)]  Loss: 3.356 (3.40)  Time: 0.264s, 1935.87/s  (0.343s, 1491.43/s)  LR: 9.501e-03  Data: 0.013 (0.035)
05/08/2023 22:36:27 - INFO - train -   Train: 18 [  51/52 (100%)]  Loss: 3.365 (3.39)  Time: 0.295s, 1737.01/s  (0.342s, 1495.49/s)  LR: 9.501e-03  Data: 0.000 (0.035)
05/08/2023 22:36:27 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:36:30 - INFO - train -   Train: 19 [   0/52 (  0%)]  Loss: 3.317 (3.32)  Time: 1.421s,  360.19/s  (1.421s,  360.19/s)  LR: 8.157e-03  Data: 1.057 (1.057)
05/08/2023 22:36:46 - INFO - train -   Train: 19 [  50/52 ( 98%)]  Loss: 3.375 (3.35)  Time: 0.309s, 1655.18/s  (0.337s, 1521.52/s)  LR: 8.157e-03  Data: 0.013 (0.034)
05/08/2023 22:36:46 - INFO - train -   Train: 19 [  51/52 (100%)]  Loss: 3.488 (3.39)  Time: 0.348s, 1472.99/s  (0.337s, 1520.56/s)  LR: 8.157e-03  Data: 0.000 (0.033)
05/08/2023 22:36:46 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:36:49 - INFO - train -   Train: 20 [   0/52 (  0%)]  Loss: 3.417 (3.42)  Time: 1.388s,  368.96/s  (1.388s,  368.96/s)  LR: 6.875e-03  Data: 1.030 (1.030)
05/08/2023 22:37:07 - INFO - train -   Train: 20 [  50/52 ( 98%)]  Loss: 3.321 (3.37)  Time: 0.362s, 1414.73/s  (0.363s, 1409.22/s)  LR: 6.875e-03  Data: 0.012 (0.033)
05/08/2023 22:37:07 - INFO - train -   Train: 20 [  51/52 (100%)]  Loss: 3.397 (3.38)  Time: 0.288s, 1775.67/s  (0.362s, 1414.83/s)  LR: 6.875e-03  Data: 0.000 (0.032)
05/08/2023 22:37:07 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:37:10 - INFO - train -   Train: 21 [   0/52 (  0%)]  Loss: 3.317 (3.32)  Time: 1.451s,  352.93/s  (1.451s,  352.93/s)  LR: 5.668e-03  Data: 1.121 (1.121)
05/08/2023 22:37:26 - INFO - train -   Train: 21 [  50/52 ( 98%)]  Loss: 3.293 (3.31)  Time: 0.297s, 1723.86/s  (0.343s, 1492.10/s)  LR: 5.668e-03  Data: 0.012 (0.035)
05/08/2023 22:37:27 - INFO - train -   Train: 21 [  51/52 (100%)]  Loss: 3.309 (3.31)  Time: 0.295s, 1735.13/s  (0.342s, 1496.13/s)  LR: 5.668e-03  Data: 0.000 (0.034)
05/08/2023 22:37:27 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:37:30 - INFO - train -   Train: 22 [   0/52 (  0%)]  Loss: 3.259 (3.26)  Time: 1.356s,  377.45/s  (1.356s,  377.45/s)  LR: 4.549e-03  Data: 1.054 (1.054)
05/08/2023 22:37:46 - INFO - train -   Train: 22 [  50/52 ( 98%)]  Loss: 3.307 (3.28)  Time: 0.304s, 1682.99/s  (0.351s, 1458.99/s)  LR: 4.549e-03  Data: 0.013 (0.033)
05/08/2023 22:37:47 - INFO - train -   Train: 22 [  51/52 (100%)]  Loss: 3.260 (3.28)  Time: 0.293s, 1747.28/s  (0.350s, 1463.64/s)  LR: 4.549e-03  Data: 0.000 (0.033)
05/08/2023 22:37:47 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:37:50 - INFO - train -   Train: 23 [   0/52 (  0%)]  Loss: 3.253 (3.25)  Time: 1.392s,  367.83/s  (1.392s,  367.83/s)  LR: 3.532e-03  Data: 1.097 (1.097)
05/08/2023 22:38:06 - INFO - train -   Train: 23 [  50/52 ( 98%)]  Loss: 3.128 (3.19)  Time: 0.305s, 1681.08/s  (0.338s, 1513.56/s)  LR: 3.532e-03  Data: 0.014 (0.035)
05/08/2023 22:38:06 - INFO - train -   Train: 23 [  51/52 (100%)]  Loss: 3.338 (3.24)  Time: 0.347s, 1473.66/s  (0.338s, 1512.77/s)  LR: 3.532e-03  Data: 0.000 (0.034)
05/08/2023 22:38:06 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:38:09 - INFO - train -   Train: 24 [   0/52 (  0%)]  Loss: 3.184 (3.18)  Time: 1.409s,  363.26/s  (1.409s,  363.26/s)  LR: 2.626e-03  Data: 1.108 (1.108)
05/08/2023 22:38:25 - INFO - train -   Train: 24 [  50/52 ( 98%)]  Loss: 3.343 (3.26)  Time: 0.308s, 1660.01/s  (0.340s, 1505.68/s)  LR: 2.626e-03  Data: 0.013 (0.034)
05/08/2023 22:38:25 - INFO - train -   Train: 24 [  51/52 (100%)]  Loss: 3.344 (3.29)  Time: 0.262s, 1953.05/s  (0.339s, 1512.34/s)  LR: 2.626e-03  Data: 0.000 (0.034)
05/08/2023 22:38:25 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:38:29 - INFO - train -   Train: 25 [   0/52 (  0%)]  Loss: 3.374 (3.37)  Time: 1.385s,  369.79/s  (1.385s,  369.79/s)  LR: 1.842e-03  Data: 1.028 (1.028)
05/08/2023 22:38:45 - INFO - train -   Train: 25 [  50/52 ( 98%)]  Loss: 3.295 (3.33)  Time: 0.364s, 1406.42/s  (0.346s, 1477.89/s)  LR: 1.842e-03  Data: 0.011 (0.033)
05/08/2023 22:38:45 - INFO - train -   Train: 25 [  51/52 (100%)]  Loss: 3.310 (3.33)  Time: 0.288s, 1775.95/s  (0.345s, 1482.67/s)  LR: 1.842e-03  Data: 0.000 (0.032)
05/08/2023 22:38:45 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:38:48 - INFO - train -   Train: 26 [   0/52 (  0%)]  Loss: 3.284 (3.28)  Time: 1.280s,  400.09/s  (1.280s,  400.09/s)  LR: 1.189e-03  Data: 0.989 (0.989)
05/08/2023 22:39:04 - INFO - train -   Train: 26 [  50/52 ( 98%)]  Loss: 3.325 (3.30)  Time: 0.315s, 1624.50/s  (0.338s, 1515.48/s)  LR: 1.189e-03  Data: 0.012 (0.032)
05/08/2023 22:39:05 - INFO - train -   Train: 26 [  51/52 (100%)]  Loss: 3.259 (3.29)  Time: 0.287s, 1784.76/s  (0.337s, 1519.89/s)  LR: 1.189e-03  Data: 0.000 (0.032)
05/08/2023 22:39:05 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:39:08 - INFO - train -   Train: 27 [   0/52 (  0%)]  Loss: 3.381 (3.38)  Time: 1.352s,  378.56/s  (1.352s,  378.56/s)  LR: 6.730e-04  Data: 0.999 (0.999)
05/08/2023 22:39:24 - INFO - train -   Train: 27 [  50/52 ( 98%)]  Loss: 3.317 (3.35)  Time: 0.310s, 1650.61/s  (0.340s, 1506.00/s)  LR: 6.730e-04  Data: 0.014 (0.032)
05/08/2023 22:39:24 - INFO - train -   Train: 27 [  51/52 (100%)]  Loss: 3.380 (3.36)  Time: 0.346s, 1481.64/s  (0.340s, 1505.52/s)  LR: 6.730e-04  Data: 0.000 (0.032)
05/08/2023 22:39:24 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:39:27 - INFO - train -   Train: 28 [   0/52 (  0%)]  Loss: 3.263 (3.26)  Time: 1.322s,  387.19/s  (1.322s,  387.19/s)  LR: 3.005e-04  Data: 1.031 (1.031)
05/08/2023 22:39:43 - INFO - train -   Train: 28 [  50/52 ( 98%)]  Loss: 3.294 (3.28)  Time: 0.276s, 1851.82/s  (0.340s, 1508.03/s)  LR: 3.005e-04  Data: 0.013 (0.033)
05/08/2023 22:39:44 - INFO - train -   Train: 28 [  51/52 (100%)]  Loss: 3.253 (3.27)  Time: 0.289s, 1768.71/s  (0.339s, 1512.32/s)  LR: 3.005e-04  Data: 0.000 (0.032)
05/08/2023 22:39:44 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:39:47 - INFO - train -   Train: 29 [   0/52 (  0%)]  Loss: 3.270 (3.27)  Time: 1.382s,  370.35/s  (1.382s,  370.35/s)  LR: 7.532e-05  Data: 1.071 (1.071)
05/08/2023 22:40:03 - INFO - train -   Train: 29 [  50/52 ( 98%)]  Loss: 3.278 (3.27)  Time: 0.365s, 1404.43/s  (0.345s, 1482.87/s)  LR: 7.532e-05  Data: 0.014 (0.034)
05/08/2023 22:40:04 - INFO - train -   Train: 29 [  51/52 (100%)]  Loss: 3.252 (3.27)  Time: 0.344s, 1488.39/s  (0.345s, 1482.98/s)  LR: 7.532e-05  Data: 0.000 (0.033)
05/08/2023 22:40:04 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:40:04 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:40:04 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:40:04 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:40:06 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:40:08 - INFO - train -   Test: [   0/19]  Time: 1.746 (1.746)  Loss:  1.2070 (1.2070)  Acc@1: 72.2656 (72.2656)  Acc@5: 99.4141 (99.4141)
05/08/2023 22:40:09 - INFO - train -   Test: [  19/19]  Time: 0.144 (0.139)  Loss:  1.2676 (1.2483)  Acc@1: 70.9559 (69.6200)  Acc@5: 99.2647 (98.7500)
05/08/2023 22:40:09 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:40:09 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:40:09 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:40:09 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:40:10 - INFO - train -   Test: [   0/19]  Time: 0.865 (0.865)  Loss:  0.9238 (0.9238)  Acc@1: 87.3047 (87.3047)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:11 - INFO - train -   Test: [  19/19]  Time: 0.319 (0.123)  Loss:  0.9375 (0.9042)  Acc@1: 84.9265 (87.6100)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:11 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:40:11 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:40:11 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:40:11 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:40:12 - INFO - train -   Test: [   0/19]  Time: 0.823 (0.823)  Loss:  0.9438 (0.9438)  Acc@1: 85.3516 (85.3516)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:14 - INFO - train -   Test: [  19/19]  Time: 0.021 (0.101)  Loss:  0.9995 (0.9899)  Acc@1: 81.9853 (82.0100)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:14 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:40:14 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:40:14 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:40:14 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:40:15 - INFO - train -   Test: [   0/19]  Time: 0.874 (0.874)  Loss:  0.8701 (0.8701)  Acc@1: 89.8438 (89.8438)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:16 - INFO - train -   Test: [  19/19]  Time: 0.021 (0.101)  Loss:  0.8872 (0.8653)  Acc@1: 87.1324 (88.4800)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:16 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:40:16 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:40:16 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:40:16 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:40:17 - INFO - train -   Test: [   0/19]  Time: 0.861 (0.861)  Loss:  1.0615 (1.0615)  Acc@1: 83.9844 (83.9844)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:18 - INFO - train -   Test: [  19/19]  Time: 0.020 (0.100)  Loss:  0.9995 (1.0188)  Acc@1: 86.3971 (85.3400)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:18 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:40:18 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:40:18 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:40:18 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:40:19 - INFO - train -   Test: [   0/19]  Time: 0.868 (0.868)  Loss:  1.1934 (1.1934)  Acc@1: 81.6406 (81.6406)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:20 - INFO - train -   Test: [  19/19]  Time: 0.017 (0.098)  Loss:  1.1465 (1.1647)  Acc@1: 84.1912 (82.6400)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:20 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:40:20 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:40:20 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:40:20 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:40:21 - INFO - train -   Test: [   0/19]  Time: 0.876 (0.876)  Loss:  1.0811 (1.0811)  Acc@1: 84.9609 (84.9609)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:22 - INFO - train -   Test: [  19/19]  Time: 0.016 (0.097)  Loss:  1.0176 (1.0721)  Acc@1: 85.2941 (83.8900)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:22 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:40:22 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:40:22 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:40:22 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:40:23 - INFO - train -   Test: [   0/19]  Time: 0.847 (0.847)  Loss:  0.9814 (0.9814)  Acc@1: 87.3047 (87.3047)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:24 - INFO - train -   Test: [  19/19]  Time: 0.016 (0.095)  Loss:  1.0000 (0.9937)  Acc@1: 86.7647 (84.5800)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:24 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:40:24 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:40:24 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:40:24 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:40:25 - INFO - train -   Test: [   0/19]  Time: 0.863 (0.863)  Loss:  1.1113 (1.1113)  Acc@1: 91.0156 (91.0156)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:26 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:40:26 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:40:26 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:40:26 - INFO - train -   Test: [  19/19]  Time: 0.014 (0.093)  Loss:  1.0615 (1.0974)  Acc@1: 89.7059 (88.1300)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:26 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:40:27 - INFO - train -   Test: [   0/19]  Time: 0.816 (0.816)  Loss:  0.9995 (0.9995)  Acc@1: 94.1406 (94.1406)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:40:28 - INFO - train -   Test: [  19/19]  Time: 0.013 (0.088)  Loss:  0.9658 (0.9939)  Acc@1: 94.1176 (93.0200)  Acc@5: 100.0000 (100.0000)
