05/08/2023 15:24:17 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 4, device cuda:0.
05/08/2023 15:24:17 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 4, device cuda:1.
05/08/2023 15:24:17 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 3, total 4, device cuda:3.
05/08/2023 15:24:17 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 2, total 4, device cuda:2.
05/08/2023 15:24:20 - INFO - train -   Model resnet18 created, param count:33651792
05/08/2023 15:25:05 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/08/2023 15:25:05 - INFO - train -   Using native Torch DistributedDataParallel.
05/08/2023 15:25:21 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/08/2023 15:25:29 - INFO - train -   Train: 0 [   0/52 (  0%)]  Loss: 6.942 (6.94)  Time: 8.375s,   61.13/s  (8.375s,   61.13/s)  LR: 5.500e-06  Data: 1.838 (1.838)
05/08/2023 15:25:43 - INFO - train -   Train: 0 [  50/52 ( 98%)]  Loss: 6.945 (6.94)  Time: 0.262s, 1951.60/s  (0.429s, 1192.13/s)  LR: 5.500e-06  Data: 0.014 (0.050)
05/08/2023 15:25:43 - INFO - train -   Train: 0 [  51/52 (100%)]  Loss: 7.334 (7.07)  Time: 0.248s, 2068.17/s  (0.426s, 1201.92/s)  LR: 5.500e-06  Data: 0.000 (0.049)
05/08/2023 15:25:43 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:25:47 - INFO - train -   Train: 1 [   0/52 (  0%)]  Loss: 7.338 (7.34)  Time: 1.440s,  355.54/s  (1.440s,  355.54/s)  LR: 5.504e-03  Data: 1.190 (1.190)
05/08/2023 15:26:00 - INFO - train -   Train: 1 [  50/52 ( 98%)]  Loss: 4.122 (5.73)  Time: 0.272s, 1883.73/s  (0.289s, 1772.33/s)  LR: 5.504e-03  Data: 0.015 (0.037)
05/08/2023 15:26:00 - INFO - train -   Train: 1 [  51/52 (100%)]  Loss: 4.125 (5.20)  Time: 0.258s, 1982.45/s  (0.288s, 1775.95/s)  LR: 5.504e-03  Data: 0.000 (0.037)
05/08/2023 15:26:00 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:26:04 - INFO - train -   Train: 2 [   0/52 (  0%)]  Loss: 4.089 (4.09)  Time: 1.636s,  313.03/s  (1.636s,  313.03/s)  LR: 1.100e-02  Data: 1.381 (1.381)
05/08/2023 15:26:17 - INFO - train -   Train: 2 [  50/52 ( 98%)]  Loss: 3.613 (3.85)  Time: 0.257s, 1994.03/s  (0.295s, 1736.93/s)  LR: 1.100e-02  Data: 0.015 (0.041)
05/08/2023 15:26:18 - INFO - train -   Train: 2 [  51/52 (100%)]  Loss: 3.735 (3.81)  Time: 0.259s, 1976.67/s  (0.294s, 1740.99/s)  LR: 1.100e-02  Data: 0.000 (0.040)
05/08/2023 15:26:18 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:26:21 - INFO - train -   Train: 3 [   0/52 (  0%)]  Loss: 3.836 (3.84)  Time: 1.545s,  331.41/s  (1.545s,  331.41/s)  LR: 1.650e-02  Data: 1.288 (1.288)
05/08/2023 15:26:34 - INFO - train -   Train: 3 [  50/52 ( 98%)]  Loss: 3.419 (3.63)  Time: 0.261s, 1960.44/s  (0.291s, 1756.73/s)  LR: 1.650e-02  Data: 0.015 (0.039)
05/08/2023 15:26:35 - INFO - train -   Train: 3 [  51/52 (100%)]  Loss: 3.492 (3.58)  Time: 0.246s, 2079.03/s  (0.291s, 1761.98/s)  LR: 1.650e-02  Data: 0.000 (0.038)
05/08/2023 15:26:35 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:26:39 - INFO - train -   Train: 4 [   0/52 (  0%)]  Loss: 3.537 (3.54)  Time: 1.603s,  319.44/s  (1.603s,  319.44/s)  LR: 2.200e-02  Data: 1.335 (1.335)
05/08/2023 15:26:52 - INFO - train -   Train: 4 [  50/52 ( 98%)]  Loss: 3.246 (3.39)  Time: 0.249s, 2053.90/s  (0.294s, 1742.96/s)  LR: 2.200e-02  Data: 0.014 (0.040)
05/08/2023 15:26:53 - INFO - train -   Train: 4 [  51/52 (100%)]  Loss: 3.378 (3.39)  Time: 0.256s, 2001.68/s  (0.293s, 1747.30/s)  LR: 2.200e-02  Data: 0.000 (0.039)
05/08/2023 15:26:53 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:26:56 - INFO - train -   Train: 5 [   0/52 (  0%)]  Loss: 3.231 (3.23)  Time: 1.615s,  316.95/s  (1.615s,  316.95/s)  LR: 2.566e-02  Data: 1.340 (1.340)
05/08/2023 15:27:09 - INFO - train -   Train: 5 [  50/52 ( 98%)]  Loss: 3.141 (3.19)  Time: 0.274s, 1866.48/s  (0.292s, 1755.17/s)  LR: 2.566e-02  Data: 0.015 (0.040)
05/08/2023 15:27:10 - INFO - train -   Train: 5 [  51/52 (100%)]  Loss: 3.239 (3.20)  Time: 0.259s, 1974.04/s  (0.291s, 1758.92/s)  LR: 2.566e-02  Data: 0.000 (0.039)
05/08/2023 15:27:10 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:27:14 - INFO - train -   Train: 6 [   0/52 (  0%)]  Loss: 3.165 (3.17)  Time: 1.529s,  334.80/s  (1.529s,  334.80/s)  LR: 2.487e-02  Data: 1.276 (1.276)
05/08/2023 15:27:27 - INFO - train -   Train: 6 [  50/52 ( 98%)]  Loss: 3.030 (3.10)  Time: 0.273s, 1874.63/s  (0.291s, 1759.98/s)  LR: 2.487e-02  Data: 0.015 (0.039)
05/08/2023 15:27:27 - INFO - train -   Train: 6 [  51/52 (100%)]  Loss: 3.006 (3.07)  Time: 0.248s, 2062.95/s  (0.290s, 1764.96/s)  LR: 2.487e-02  Data: 0.000 (0.038)
05/08/2023 15:27:27 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:27:31 - INFO - train -   Train: 7 [   0/52 (  0%)]  Loss: 2.949 (2.95)  Time: 1.469s,  348.62/s  (1.469s,  348.62/s)  LR: 2.397e-02  Data: 1.205 (1.205)
05/08/2023 15:27:44 - INFO - train -   Train: 7 [  50/52 ( 98%)]  Loss: 3.100 (3.02)  Time: 0.261s, 1959.01/s  (0.289s, 1773.69/s)  LR: 2.397e-02  Data: 0.014 (0.037)
05/08/2023 15:27:44 - INFO - train -   Train: 7 [  51/52 (100%)]  Loss: 3.009 (3.02)  Time: 0.242s, 2113.42/s  (0.288s, 1779.19/s)  LR: 2.397e-02  Data: 0.000 (0.036)
05/08/2023 15:27:44 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:27:48 - INFO - train -   Train: 8 [   0/52 (  0%)]  Loss: 3.064 (3.06)  Time: 1.456s,  351.71/s  (1.456s,  351.71/s)  LR: 2.295e-02  Data: 1.206 (1.206)
05/08/2023 15:28:01 - INFO - train -   Train: 8 [  50/52 ( 98%)]  Loss: 2.988 (3.03)  Time: 0.262s, 1954.67/s  (0.290s, 1764.68/s)  LR: 2.295e-02  Data: 0.014 (0.037)
05/08/2023 15:28:02 - INFO - train -   Train: 8 [  51/52 (100%)]  Loss: 2.990 (3.01)  Time: 0.260s, 1970.79/s  (0.290s, 1768.23/s)  LR: 2.295e-02  Data: 0.000 (0.036)
05/08/2023 15:28:02 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:28:05 - INFO - train -   Train: 9 [   0/52 (  0%)]  Loss: 3.053 (3.05)  Time: 1.477s,  346.54/s  (1.477s,  346.54/s)  LR: 2.183e-02  Data: 1.224 (1.224)
05/08/2023 15:28:18 - INFO - train -   Train: 9 [  50/52 ( 98%)]  Loss: 2.878 (2.97)  Time: 0.273s, 1876.97/s  (0.290s, 1767.39/s)  LR: 2.183e-02  Data: 0.014 (0.037)
05/08/2023 15:28:18 - INFO - train -   Train: 9 [  51/52 (100%)]  Loss: 2.907 (2.95)  Time: 0.260s, 1971.55/s  (0.289s, 1770.92/s)  LR: 2.183e-02  Data: 0.000 (0.037)
05/08/2023 15:28:18 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:28:22 - INFO - train -   Train: 10 [   0/52 (  0%)]  Loss: 2.953 (2.95)  Time: 1.472s,  347.91/s  (1.472s,  347.91/s)  LR: 2.063e-02  Data: 1.219 (1.219)
05/08/2023 15:28:35 - INFO - train -   Train: 10 [  50/52 ( 98%)]  Loss: 2.992 (2.97)  Time: 0.271s, 1891.03/s  (0.289s, 1769.57/s)  LR: 2.063e-02  Data: 0.014 (0.037)
05/08/2023 15:28:35 - INFO - train -   Train: 10 [  51/52 (100%)]  Loss: 3.086 (3.01)  Time: 0.258s, 1986.42/s  (0.289s, 1773.29/s)  LR: 2.063e-02  Data: 0.000 (0.037)
05/08/2023 15:28:35 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:28:39 - INFO - train -   Train: 11 [   0/52 (  0%)]  Loss: 3.008 (3.01)  Time: 1.447s,  353.91/s  (1.447s,  353.91/s)  LR: 1.934e-02  Data: 1.172 (1.172)
05/08/2023 15:28:52 - INFO - train -   Train: 11 [  50/52 ( 98%)]  Loss: 2.906 (2.96)  Time: 0.260s, 1969.81/s  (0.289s, 1773.74/s)  LR: 1.934e-02  Data: 0.014 (0.037)
05/08/2023 15:28:52 - INFO - train -   Train: 11 [  51/52 (100%)]  Loss: 2.911 (2.94)  Time: 0.247s, 2075.63/s  (0.288s, 1778.71/s)  LR: 1.934e-02  Data: 0.000 (0.036)
05/08/2023 15:28:52 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:28:56 - INFO - train -   Train: 12 [   0/52 (  0%)]  Loss: 3.078 (3.08)  Time: 1.437s,  356.24/s  (1.437s,  356.24/s)  LR: 1.800e-02  Data: 1.183 (1.183)
05/08/2023 15:29:09 - INFO - train -   Train: 12 [  50/52 ( 98%)]  Loss: 3.055 (3.07)  Time: 0.264s, 1939.95/s  (0.287s, 1783.59/s)  LR: 1.800e-02  Data: 0.015 (0.037)
05/08/2023 15:29:09 - INFO - train -   Train: 12 [  51/52 (100%)]  Loss: 2.888 (3.01)  Time: 0.247s, 2071.46/s  (0.286s, 1788.37/s)  LR: 1.800e-02  Data: 0.000 (0.036)
05/08/2023 15:29:09 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:29:12 - INFO - train -   Train: 13 [   0/52 (  0%)]  Loss: 2.887 (2.89)  Time: 1.513s,  338.48/s  (1.513s,  338.48/s)  LR: 1.661e-02  Data: 1.265 (1.265)
05/08/2023 15:29:26 - INFO - train -   Train: 13 [  50/52 ( 98%)]  Loss: 2.816 (2.85)  Time: 0.258s, 1987.24/s  (0.290s, 1764.34/s)  LR: 1.661e-02  Data: 0.015 (0.038)
05/08/2023 15:29:26 - INFO - train -   Train: 13 [  51/52 (100%)]  Loss: 2.781 (2.83)  Time: 0.248s, 2066.90/s  (0.289s, 1769.32/s)  LR: 1.661e-02  Data: 0.000 (0.038)
05/08/2023 15:29:26 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:29:29 - INFO - train -   Train: 14 [   0/52 (  0%)]  Loss: 2.903 (2.90)  Time: 1.441s,  355.21/s  (1.441s,  355.21/s)  LR: 1.519e-02  Data: 1.164 (1.164)
05/08/2023 15:29:43 - INFO - train -   Train: 14 [  50/52 ( 98%)]  Loss: 2.714 (2.81)  Time: 0.272s, 1882.85/s  (0.292s, 1755.94/s)  LR: 1.519e-02  Data: 0.015 (0.037)
05/08/2023 15:29:43 - INFO - train -   Train: 14 [  51/52 (100%)]  Loss: 2.824 (2.81)  Time: 0.258s, 1987.93/s  (0.291s, 1759.89/s)  LR: 1.519e-02  Data: 0.000 (0.036)
05/08/2023 15:29:43 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:29:46 - INFO - train -   Train: 15 [   0/52 (  0%)]  Loss: 2.838 (2.84)  Time: 1.502s,  340.86/s  (1.502s,  340.86/s)  LR: 1.375e-02  Data: 1.250 (1.250)
05/08/2023 15:29:59 - INFO - train -   Train: 15 [  50/52 ( 98%)]  Loss: 2.831 (2.83)  Time: 0.249s, 2054.61/s  (0.291s, 1762.48/s)  LR: 1.375e-02  Data: 0.014 (0.038)
05/08/2023 15:29:59 - INFO - train -   Train: 15 [  51/52 (100%)]  Loss: 2.878 (2.85)  Time: 0.247s, 2069.65/s  (0.290s, 1767.52/s)  LR: 1.375e-02  Data: 0.000 (0.038)
05/08/2023 15:29:59 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:30:03 - INFO - train -   Train: 16 [   0/52 (  0%)]  Loss: 2.775 (2.77)  Time: 1.544s,  331.58/s  (1.544s,  331.58/s)  LR: 1.231e-02  Data: 1.290 (1.290)
05/08/2023 15:30:16 - INFO - train -   Train: 16 [  50/52 ( 98%)]  Loss: 2.815 (2.79)  Time: 0.271s, 1888.87/s  (0.292s, 1755.56/s)  LR: 1.231e-02  Data: 0.014 (0.039)
05/08/2023 15:30:16 - INFO - train -   Train: 16 [  51/52 (100%)]  Loss: 2.811 (2.80)  Time: 0.243s, 2110.87/s  (0.291s, 1761.26/s)  LR: 1.231e-02  Data: 0.000 (0.038)
05/08/2023 15:30:16 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:30:20 - INFO - train -   Train: 17 [   0/52 (  0%)]  Loss: 2.843 (2.84)  Time: 1.568s,  326.47/s  (1.568s,  326.47/s)  LR: 1.089e-02  Data: 1.319 (1.319)
05/08/2023 15:30:33 - INFO - train -   Train: 17 [  50/52 ( 98%)]  Loss: 2.859 (2.85)  Time: 0.248s, 2060.57/s  (0.289s, 1771.25/s)  LR: 1.089e-02  Data: 0.014 (0.039)
05/08/2023 15:30:33 - INFO - train -   Train: 17 [  51/52 (100%)]  Loss: 2.823 (2.84)  Time: 0.247s, 2071.04/s  (0.288s, 1776.20/s)  LR: 1.089e-02  Data: 0.000 (0.039)
05/08/2023 15:30:33 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:30:37 - INFO - train -   Train: 18 [   0/52 (  0%)]  Loss: 2.815 (2.82)  Time: 1.559s,  328.36/s  (1.559s,  328.36/s)  LR: 9.501e-03  Data: 1.294 (1.294)
05/08/2023 15:30:50 - INFO - train -   Train: 18 [  50/52 ( 98%)]  Loss: 2.841 (2.83)  Time: 0.245s, 2086.99/s  (0.290s, 1763.16/s)  LR: 9.501e-03  Data: 0.015 (0.039)
05/08/2023 15:30:50 - INFO - train -   Train: 18 [  51/52 (100%)]  Loss: 2.788 (2.81)  Time: 0.249s, 2059.63/s  (0.290s, 1768.06/s)  LR: 9.501e-03  Data: 0.000 (0.038)
05/08/2023 15:30:50 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:30:54 - INFO - train -   Train: 19 [   0/52 (  0%)]  Loss: 2.766 (2.77)  Time: 1.686s,  303.69/s  (1.686s,  303.69/s)  LR: 8.157e-03  Data: 1.434 (1.434)
05/08/2023 15:31:07 - INFO - train -   Train: 19 [  50/52 ( 98%)]  Loss: 2.664 (2.72)  Time: 0.261s, 1959.48/s  (0.293s, 1749.27/s)  LR: 8.157e-03  Data: 0.014 (0.042)
05/08/2023 15:31:07 - INFO - train -   Train: 19 [  51/52 (100%)]  Loss: 2.793 (2.74)  Time: 0.258s, 1985.36/s  (0.292s, 1753.28/s)  LR: 8.157e-03  Data: 0.000 (0.041)
05/08/2023 15:31:07 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:31:10 - INFO - train -   Train: 20 [   0/52 (  0%)]  Loss: 2.756 (2.76)  Time: 1.410s,  363.13/s  (1.410s,  363.13/s)  LR: 6.875e-03  Data: 1.148 (1.148)
05/08/2023 15:31:24 - INFO - train -   Train: 20 [  50/52 ( 98%)]  Loss: 2.937 (2.85)  Time: 0.271s, 1891.27/s  (0.293s, 1750.25/s)  LR: 6.875e-03  Data: 0.014 (0.037)
05/08/2023 15:31:24 - INFO - train -   Train: 20 [  51/52 (100%)]  Loss: 2.766 (2.82)  Time: 0.248s, 2064.87/s  (0.292s, 1755.39/s)  LR: 6.875e-03  Data: 0.000 (0.036)
05/08/2023 15:31:24 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:31:28 - INFO - train -   Train: 21 [   0/52 (  0%)]  Loss: 2.796 (2.80)  Time: 1.596s,  320.85/s  (1.596s,  320.85/s)  LR: 5.668e-03  Data: 1.345 (1.345)
05/08/2023 15:31:41 - INFO - train -   Train: 21 [  50/52 ( 98%)]  Loss: 2.704 (2.75)  Time: 0.256s, 2002.51/s  (0.291s, 1759.17/s)  LR: 5.668e-03  Data: 0.015 (0.040)
05/08/2023 15:31:41 - INFO - train -   Train: 21 [  51/52 (100%)]  Loss: 2.822 (2.77)  Time: 0.247s, 2072.92/s  (0.290s, 1764.31/s)  LR: 5.668e-03  Data: 0.000 (0.039)
05/08/2023 15:31:41 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:31:45 - INFO - train -   Train: 22 [   0/52 (  0%)]  Loss: 2.629 (2.63)  Time: 1.579s,  324.23/s  (1.579s,  324.23/s)  LR: 4.549e-03  Data: 1.327 (1.327)
05/08/2023 15:31:58 - INFO - train -   Train: 22 [  50/52 ( 98%)]  Loss: 2.743 (2.69)  Time: 0.262s, 1957.43/s  (0.293s, 1749.12/s)  LR: 4.549e-03  Data: 0.014 (0.040)
05/08/2023 15:31:58 - INFO - train -   Train: 22 [  51/52 (100%)]  Loss: 2.725 (2.70)  Time: 0.248s, 2063.17/s  (0.292s, 1754.25/s)  LR: 4.549e-03  Data: 0.000 (0.039)
05/08/2023 15:31:58 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:32:02 - INFO - train -   Train: 23 [   0/52 (  0%)]  Loss: 2.743 (2.74)  Time: 1.614s,  317.24/s  (1.614s,  317.24/s)  LR: 3.532e-03  Data: 1.356 (1.356)
05/08/2023 15:32:15 - INFO - train -   Train: 23 [  50/52 ( 98%)]  Loss: 2.715 (2.73)  Time: 0.261s, 1958.98/s  (0.291s, 1758.39/s)  LR: 3.532e-03  Data: 0.014 (0.040)
05/08/2023 15:32:15 - INFO - train -   Train: 23 [  51/52 (100%)]  Loss: 2.803 (2.75)  Time: 0.258s, 1984.56/s  (0.291s, 1762.25/s)  LR: 3.532e-03  Data: 0.000 (0.039)
05/08/2023 15:32:15 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:32:18 - INFO - train -   Train: 24 [   0/52 (  0%)]  Loss: 2.843 (2.84)  Time: 1.566s,  326.89/s  (1.566s,  326.89/s)  LR: 2.626e-03  Data: 1.313 (1.313)
05/08/2023 15:32:32 - INFO - train -   Train: 24 [  50/52 ( 98%)]  Loss: 2.829 (2.84)  Time: 0.263s, 1947.11/s  (0.291s, 1760.98/s)  LR: 2.626e-03  Data: 0.015 (0.039)
05/08/2023 15:32:32 - INFO - train -   Train: 24 [  51/52 (100%)]  Loss: 2.841 (2.84)  Time: 0.239s, 2143.44/s  (0.290s, 1767.05/s)  LR: 2.626e-03  Data: 0.000 (0.039)
05/08/2023 15:32:32 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:32:35 - INFO - train -   Train: 25 [   0/52 (  0%)]  Loss: 2.761 (2.76)  Time: 1.574s,  325.31/s  (1.574s,  325.31/s)  LR: 1.842e-03  Data: 1.308 (1.308)
05/08/2023 15:32:49 - INFO - train -   Train: 25 [  50/52 ( 98%)]  Loss: 2.756 (2.76)  Time: 0.274s, 1865.61/s  (0.292s, 1753.59/s)  LR: 1.842e-03  Data: 0.015 (0.039)
05/08/2023 15:32:49 - INFO - train -   Train: 25 [  51/52 (100%)]  Loss: 2.727 (2.75)  Time: 0.249s, 2060.27/s  (0.291s, 1758.62/s)  LR: 1.842e-03  Data: 0.000 (0.038)
05/08/2023 15:32:49 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:32:53 - INFO - train -   Train: 26 [   0/52 (  0%)]  Loss: 2.891 (2.89)  Time: 1.574s,  325.29/s  (1.574s,  325.29/s)  LR: 1.189e-03  Data: 1.322 (1.322)
05/08/2023 15:33:06 - INFO - train -   Train: 26 [  50/52 ( 98%)]  Loss: 2.776 (2.83)  Time: 0.262s, 1956.23/s  (0.290s, 1763.50/s)  LR: 1.189e-03  Data: 0.014 (0.040)
05/08/2023 15:33:06 - INFO - train -   Train: 26 [  51/52 (100%)]  Loss: 2.738 (2.80)  Time: 0.248s, 2064.76/s  (0.290s, 1768.47/s)  LR: 1.189e-03  Data: 0.000 (0.039)
05/08/2023 15:33:06 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:33:10 - INFO - train -   Train: 27 [   0/52 (  0%)]  Loss: 2.775 (2.77)  Time: 1.567s,  326.78/s  (1.567s,  326.78/s)  LR: 6.730e-04  Data: 1.301 (1.301)
05/08/2023 15:33:23 - INFO - train -   Train: 27 [  50/52 ( 98%)]  Loss: 2.796 (2.79)  Time: 0.262s, 1951.56/s  (0.290s, 1762.70/s)  LR: 6.730e-04  Data: 0.014 (0.039)
05/08/2023 15:33:23 - INFO - train -   Train: 27 [  51/52 (100%)]  Loss: 2.825 (2.80)  Time: 0.257s, 1992.54/s  (0.290s, 1766.62/s)  LR: 6.730e-04  Data: 0.000 (0.038)
05/08/2023 15:33:23 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:33:26 - INFO - train -   Train: 28 [   0/52 (  0%)]  Loss: 2.802 (2.80)  Time: 1.466s,  349.35/s  (1.466s,  349.35/s)  LR: 3.005e-04  Data: 1.213 (1.213)
05/08/2023 15:33:40 - INFO - train -   Train: 28 [  50/52 ( 98%)]  Loss: 2.928 (2.86)  Time: 0.254s, 2013.99/s  (0.289s, 1769.85/s)  LR: 3.005e-04  Data: 0.014 (0.038)
05/08/2023 15:33:40 - INFO - train -   Train: 28 [  51/52 (100%)]  Loss: 2.700 (2.81)  Time: 0.248s, 2064.30/s  (0.288s, 1774.72/s)  LR: 3.005e-04  Data: 0.000 (0.037)
05/08/2023 15:33:40 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:33:43 - INFO - train -   Train: 29 [   0/52 (  0%)]  Loss: 2.804 (2.80)  Time: 1.558s,  328.64/s  (1.558s,  328.64/s)  LR: 7.532e-05  Data: 1.303 (1.303)
05/08/2023 15:33:57 - INFO - train -   Train: 29 [  50/52 ( 98%)]  Loss: 2.694 (2.75)  Time: 0.274s, 1871.79/s  (0.292s, 1753.00/s)  LR: 7.532e-05  Data: 0.014 (0.040)
05/08/2023 15:33:57 - INFO - train -   Train: 29 [  51/52 (100%)]  Loss: 2.753 (2.75)  Time: 0.257s, 1992.48/s  (0.291s, 1757.06/s)  LR: 7.532e-05  Data: 0.000 (0.039)
05/08/2023 15:33:57 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 15:33:57 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 15:33:57 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 15:33:57 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 15:33:59 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 15:34:01 - INFO - train -   Test: [   0/19]  Time: 2.239 (2.239)  Loss:  1.3311 (1.3311)  Acc@1: 66.0156 (66.0156)  Acc@5: 98.4375 (98.4375)
05/08/2023 15:34:03 - INFO - train -   Test: [  19/19]  Time: 0.149 (0.224)  Loss:  1.3213 (1.3142)  Acc@1: 66.1765 (66.5800)  Acc@5: 98.5294 (98.7800)
05/08/2023 15:34:03 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 15:34:03 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 15:34:03 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 15:34:03 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 15:34:04 - INFO - train -   Test: [   0/19]  Time: 0.890 (0.890)  Loss:  0.7588 (0.7588)  Acc@1: 86.1328 (86.1328)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:05 - INFO - train -   Test: [  19/19]  Time: 0.016 (0.098)  Loss:  0.7598 (0.7588)  Acc@1: 83.8235 (85.0500)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:05 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 15:34:05 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 15:34:05 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 15:34:05 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 15:34:06 - INFO - train -   Test: [   0/19]  Time: 0.854 (0.854)  Loss:  0.8223 (0.8223)  Acc@1: 81.4453 (81.4453)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:07 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 15:34:07 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 15:34:07 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 15:34:07 - INFO - train -   Test: [  19/19]  Time: 0.029 (0.093)  Loss:  0.7822 (0.7972)  Acc@1: 86.3971 (83.5200)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:07 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 15:34:08 - INFO - train -   Test: [   0/19]  Time: 0.843 (0.843)  Loss:  0.7998 (0.7998)  Acc@1: 82.2266 (82.2266)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:09 - INFO - train -   Test: [  19/19]  Time: 0.015 (0.093)  Loss:  0.7715 (0.7870)  Acc@1: 86.7647 (83.4100)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:09 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 15:34:09 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 15:34:09 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 15:34:09 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 15:34:10 - INFO - train -   Test: [   0/19]  Time: 0.830 (0.830)  Loss:  0.8442 (0.8442)  Acc@1: 81.0547 (81.0547)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:12 - INFO - train -   Test: [  19/19]  Time: 0.014 (0.095)  Loss:  0.8467 (0.8553)  Acc@1: 80.5147 (80.8500)  Acc@5: 100.0000 (99.9900)
05/08/2023 15:34:12 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 15:34:12 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 15:34:12 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 15:34:12 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 15:34:12 - INFO - train -   Test: [   0/19]  Time: 0.790 (0.790)  Loss:  0.8652 (0.8652)  Acc@1: 84.9609 (84.9609)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:13 - INFO - train -   Test: [  19/19]  Time: 0.028 (0.092)  Loss:  0.8657 (0.8725)  Acc@1: 88.2353 (86.2500)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:13 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 15:34:13 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 15:34:13 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 15:34:13 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 15:34:14 - INFO - train -   Test: [   0/19]  Time: 0.829 (0.829)  Loss:  0.9258 (0.9258)  Acc@1: 77.9297 (77.9297)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:15 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 15:34:15 - INFO - train -   Test: [  19/19]  Time: 0.013 (0.091)  Loss:  0.9414 (0.9259)  Acc@1: 77.5735 (78.2200)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:15 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 15:34:15 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 15:34:15 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 15:34:16 - INFO - train -   Test: [   0/19]  Time: 0.827 (0.827)  Loss:  0.9648 (0.9648)  Acc@1: 76.5625 (76.5625)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:17 - INFO - train -   Test: [  19/19]  Time: 0.013 (0.089)  Loss:  0.9873 (0.9611)  Acc@1: 75.0000 (76.4700)  Acc@5: 100.0000 (99.9900)
05/08/2023 15:34:17 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 15:34:17 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 15:34:17 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 15:34:17 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 15:34:18 - INFO - train -   Test: [   0/19]  Time: 0.821 (0.821)  Loss:  1.1006 (1.1006)  Acc@1: 74.2188 (74.2188)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:19 - INFO - train -   Test: [  19/19]  Time: 0.012 (0.089)  Loss:  1.1035 (1.0980)  Acc@1: 78.6765 (74.0600)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:19 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 15:34:19 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 15:34:19 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 15:34:19 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 15:34:20 - INFO - train -   Test: [   0/19]  Time: 0.790 (0.790)  Loss:  1.2598 (1.2598)  Acc@1: 70.5078 (70.5078)  Acc@5: 100.0000 (100.0000)
05/08/2023 15:34:21 - INFO - train -   Test: [  19/19]  Time: 0.011 (0.088)  Loss:  1.2754 (1.2611)  Acc@1: 70.9559 (70.8800)  Acc@5: 100.0000 (100.0000)
