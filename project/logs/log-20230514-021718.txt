05/14/2023 02:17:18 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 02:17:18 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 02:17:24 - INFO - train -   Model resnet18 created, param count:77191760
05/14/2023 02:19:34 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 02:19:34 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 02:19:46 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 02:19:56 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 6.599 (6.60)  Time: 9.474s,   27.02/s  (9.474s,   27.02/s)  LR: 5.500e-06  Data: 1.077 (1.077)
05/14/2023 02:20:19 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 7.017 (6.81)  Time: 0.468s,  547.28/s  (0.640s,  400.15/s)  LR: 5.500e-06  Data: 0.011 (0.032)
05/14/2023 02:20:42 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 7.386 (7.00)  Time: 0.457s,  560.13/s  (0.548s,  466.99/s)  LR: 5.500e-06  Data: 0.011 (0.022)
05/14/2023 02:20:44 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.699 (6.93)  Time: 0.448s,  571.03/s  (0.543s,  471.05/s)  LR: 5.500e-06  Data: 0.000 (0.021)
05/14/2023 02:20:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:20:48 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.708 (6.71)  Time: 1.011s,  253.32/s  (1.011s,  253.32/s)  LR: 5.504e-03  Data: 0.547 (0.547)
05/14/2023 02:21:11 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 6.287 (6.50)  Time: 0.399s,  641.25/s  (0.461s,  555.05/s)  LR: 5.504e-03  Data: 0.010 (0.021)
05/14/2023 02:21:34 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 5.807 (6.27)  Time: 0.481s,  532.29/s  (0.458s,  559.49/s)  LR: 5.504e-03  Data: 0.010 (0.016)
05/14/2023 02:21:35 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 5.969 (6.19)  Time: 0.448s,  570.91/s  (0.457s,  559.97/s)  LR: 5.504e-03  Data: 0.000 (0.016)
05/14/2023 02:21:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:21:40 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 5.724 (5.72)  Time: 1.179s,  217.09/s  (1.179s,  217.09/s)  LR: 1.100e-02  Data: 0.697 (0.697)
05/14/2023 02:22:02 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.801 (5.26)  Time: 0.468s,  547.56/s  (0.461s,  554.72/s)  LR: 1.100e-02  Data: 0.011 (0.024)
05/14/2023 02:22:25 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.410 (4.98)  Time: 0.471s,  543.92/s  (0.457s,  560.38/s)  LR: 1.100e-02  Data: 0.012 (0.018)
05/14/2023 02:22:27 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.439 (4.84)  Time: 0.462s,  553.52/s  (0.456s,  560.85/s)  LR: 1.100e-02  Data: 0.000 (0.018)
05/14/2023 02:22:27 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:22:31 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 4.510 (4.51)  Time: 1.046s,  244.72/s  (1.046s,  244.72/s)  LR: 1.650e-02  Data: 0.567 (0.567)
05/14/2023 02:22:54 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 4.191 (4.35)  Time: 0.480s,  533.86/s  (0.468s,  547.54/s)  LR: 1.650e-02  Data: 0.011 (0.022)
05/14/2023 02:23:17 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 4.174 (4.29)  Time: 0.450s,  568.60/s  (0.460s,  556.61/s)  LR: 1.650e-02  Data: 0.011 (0.017)
05/14/2023 02:23:19 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 4.207 (4.27)  Time: 0.465s,  550.21/s  (0.460s,  556.95/s)  LR: 1.650e-02  Data: 0.000 (0.016)
05/14/2023 02:23:19 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:23:23 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 4.160 (4.16)  Time: 0.970s,  263.81/s  (0.970s,  263.81/s)  LR: 2.200e-02  Data: 0.564 (0.564)
05/14/2023 02:23:45 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 3.972 (4.07)  Time: 0.451s,  567.61/s  (0.460s,  556.95/s)  LR: 2.200e-02  Data: 0.010 (0.022)
05/14/2023 02:24:08 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 4.910 (4.35)  Time: 0.471s,  543.32/s  (0.456s,  561.00/s)  LR: 2.200e-02  Data: 0.011 (0.016)
05/14/2023 02:24:10 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 3.923 (4.24)  Time: 0.438s,  584.51/s  (0.456s,  561.29/s)  LR: 2.200e-02  Data: 0.000 (0.016)
05/14/2023 02:24:10 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:24:14 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 3.769 (3.77)  Time: 1.033s,  247.83/s  (1.033s,  247.83/s)  LR: 2.566e-02  Data: 0.594 (0.594)
05/14/2023 02:24:37 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 3.788 (3.78)  Time: 0.416s,  614.89/s  (0.460s,  557.09/s)  LR: 2.566e-02  Data: 0.011 (0.022)
05/14/2023 02:24:59 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 3.767 (3.77)  Time: 0.459s,  557.73/s  (0.454s,  563.64/s)  LR: 2.566e-02  Data: 0.011 (0.017)
05/14/2023 02:25:01 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 3.833 (3.79)  Time: 0.474s,  539.76/s  (0.455s,  563.20/s)  LR: 2.566e-02  Data: 0.000 (0.016)
05/14/2023 02:25:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:25:05 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 3.844 (3.84)  Time: 1.017s,  251.84/s  (1.017s,  251.84/s)  LR: 2.487e-02  Data: 0.556 (0.556)
05/14/2023 02:25:28 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 3.679 (3.76)  Time: 0.436s,  587.38/s  (0.460s,  556.17/s)  LR: 2.487e-02  Data: 0.011 (0.022)
05/14/2023 02:25:51 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 3.556 (3.69)  Time: 0.468s,  546.82/s  (0.457s,  560.07/s)  LR: 2.487e-02  Data: 0.010 (0.017)
05/14/2023 02:25:52 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 3.342 (3.61)  Time: 0.459s,  557.48/s  (0.456s,  561.34/s)  LR: 2.487e-02  Data: 0.000 (0.016)
05/14/2023 02:25:52 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:25:57 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 3.611 (3.61)  Time: 1.017s,  251.81/s  (1.017s,  251.81/s)  LR: 2.397e-02  Data: 0.578 (0.578)
05/14/2023 02:26:19 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 3.441 (3.53)  Time: 0.452s,  566.55/s  (0.463s,  553.41/s)  LR: 2.397e-02  Data: 0.012 (0.022)
05/14/2023 02:26:42 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 3.527 (3.53)  Time: 0.414s,  618.02/s  (0.457s,  559.93/s)  LR: 2.397e-02  Data: 0.011 (0.017)
05/14/2023 02:26:43 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 3.396 (3.49)  Time: 0.411s,  623.55/s  (0.455s,  562.29/s)  LR: 2.397e-02  Data: 0.000 (0.017)
05/14/2023 02:26:43 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:26:48 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 3.349 (3.35)  Time: 1.032s,  247.96/s  (1.032s,  247.96/s)  LR: 2.295e-02  Data: 0.579 (0.579)
05/14/2023 02:27:11 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 3.481 (3.42)  Time: 0.447s,  572.51/s  (0.463s,  552.47/s)  LR: 2.295e-02  Data: 0.014 (0.022)
05/14/2023 02:27:33 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 3.461 (3.43)  Time: 0.426s,  601.48/s  (0.455s,  562.77/s)  LR: 2.295e-02  Data: 0.011 (0.017)
05/14/2023 02:27:35 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 3.424 (3.43)  Time: 0.432s,  592.90/s  (0.454s,  563.57/s)  LR: 2.295e-02  Data: 0.000 (0.016)
05/14/2023 02:27:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:27:39 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 3.459 (3.46)  Time: 1.043s,  245.49/s  (1.043s,  245.49/s)  LR: 2.183e-02  Data: 0.559 (0.559)
05/14/2023 02:28:02 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 3.329 (3.39)  Time: 0.407s,  629.50/s  (0.461s,  555.10/s)  LR: 2.183e-02  Data: 0.012 (0.022)
05/14/2023 02:28:24 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 3.394 (3.39)  Time: 0.395s,  647.67/s  (0.455s,  562.25/s)  LR: 2.183e-02  Data: 0.011 (0.017)
05/14/2023 02:28:26 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 3.213 (3.35)  Time: 0.468s,  546.73/s  (0.456s,  561.85/s)  LR: 2.183e-02  Data: 0.000 (0.016)
05/14/2023 02:28:26 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:28:30 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 3.303 (3.30)  Time: 1.046s,  244.65/s  (1.046s,  244.65/s)  LR: 2.063e-02  Data: 0.607 (0.607)
05/14/2023 02:28:53 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 3.256 (3.28)  Time: 0.447s,  572.67/s  (0.459s,  558.30/s)  LR: 2.063e-02  Data: 0.011 (0.023)
05/14/2023 02:29:15 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 3.199 (3.25)  Time: 0.473s,  541.04/s  (0.453s,  565.27/s)  LR: 2.063e-02  Data: 0.012 (0.017)
05/14/2023 02:29:17 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 3.711 (3.37)  Time: 0.434s,  590.14/s  (0.453s,  565.49/s)  LR: 2.063e-02  Data: 0.000 (0.017)
05/14/2023 02:29:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:29:21 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 3.214 (3.21)  Time: 1.119s,  228.68/s  (1.119s,  228.68/s)  LR: 1.934e-02  Data: 0.607 (0.607)
05/14/2023 02:29:44 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 3.231 (3.22)  Time: 0.404s,  634.24/s  (0.471s,  543.90/s)  LR: 1.934e-02  Data: 0.012 (0.023)
05/14/2023 02:30:07 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 3.502 (3.32)  Time: 0.467s,  548.02/s  (0.462s,  554.38/s)  LR: 1.934e-02  Data: 0.011 (0.017)
05/14/2023 02:30:09 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 3.621 (3.39)  Time: 0.402s,  636.15/s  (0.461s,  555.55/s)  LR: 1.934e-02  Data: 0.000 (0.017)
05/14/2023 02:30:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:30:13 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 3.214 (3.21)  Time: 1.071s,  239.07/s  (1.071s,  239.07/s)  LR: 1.800e-02  Data: 0.569 (0.569)
05/14/2023 02:30:36 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 3.184 (3.20)  Time: 0.457s,  560.05/s  (0.467s,  548.51/s)  LR: 1.800e-02  Data: 0.012 (0.022)
05/14/2023 02:30:58 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 3.204 (3.20)  Time: 0.416s,  614.76/s  (0.458s,  558.55/s)  LR: 1.800e-02  Data: 0.011 (0.017)
05/14/2023 02:31:00 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 3.175 (3.19)  Time: 0.458s,  558.44/s  (0.457s,  559.76/s)  LR: 1.800e-02  Data: 0.000 (0.017)
05/14/2023 02:31:00 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:31:05 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 3.073 (3.07)  Time: 1.046s,  244.76/s  (1.046s,  244.76/s)  LR: 1.661e-02  Data: 0.589 (0.589)
05/14/2023 02:31:27 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 3.044 (3.06)  Time: 0.428s,  597.50/s  (0.456s,  561.45/s)  LR: 1.661e-02  Data: 0.011 (0.022)
05/14/2023 02:31:49 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 3.189 (3.10)  Time: 0.430s,  595.41/s  (0.455s,  563.24/s)  LR: 1.661e-02  Data: 0.010 (0.017)
05/14/2023 02:31:51 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 3.110 (3.10)  Time: 0.379s,  675.58/s  (0.454s,  563.67/s)  LR: 1.661e-02  Data: 0.000 (0.017)
05/14/2023 02:31:51 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:31:56 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 2.944 (2.94)  Time: 1.086s,  235.69/s  (1.086s,  235.69/s)  LR: 1.519e-02  Data: 0.628 (0.628)
05/14/2023 02:32:18 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 3.016 (2.98)  Time: 0.471s,  543.20/s  (0.465s,  550.02/s)  LR: 1.519e-02  Data: 0.015 (0.024)
05/14/2023 02:32:41 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 3.140 (3.03)  Time: 0.444s,  576.79/s  (0.461s,  555.60/s)  LR: 1.519e-02  Data: 0.011 (0.017)
05/14/2023 02:32:43 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 2.995 (3.02)  Time: 0.448s,  570.89/s  (0.460s,  556.63/s)  LR: 1.519e-02  Data: 0.000 (0.017)
05/14/2023 02:32:43 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:32:47 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 3.306 (3.31)  Time: 1.010s,  253.34/s  (1.010s,  253.34/s)  LR: 1.375e-02  Data: 0.526 (0.526)
05/14/2023 02:33:10 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 3.108 (3.21)  Time: 0.433s,  591.85/s  (0.461s,  555.09/s)  LR: 1.375e-02  Data: 0.013 (0.021)
05/14/2023 02:33:32 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 3.112 (3.18)  Time: 0.471s,  543.76/s  (0.457s,  559.81/s)  LR: 1.375e-02  Data: 0.011 (0.016)
05/14/2023 02:33:34 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 2.907 (3.11)  Time: 0.448s,  571.55/s  (0.457s,  560.12/s)  LR: 1.375e-02  Data: 0.000 (0.016)
05/14/2023 02:33:34 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:33:39 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 2.984 (2.98)  Time: 1.022s,  250.56/s  (1.022s,  250.56/s)  LR: 1.231e-02  Data: 0.598 (0.598)
05/14/2023 02:34:01 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 2.906 (2.95)  Time: 0.481s,  532.27/s  (0.464s,  551.83/s)  LR: 1.231e-02  Data: 0.010 (0.023)
05/14/2023 02:34:24 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 3.086 (2.99)  Time: 0.472s,  542.40/s  (0.456s,  561.49/s)  LR: 1.231e-02  Data: 0.011 (0.017)
05/14/2023 02:34:25 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 3.579 (3.14)  Time: 0.408s,  627.53/s  (0.455s,  562.02/s)  LR: 1.231e-02  Data: 0.000 (0.017)
05/14/2023 02:34:25 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:34:30 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 3.113 (3.11)  Time: 1.024s,  250.04/s  (1.024s,  250.04/s)  LR: 1.089e-02  Data: 0.543 (0.543)
05/14/2023 02:34:53 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.963 (3.04)  Time: 0.440s,  581.77/s  (0.463s,  552.54/s)  LR: 1.089e-02  Data: 0.011 (0.022)
05/14/2023 02:35:15 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 3.321 (3.13)  Time: 0.480s,  533.79/s  (0.461s,  555.78/s)  LR: 1.089e-02  Data: 0.011 (0.017)
05/14/2023 02:35:17 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 2.921 (3.08)  Time: 0.470s,  544.19/s  (0.460s,  556.00/s)  LR: 1.089e-02  Data: 0.000 (0.016)
05/14/2023 02:35:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:35:22 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 3.034 (3.03)  Time: 1.104s,  231.83/s  (1.104s,  231.83/s)  LR: 9.501e-03  Data: 0.649 (0.649)
05/14/2023 02:35:45 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 3.051 (3.04)  Time: 0.421s,  608.02/s  (0.467s,  548.41/s)  LR: 9.501e-03  Data: 0.013 (0.024)
05/14/2023 02:36:08 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 3.038 (3.04)  Time: 0.406s,  630.38/s  (0.461s,  555.90/s)  LR: 9.501e-03  Data: 0.010 (0.018)
05/14/2023 02:36:09 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 3.005 (3.03)  Time: 0.423s,  605.27/s  (0.459s,  557.64/s)  LR: 9.501e-03  Data: 0.000 (0.017)
05/14/2023 02:36:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:36:14 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 2.891 (2.89)  Time: 1.004s,  254.91/s  (1.004s,  254.91/s)  LR: 8.157e-03  Data: 0.560 (0.560)
05/14/2023 02:36:36 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 2.855 (2.87)  Time: 0.469s,  545.44/s  (0.461s,  555.03/s)  LR: 8.157e-03  Data: 0.011 (0.022)
05/14/2023 02:36:59 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 2.799 (2.85)  Time: 0.470s,  544.79/s  (0.455s,  562.15/s)  LR: 8.157e-03  Data: 0.011 (0.017)
05/14/2023 02:37:01 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 2.844 (2.85)  Time: 0.433s,  590.97/s  (0.455s,  563.22/s)  LR: 8.157e-03  Data: 0.000 (0.017)
05/14/2023 02:37:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:37:05 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 2.804 (2.80)  Time: 1.035s,  247.36/s  (1.035s,  247.36/s)  LR: 6.875e-03  Data: 0.583 (0.583)
05/14/2023 02:37:28 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 2.894 (2.85)  Time: 0.421s,  607.88/s  (0.459s,  557.85/s)  LR: 6.875e-03  Data: 0.011 (0.023)
05/14/2023 02:37:50 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 3.032 (2.91)  Time: 0.468s,  547.01/s  (0.458s,  559.05/s)  LR: 6.875e-03  Data: 0.011 (0.017)
05/14/2023 02:37:52 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 2.895 (2.91)  Time: 0.430s,  595.48/s  (0.458s,  559.15/s)  LR: 6.875e-03  Data: 0.000 (0.017)
05/14/2023 02:37:52 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:37:57 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 3.024 (3.02)  Time: 0.964s,  265.46/s  (0.964s,  265.46/s)  LR: 5.668e-03  Data: 0.528 (0.528)
05/14/2023 02:38:20 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 3.154 (3.09)  Time: 0.478s,  535.62/s  (0.462s,  554.56/s)  LR: 5.668e-03  Data: 0.011 (0.022)
05/14/2023 02:38:42 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 3.293 (3.16)  Time: 0.486s,  526.78/s  (0.457s,  559.61/s)  LR: 5.668e-03  Data: 0.011 (0.017)
05/14/2023 02:38:44 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 3.346 (3.20)  Time: 0.412s,  621.37/s  (0.457s,  559.66/s)  LR: 5.668e-03  Data: 0.000 (0.017)
05/14/2023 02:38:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:38:49 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.962 (2.96)  Time: 1.061s,  241.33/s  (1.061s,  241.33/s)  LR: 4.549e-03  Data: 0.594 (0.594)
05/14/2023 02:39:12 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 2.966 (2.96)  Time: 0.410s,  624.95/s  (0.466s,  549.15/s)  LR: 4.549e-03  Data: 0.011 (0.024)
05/14/2023 02:39:35 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 2.900 (2.94)  Time: 0.440s,  582.30/s  (0.459s,  557.88/s)  LR: 4.549e-03  Data: 0.011 (0.018)
05/14/2023 02:39:36 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 3.461 (3.07)  Time: 0.465s,  550.81/s  (0.458s,  558.57/s)  LR: 4.549e-03  Data: 0.000 (0.017)
05/14/2023 02:39:36 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:39:41 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 2.939 (2.94)  Time: 1.006s,  254.57/s  (1.006s,  254.57/s)  LR: 3.532e-03  Data: 0.571 (0.571)
05/14/2023 02:40:03 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 2.737 (2.84)  Time: 0.474s,  540.04/s  (0.458s,  558.61/s)  LR: 3.532e-03  Data: 0.012 (0.023)
05/14/2023 02:40:26 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 2.863 (2.85)  Time: 0.482s,  530.74/s  (0.456s,  561.50/s)  LR: 3.532e-03  Data: 0.011 (0.017)
05/14/2023 02:40:28 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 3.253 (2.95)  Time: 0.468s,  546.43/s  (0.456s,  561.04/s)  LR: 3.532e-03  Data: 0.000 (0.017)
05/14/2023 02:40:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:40:33 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 2.886 (2.89)  Time: 0.970s,  263.89/s  (0.970s,  263.89/s)  LR: 2.626e-03  Data: 0.518 (0.518)
05/14/2023 02:40:55 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 3.010 (2.95)  Time: 0.471s,  543.46/s  (0.456s,  561.69/s)  LR: 2.626e-03  Data: 0.011 (0.021)
05/14/2023 02:41:17 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 2.977 (2.96)  Time: 0.484s,  528.75/s  (0.452s,  565.85/s)  LR: 2.626e-03  Data: 0.011 (0.017)
05/14/2023 02:41:19 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 3.380 (3.06)  Time: 0.462s,  553.76/s  (0.452s,  566.45/s)  LR: 2.626e-03  Data: 0.000 (0.016)
05/14/2023 02:41:19 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:41:24 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 2.856 (2.86)  Time: 1.100s,  232.66/s  (1.100s,  232.66/s)  LR: 1.842e-03  Data: 0.618 (0.618)
05/14/2023 02:41:46 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 2.846 (2.85)  Time: 0.439s,  582.51/s  (0.460s,  556.95/s)  LR: 1.842e-03  Data: 0.011 (0.023)
05/14/2023 02:42:09 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 2.804 (2.84)  Time: 0.447s,  573.26/s  (0.458s,  559.24/s)  LR: 1.842e-03  Data: 0.012 (0.017)
05/14/2023 02:42:11 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 3.126 (2.91)  Time: 0.487s,  525.24/s  (0.458s,  558.87/s)  LR: 1.842e-03  Data: 0.000 (0.017)
05/14/2023 02:42:11 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:42:15 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 2.843 (2.84)  Time: 0.959s,  267.03/s  (0.959s,  267.03/s)  LR: 1.189e-03  Data: 0.517 (0.517)
05/14/2023 02:42:38 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 2.848 (2.85)  Time: 0.411s,  623.05/s  (0.462s,  554.22/s)  LR: 1.189e-03  Data: 0.011 (0.021)
05/14/2023 02:43:00 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 3.002 (2.90)  Time: 0.407s,  629.41/s  (0.458s,  558.80/s)  LR: 1.189e-03  Data: 0.011 (0.016)
05/14/2023 02:43:02 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 3.688 (3.10)  Time: 0.458s,  558.65/s  (0.457s,  559.60/s)  LR: 1.189e-03  Data: 0.000 (0.016)
05/14/2023 02:43:02 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:43:07 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 2.987 (2.99)  Time: 1.077s,  237.70/s  (1.077s,  237.70/s)  LR: 6.730e-04  Data: 0.623 (0.623)
05/14/2023 02:43:30 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 2.905 (2.95)  Time: 0.415s,  616.46/s  (0.468s,  547.26/s)  LR: 6.730e-04  Data: 0.010 (0.023)
05/14/2023 02:43:53 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 2.864 (2.92)  Time: 0.442s,  579.44/s  (0.461s,  555.69/s)  LR: 6.730e-04  Data: 0.012 (0.017)
05/14/2023 02:43:54 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 2.943 (2.92)  Time: 0.430s,  595.88/s  (0.460s,  556.57/s)  LR: 6.730e-04  Data: 0.000 (0.017)
05/14/2023 02:43:54 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:43:59 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 3.019 (3.02)  Time: 1.023s,  250.33/s  (1.023s,  250.33/s)  LR: 3.005e-04  Data: 0.603 (0.603)
05/14/2023 02:44:21 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 2.902 (2.96)  Time: 0.474s,  540.61/s  (0.461s,  555.54/s)  LR: 3.005e-04  Data: 0.010 (0.023)
05/14/2023 02:44:43 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 2.944 (2.96)  Time: 0.434s,  589.97/s  (0.454s,  564.48/s)  LR: 3.005e-04  Data: 0.011 (0.017)
05/14/2023 02:44:45 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 2.938 (2.95)  Time: 0.447s,  573.26/s  (0.454s,  564.38/s)  LR: 3.005e-04  Data: 0.000 (0.017)
05/14/2023 02:44:45 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:44:49 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 3.135 (3.14)  Time: 0.975s,  262.63/s  (0.975s,  262.63/s)  LR: 7.532e-05  Data: 0.481 (0.481)
05/14/2023 02:45:12 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 2.912 (3.02)  Time: 0.422s,  606.44/s  (0.458s,  558.97/s)  LR: 7.532e-05  Data: 0.014 (0.021)
05/14/2023 02:45:34 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 2.840 (2.96)  Time: 0.441s,  580.82/s  (0.452s,  565.97/s)  LR: 7.532e-05  Data: 0.011 (0.016)
05/14/2023 02:45:36 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 3.026 (2.98)  Time: 0.456s,  560.80/s  (0.452s,  566.43/s)  LR: 7.532e-05  Data: 0.000 (0.016)
05/14/2023 02:45:36 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:45:36 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 02:45:39 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 02:45:41 - INFO - train -   Test: [   0/39]  Time: 1.368 (1.368)  Loss:  1.8750 (1.8750)  Acc@1: 54.2969 (54.2969)  Acc@5: 98.4375 (98.4375)
05/14/2023 02:45:44 - INFO - train -   Test: [  39/39]  Time: 0.198 (0.108)  Loss:  1.4951 (1.9610)  Acc@1: 68.7500 (54.6500)  Acc@5: 100.0000 (98.8300)
05/14/2023 02:45:44 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 02:45:44 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 02:45:45 - INFO - train -   Test: [   0/39]  Time: 0.606 (0.606)  Loss:  0.7539 (0.7539)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:45:49 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 02:45:49 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.101)  Loss:  0.6709 (0.7626)  Acc@1: 100.0000 (98.6300)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:45:49 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 02:45:50 - INFO - train -   Test: [   0/39]  Time: 0.632 (0.632)  Loss:  0.5952 (0.5952)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:45:53 - INFO - train -   Test: [  39/39]  Time: 0.047 (0.103)  Loss:  0.6934 (0.5385)  Acc@1: 93.7500 (96.3300)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:45:53 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 02:45:53 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 02:45:54 - INFO - train -   Test: [   0/39]  Time: 0.600 (0.600)  Loss:  0.6895 (0.6895)  Acc@1: 94.9219 (94.9219)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:45:58 - INFO - train -   Test: [  39/39]  Time: 0.022 (0.102)  Loss:  0.4062 (0.6220)  Acc@1: 100.0000 (96.7000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:45:58 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 02:45:58 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 02:45:59 - INFO - train -   Test: [   0/39]  Time: 0.714 (0.714)  Loss:  0.4121 (0.4121)  Acc@1: 98.8281 (98.8281)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:02 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.105)  Loss:  0.2822 (0.4107)  Acc@1: 100.0000 (98.8600)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:02 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 02:46:02 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 02:46:03 - INFO - train -   Test: [   0/39]  Time: 0.622 (0.622)  Loss:  0.9526 (0.9526)  Acc@1: 77.7344 (77.7344)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:07 - INFO - train -   Test: [  39/39]  Time: 0.052 (0.104)  Loss:  1.0146 (0.9282)  Acc@1: 62.5000 (75.9200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:07 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 02:46:07 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 02:46:08 - INFO - train -   Test: [   0/39]  Time: 0.651 (0.651)  Loss:  0.8867 (0.8867)  Acc@1: 75.7812 (75.7812)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:11 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.103)  Loss:  0.8262 (0.8805)  Acc@1: 81.2500 (76.8600)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:11 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 02:46:11 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 02:46:12 - INFO - train -   Test: [   0/39]  Time: 0.655 (0.655)  Loss:  0.9248 (0.9248)  Acc@1: 73.4375 (73.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:16 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 02:46:16 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.102)  Loss:  0.9834 (0.8922)  Acc@1: 75.0000 (77.4500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:16 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 02:46:17 - INFO - train -   Test: [   0/39]  Time: 0.618 (0.618)  Loss:  0.8389 (0.8389)  Acc@1: 91.0156 (91.0156)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:20 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 02:46:20 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.100)  Loss:  0.8198 (0.7886)  Acc@1: 93.7500 (92.3200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:20 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 02:46:21 - INFO - train -   Test: [   0/39]  Time: 0.693 (0.693)  Loss:  0.8472 (0.8472)  Acc@1: 83.2031 (83.2031)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:25 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.099)  Loss:  0.9248 (0.8150)  Acc@1: 75.0000 (82.6500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:25 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 02:46:25 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 02:46:26 - INFO - train -   Test: [   0/39]  Time: 0.643 (0.643)  Loss:  0.9810 (0.9810)  Acc@1: 76.9531 (76.9531)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:29 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.096)  Loss:  1.0244 (0.9365)  Acc@1: 75.0000 (76.9100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:29 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 02:46:29 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 02:46:30 - INFO - train -   Test: [   0/39]  Time: 0.630 (0.630)  Loss:  1.0176 (1.0176)  Acc@1: 73.0469 (73.0469)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:33 - INFO - train -   Test: [  39/39]  Time: 0.020 (0.096)  Loss:  1.1270 (0.9924)  Acc@1: 75.0000 (74.2200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:33 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 02:46:33 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 02:46:34 - INFO - train -   Test: [   0/39]  Time: 0.574 (0.574)  Loss:  1.1074 (1.1074)  Acc@1: 69.9219 (69.9219)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:37 - INFO - train -   Test: [  39/39]  Time: 0.021 (0.093)  Loss:  0.8716 (1.0680)  Acc@1: 87.5000 (71.9500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:37 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 02:46:37 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 02:46:38 - INFO - train -   Test: [   0/39]  Time: 0.698 (0.698)  Loss:  1.1133 (1.1133)  Acc@1: 72.6562 (72.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:41 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 02:46:41 - INFO - train -   Test: [  39/39]  Time: 0.021 (0.095)  Loss:  1.2539 (1.0627)  Acc@1: 75.0000 (74.1800)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:41 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 02:46:42 - INFO - train -   Test: [   0/39]  Time: 0.622 (0.622)  Loss:  1.0957 (1.0957)  Acc@1: 74.2188 (74.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:45 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.091)  Loss:  1.2188 (1.0142)  Acc@1: 56.2500 (76.6500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:45 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 02:46:45 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 02:46:46 - INFO - train -   Test: [   0/39]  Time: 0.660 (0.660)  Loss:  0.9404 (0.9404)  Acc@1: 82.8125 (82.8125)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:49 - INFO - train -   Test: [  39/39]  Time: 0.018 (0.090)  Loss:  1.0469 (0.9051)  Acc@1: 81.2500 (82.7400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:49 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 02:46:49 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 02:46:50 - INFO - train -   Test: [   0/39]  Time: 0.636 (0.636)  Loss:  0.9951 (0.9951)  Acc@1: 83.2031 (83.2031)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:53 - INFO - train -   Test: [  39/39]  Time: 0.020 (0.091)  Loss:  1.1504 (0.9834)  Acc@1: 62.5000 (81.4700)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:53 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 02:46:53 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 02:46:54 - INFO - train -   Test: [   0/39]  Time: 0.593 (0.593)  Loss:  1.2188 (1.2188)  Acc@1: 67.1875 (67.1875)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:46:57 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.088)  Loss:  1.0635 (1.1762)  Acc@1: 75.0000 (67.6300)  Acc@5: 100.0000 (99.9900)
05/14/2023 02:46:57 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 02:46:57 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 02:46:58 - INFO - train -   Test: [   0/39]  Time: 0.564 (0.564)  Loss:  1.1758 (1.1758)  Acc@1: 68.3594 (68.3594)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:01 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.084)  Loss:  1.1035 (1.1400)  Acc@1: 75.0000 (70.4400)  Acc@5: 100.0000 (99.9900)
05/14/2023 02:47:01 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 02:47:01 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 02:47:02 - INFO - train -   Test: [   0/39]  Time: 0.626 (0.626)  Loss:  1.2002 (1.2002)  Acc@1: 71.0938 (71.0938)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:04 - INFO - train -   Test: [  39/39]  Time: 0.016 (0.085)  Loss:  1.1719 (1.1745)  Acc@1: 62.5000 (70.1100)  Acc@5: 100.0000 (99.9800)
05/14/2023 02:47:04 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 02:47:04 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 02:47:05 - INFO - train -   Test: [   0/39]  Time: 0.547 (0.547)  Loss:  0.7900 (0.7900)  Acc@1: 92.5781 (92.5781)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:08 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.083)  Loss:  0.7266 (0.7833)  Acc@1: 93.7500 (92.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:08 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 02:47:08 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 02:47:09 - INFO - train -   Test: [   0/39]  Time: 0.659 (0.659)  Loss:  0.8354 (0.8354)  Acc@1: 89.8438 (89.8438)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:12 - INFO - train -   Test: [  39/39]  Time: 0.015 (0.084)  Loss:  0.9131 (0.8524)  Acc@1: 87.5000 (89.1600)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:12 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 02:47:12 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 02:47:13 - INFO - train -   Test: [   0/39]  Time: 0.648 (0.648)  Loss:  0.6934 (0.6934)  Acc@1: 94.5312 (94.5312)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:15 - INFO - train -   Test: [  39/39]  Time: 0.015 (0.082)  Loss:  0.7695 (0.7141)  Acc@1: 93.7500 (93.5700)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:15 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 02:47:15 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 02:47:16 - INFO - train -   Test: [   0/39]  Time: 0.581 (0.581)  Loss:  0.5918 (0.5918)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:19 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.079)  Loss:  0.5811 (0.5992)  Acc@1: 93.7500 (97.3500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:19 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 02:47:19 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 02:47:20 - INFO - train -   Test: [   0/39]  Time: 0.587 (0.587)  Loss:  0.4287 (0.4287)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:22 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 02:47:22 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.080)  Loss:  0.4492 (0.4267)  Acc@1: 100.0000 (99.5200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:22 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 02:47:23 - INFO - train -   Test: [   0/39]  Time: 0.582 (0.582)  Loss:  0.6455 (0.6455)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:26 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.077)  Loss:  0.7256 (0.6325)  Acc@1: 93.7500 (99.0100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:26 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 02:47:26 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 02:47:26 - INFO - train -   Test: [   0/39]  Time: 0.564 (0.564)  Loss:  0.3708 (0.3708)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:47:29 - INFO - train -   Test: [  39/39]  Time: 0.013 (0.075)  Loss:  0.4495 (0.3752)  Acc@1: 100.0000 (99.9900)  Acc@5: 100.0000 (100.0000)
