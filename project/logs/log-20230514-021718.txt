05/14/2023 02:17:18 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 02:17:18 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 02:17:24 - INFO - train -   Model resnet18 created, param count:77191760
05/14/2023 02:19:34 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 02:19:34 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 02:19:46 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 02:19:56 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 6.599 (6.60)  Time: 9.474s,   27.02/s  (9.474s,   27.02/s)  LR: 5.500e-06  Data: 1.077 (1.077)
05/14/2023 02:20:19 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 7.017 (6.81)  Time: 0.468s,  547.28/s  (0.640s,  400.15/s)  LR: 5.500e-06  Data: 0.011 (0.032)
05/14/2023 02:20:42 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 7.386 (7.00)  Time: 0.457s,  560.13/s  (0.548s,  466.99/s)  LR: 5.500e-06  Data: 0.011 (0.022)
05/14/2023 02:20:44 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.699 (6.93)  Time: 0.448s,  571.03/s  (0.543s,  471.05/s)  LR: 5.500e-06  Data: 0.000 (0.021)
05/14/2023 02:20:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:20:48 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.708 (6.71)  Time: 1.011s,  253.32/s  (1.011s,  253.32/s)  LR: 5.504e-03  Data: 0.547 (0.547)
05/14/2023 02:21:11 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 6.287 (6.50)  Time: 0.399s,  641.25/s  (0.461s,  555.05/s)  LR: 5.504e-03  Data: 0.010 (0.021)
05/14/2023 02:21:34 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 5.807 (6.27)  Time: 0.481s,  532.29/s  (0.458s,  559.49/s)  LR: 5.504e-03  Data: 0.010 (0.016)
05/14/2023 02:21:35 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 5.969 (6.19)  Time: 0.448s,  570.91/s  (0.457s,  559.97/s)  LR: 5.504e-03  Data: 0.000 (0.016)
05/14/2023 02:21:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:21:40 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 5.724 (5.72)  Time: 1.179s,  217.09/s  (1.179s,  217.09/s)  LR: 1.100e-02  Data: 0.697 (0.697)
05/14/2023 02:22:02 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.801 (5.26)  Time: 0.468s,  547.56/s  (0.461s,  554.72/s)  LR: 1.100e-02  Data: 0.011 (0.024)
05/14/2023 02:22:25 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.410 (4.98)  Time: 0.471s,  543.92/s  (0.457s,  560.38/s)  LR: 1.100e-02  Data: 0.012 (0.018)
05/14/2023 02:22:27 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.439 (4.84)  Time: 0.462s,  553.52/s  (0.456s,  560.85/s)  LR: 1.100e-02  Data: 0.000 (0.018)
05/14/2023 02:22:27 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:22:31 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 4.510 (4.51)  Time: 1.046s,  244.72/s  (1.046s,  244.72/s)  LR: 1.650e-02  Data: 0.567 (0.567)
05/14/2023 02:22:54 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 4.191 (4.35)  Time: 0.480s,  533.86/s  (0.468s,  547.54/s)  LR: 1.650e-02  Data: 0.011 (0.022)
05/14/2023 02:23:17 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 4.174 (4.29)  Time: 0.450s,  568.60/s  (0.460s,  556.61/s)  LR: 1.650e-02  Data: 0.011 (0.017)
05/14/2023 02:23:19 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 4.207 (4.27)  Time: 0.465s,  550.21/s  (0.460s,  556.95/s)  LR: 1.650e-02  Data: 0.000 (0.016)
05/14/2023 02:23:19 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:23:23 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 4.160 (4.16)  Time: 0.970s,  263.81/s  (0.970s,  263.81/s)  LR: 2.200e-02  Data: 0.564 (0.564)
05/14/2023 02:23:45 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 3.972 (4.07)  Time: 0.451s,  567.61/s  (0.460s,  556.95/s)  LR: 2.200e-02  Data: 0.010 (0.022)
05/14/2023 02:24:08 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 4.910 (4.35)  Time: 0.471s,  543.32/s  (0.456s,  561.00/s)  LR: 2.200e-02  Data: 0.011 (0.016)
05/14/2023 02:24:10 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 3.923 (4.24)  Time: 0.438s,  584.51/s  (0.456s,  561.29/s)  LR: 2.200e-02  Data: 0.000 (0.016)
05/14/2023 02:24:10 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:24:14 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 3.769 (3.77)  Time: 1.033s,  247.83/s  (1.033s,  247.83/s)  LR: 2.566e-02  Data: 0.594 (0.594)
05/14/2023 02:24:37 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 3.788 (3.78)  Time: 0.416s,  614.89/s  (0.460s,  557.09/s)  LR: 2.566e-02  Data: 0.011 (0.022)
05/14/2023 02:24:59 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 3.767 (3.77)  Time: 0.459s,  557.73/s  (0.454s,  563.64/s)  LR: 2.566e-02  Data: 0.011 (0.017)
05/14/2023 02:25:01 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 3.833 (3.79)  Time: 0.474s,  539.76/s  (0.455s,  563.20/s)  LR: 2.566e-02  Data: 0.000 (0.016)
05/14/2023 02:25:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:25:05 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 3.844 (3.84)  Time: 1.017s,  251.84/s  (1.017s,  251.84/s)  LR: 2.487e-02  Data: 0.556 (0.556)
05/14/2023 02:25:28 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 3.679 (3.76)  Time: 0.436s,  587.38/s  (0.460s,  556.17/s)  LR: 2.487e-02  Data: 0.011 (0.022)
05/14/2023 02:25:51 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 3.556 (3.69)  Time: 0.468s,  546.82/s  (0.457s,  560.07/s)  LR: 2.487e-02  Data: 0.010 (0.017)
05/14/2023 02:25:52 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 3.342 (3.61)  Time: 0.459s,  557.48/s  (0.456s,  561.34/s)  LR: 2.487e-02  Data: 0.000 (0.016)
05/14/2023 02:25:52 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:25:57 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 3.611 (3.61)  Time: 1.017s,  251.81/s  (1.017s,  251.81/s)  LR: 2.397e-02  Data: 0.578 (0.578)
05/14/2023 02:26:19 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 3.441 (3.53)  Time: 0.452s,  566.55/s  (0.463s,  553.41/s)  LR: 2.397e-02  Data: 0.012 (0.022)
05/14/2023 02:26:42 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 3.527 (3.53)  Time: 0.414s,  618.02/s  (0.457s,  559.93/s)  LR: 2.397e-02  Data: 0.011 (0.017)
05/14/2023 02:26:43 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 3.396 (3.49)  Time: 0.411s,  623.55/s  (0.455s,  562.29/s)  LR: 2.397e-02  Data: 0.000 (0.017)
05/14/2023 02:26:43 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:26:48 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 3.349 (3.35)  Time: 1.032s,  247.96/s  (1.032s,  247.96/s)  LR: 2.295e-02  Data: 0.579 (0.579)
05/14/2023 02:27:11 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 3.481 (3.42)  Time: 0.447s,  572.51/s  (0.463s,  552.47/s)  LR: 2.295e-02  Data: 0.014 (0.022)
05/14/2023 02:27:33 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 3.461 (3.43)  Time: 0.426s,  601.48/s  (0.455s,  562.77/s)  LR: 2.295e-02  Data: 0.011 (0.017)
05/14/2023 02:27:35 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 3.424 (3.43)  Time: 0.432s,  592.90/s  (0.454s,  563.57/s)  LR: 2.295e-02  Data: 0.000 (0.016)
05/14/2023 02:27:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:27:39 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 3.459 (3.46)  Time: 1.043s,  245.49/s  (1.043s,  245.49/s)  LR: 2.183e-02  Data: 0.559 (0.559)
05/14/2023 02:28:02 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 3.329 (3.39)  Time: 0.407s,  629.50/s  (0.461s,  555.10/s)  LR: 2.183e-02  Data: 0.012 (0.022)
05/14/2023 02:28:24 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 3.394 (3.39)  Time: 0.395s,  647.67/s  (0.455s,  562.25/s)  LR: 2.183e-02  Data: 0.011 (0.017)
05/14/2023 02:28:26 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 3.213 (3.35)  Time: 0.468s,  546.73/s  (0.456s,  561.85/s)  LR: 2.183e-02  Data: 0.000 (0.016)
05/14/2023 02:28:26 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:28:30 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 3.303 (3.30)  Time: 1.046s,  244.65/s  (1.046s,  244.65/s)  LR: 2.063e-02  Data: 0.607 (0.607)
05/14/2023 02:28:53 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 3.256 (3.28)  Time: 0.447s,  572.67/s  (0.459s,  558.30/s)  LR: 2.063e-02  Data: 0.011 (0.023)
05/14/2023 02:29:15 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 3.199 (3.25)  Time: 0.473s,  541.04/s  (0.453s,  565.27/s)  LR: 2.063e-02  Data: 0.012 (0.017)
05/14/2023 02:29:17 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 3.711 (3.37)  Time: 0.434s,  590.14/s  (0.453s,  565.49/s)  LR: 2.063e-02  Data: 0.000 (0.017)
05/14/2023 02:29:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:29:21 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 3.214 (3.21)  Time: 1.119s,  228.68/s  (1.119s,  228.68/s)  LR: 1.934e-02  Data: 0.607 (0.607)
05/14/2023 02:29:44 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 3.231 (3.22)  Time: 0.404s,  634.24/s  (0.471s,  543.90/s)  LR: 1.934e-02  Data: 0.012 (0.023)
05/14/2023 02:30:07 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 3.502 (3.32)  Time: 0.467s,  548.02/s  (0.462s,  554.38/s)  LR: 1.934e-02  Data: 0.011 (0.017)
05/14/2023 02:30:09 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 3.621 (3.39)  Time: 0.402s,  636.15/s  (0.461s,  555.55/s)  LR: 1.934e-02  Data: 0.000 (0.017)
05/14/2023 02:30:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:30:13 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 3.214 (3.21)  Time: 1.071s,  239.07/s  (1.071s,  239.07/s)  LR: 1.800e-02  Data: 0.569 (0.569)
05/14/2023 02:30:36 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 3.184 (3.20)  Time: 0.457s,  560.05/s  (0.467s,  548.51/s)  LR: 1.800e-02  Data: 0.012 (0.022)
05/14/2023 02:30:58 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 3.204 (3.20)  Time: 0.416s,  614.76/s  (0.458s,  558.55/s)  LR: 1.800e-02  Data: 0.011 (0.017)
05/14/2023 02:31:00 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 3.175 (3.19)  Time: 0.458s,  558.44/s  (0.457s,  559.76/s)  LR: 1.800e-02  Data: 0.000 (0.017)
05/14/2023 02:31:00 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:31:05 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 3.073 (3.07)  Time: 1.046s,  244.76/s  (1.046s,  244.76/s)  LR: 1.661e-02  Data: 0.589 (0.589)
05/14/2023 02:31:27 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 3.044 (3.06)  Time: 0.428s,  597.50/s  (0.456s,  561.45/s)  LR: 1.661e-02  Data: 0.011 (0.022)
05/14/2023 02:31:49 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 3.189 (3.10)  Time: 0.430s,  595.41/s  (0.455s,  563.24/s)  LR: 1.661e-02  Data: 0.010 (0.017)
05/14/2023 02:31:51 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 3.110 (3.10)  Time: 0.379s,  675.58/s  (0.454s,  563.67/s)  LR: 1.661e-02  Data: 0.000 (0.017)
05/14/2023 02:31:51 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:31:56 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 2.944 (2.94)  Time: 1.086s,  235.69/s  (1.086s,  235.69/s)  LR: 1.519e-02  Data: 0.628 (0.628)
05/14/2023 02:32:18 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 3.016 (2.98)  Time: 0.471s,  543.20/s  (0.465s,  550.02/s)  LR: 1.519e-02  Data: 0.015 (0.024)
05/14/2023 02:32:41 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 3.140 (3.03)  Time: 0.444s,  576.79/s  (0.461s,  555.60/s)  LR: 1.519e-02  Data: 0.011 (0.017)
05/14/2023 02:32:43 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 2.995 (3.02)  Time: 0.448s,  570.89/s  (0.460s,  556.63/s)  LR: 1.519e-02  Data: 0.000 (0.017)
05/14/2023 02:32:43 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:32:47 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 3.306 (3.31)  Time: 1.010s,  253.34/s  (1.010s,  253.34/s)  LR: 1.375e-02  Data: 0.526 (0.526)
05/14/2023 02:33:10 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 3.108 (3.21)  Time: 0.433s,  591.85/s  (0.461s,  555.09/s)  LR: 1.375e-02  Data: 0.013 (0.021)
05/14/2023 02:33:32 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 3.112 (3.18)  Time: 0.471s,  543.76/s  (0.457s,  559.81/s)  LR: 1.375e-02  Data: 0.011 (0.016)
05/14/2023 02:33:34 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 2.907 (3.11)  Time: 0.448s,  571.55/s  (0.457s,  560.12/s)  LR: 1.375e-02  Data: 0.000 (0.016)
05/14/2023 02:33:34 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:33:39 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 2.984 (2.98)  Time: 1.022s,  250.56/s  (1.022s,  250.56/s)  LR: 1.231e-02  Data: 0.598 (0.598)
05/14/2023 02:34:01 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 2.906 (2.95)  Time: 0.481s,  532.27/s  (0.464s,  551.83/s)  LR: 1.231e-02  Data: 0.010 (0.023)
05/14/2023 02:34:24 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 3.086 (2.99)  Time: 0.472s,  542.40/s  (0.456s,  561.49/s)  LR: 1.231e-02  Data: 0.011 (0.017)
05/14/2023 02:34:25 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 3.579 (3.14)  Time: 0.408s,  627.53/s  (0.455s,  562.02/s)  LR: 1.231e-02  Data: 0.000 (0.017)
05/14/2023 02:34:25 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:34:30 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 3.113 (3.11)  Time: 1.024s,  250.04/s  (1.024s,  250.04/s)  LR: 1.089e-02  Data: 0.543 (0.543)
05/14/2023 02:34:53 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.963 (3.04)  Time: 0.440s,  581.77/s  (0.463s,  552.54/s)  LR: 1.089e-02  Data: 0.011 (0.022)
05/14/2023 02:35:15 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 3.321 (3.13)  Time: 0.480s,  533.79/s  (0.461s,  555.78/s)  LR: 1.089e-02  Data: 0.011 (0.017)
05/14/2023 02:35:17 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 2.921 (3.08)  Time: 0.470s,  544.19/s  (0.460s,  556.00/s)  LR: 1.089e-02  Data: 0.000 (0.016)
05/14/2023 02:35:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:35:22 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 3.034 (3.03)  Time: 1.104s,  231.83/s  (1.104s,  231.83/s)  LR: 9.501e-03  Data: 0.649 (0.649)
05/14/2023 02:35:45 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 3.051 (3.04)  Time: 0.421s,  608.02/s  (0.467s,  548.41/s)  LR: 9.501e-03  Data: 0.013 (0.024)
05/14/2023 02:36:08 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 3.038 (3.04)  Time: 0.406s,  630.38/s  (0.461s,  555.90/s)  LR: 9.501e-03  Data: 0.010 (0.018)
05/14/2023 02:36:09 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 3.005 (3.03)  Time: 0.423s,  605.27/s  (0.459s,  557.64/s)  LR: 9.501e-03  Data: 0.000 (0.017)
05/14/2023 02:36:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:36:14 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 2.891 (2.89)  Time: 1.004s,  254.91/s  (1.004s,  254.91/s)  LR: 8.157e-03  Data: 0.560 (0.560)
05/14/2023 02:36:36 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 2.855 (2.87)  Time: 0.469s,  545.44/s  (0.461s,  555.03/s)  LR: 8.157e-03  Data: 0.011 (0.022)
05/14/2023 02:36:59 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 2.799 (2.85)  Time: 0.470s,  544.79/s  (0.455s,  562.15/s)  LR: 8.157e-03  Data: 0.011 (0.017)
05/14/2023 02:37:01 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 2.844 (2.85)  Time: 0.433s,  590.97/s  (0.455s,  563.22/s)  LR: 8.157e-03  Data: 0.000 (0.017)
05/14/2023 02:37:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:37:05 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 2.804 (2.80)  Time: 1.035s,  247.36/s  (1.035s,  247.36/s)  LR: 6.875e-03  Data: 0.583 (0.583)
05/14/2023 02:37:28 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 2.894 (2.85)  Time: 0.421s,  607.88/s  (0.459s,  557.85/s)  LR: 6.875e-03  Data: 0.011 (0.023)
05/14/2023 02:37:50 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 3.032 (2.91)  Time: 0.468s,  547.01/s  (0.458s,  559.05/s)  LR: 6.875e-03  Data: 0.011 (0.017)
05/14/2023 02:37:52 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 2.895 (2.91)  Time: 0.430s,  595.48/s  (0.458s,  559.15/s)  LR: 6.875e-03  Data: 0.000 (0.017)
05/14/2023 02:37:52 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:37:57 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 3.024 (3.02)  Time: 0.964s,  265.46/s  (0.964s,  265.46/s)  LR: 5.668e-03  Data: 0.528 (0.528)
05/14/2023 02:38:20 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 3.154 (3.09)  Time: 0.478s,  535.62/s  (0.462s,  554.56/s)  LR: 5.668e-03  Data: 0.011 (0.022)
05/14/2023 02:38:42 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 3.293 (3.16)  Time: 0.486s,  526.78/s  (0.457s,  559.61/s)  LR: 5.668e-03  Data: 0.011 (0.017)
05/14/2023 02:38:44 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 3.346 (3.20)  Time: 0.412s,  621.37/s  (0.457s,  559.66/s)  LR: 5.668e-03  Data: 0.000 (0.017)
05/14/2023 02:38:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:38:49 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.962 (2.96)  Time: 1.061s,  241.33/s  (1.061s,  241.33/s)  LR: 4.549e-03  Data: 0.594 (0.594)
05/14/2023 02:39:12 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 2.966 (2.96)  Time: 0.410s,  624.95/s  (0.466s,  549.15/s)  LR: 4.549e-03  Data: 0.011 (0.024)
05/14/2023 02:39:35 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 2.900 (2.94)  Time: 0.440s,  582.30/s  (0.459s,  557.88/s)  LR: 4.549e-03  Data: 0.011 (0.018)
05/14/2023 02:39:36 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 3.461 (3.07)  Time: 0.465s,  550.81/s  (0.458s,  558.57/s)  LR: 4.549e-03  Data: 0.000 (0.017)
05/14/2023 02:39:36 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:39:41 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 2.939 (2.94)  Time: 1.006s,  254.57/s  (1.006s,  254.57/s)  LR: 3.532e-03  Data: 0.571 (0.571)
05/14/2023 02:40:03 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 2.737 (2.84)  Time: 0.474s,  540.04/s  (0.458s,  558.61/s)  LR: 3.532e-03  Data: 0.012 (0.023)
05/14/2023 02:40:26 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 2.863 (2.85)  Time: 0.482s,  530.74/s  (0.456s,  561.50/s)  LR: 3.532e-03  Data: 0.011 (0.017)
05/14/2023 02:40:28 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 3.253 (2.95)  Time: 0.468s,  546.43/s  (0.456s,  561.04/s)  LR: 3.532e-03  Data: 0.000 (0.017)
05/14/2023 02:40:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:40:33 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 2.886 (2.89)  Time: 0.970s,  263.89/s  (0.970s,  263.89/s)  LR: 2.626e-03  Data: 0.518 (0.518)
05/14/2023 02:40:55 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 3.010 (2.95)  Time: 0.471s,  543.46/s  (0.456s,  561.69/s)  LR: 2.626e-03  Data: 0.011 (0.021)
