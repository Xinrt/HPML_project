05/14/2023 04:32:50 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 04:32:50 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 04:32:51 - INFO - train -   Model resnet18 created, param count:43973840
05/14/2023 04:33:00 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 04:33:00 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 04:33:09 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 04:33:16 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 6.907 (6.91)  Time: 6.838s,   37.44/s  (6.838s,   37.44/s)  LR: 5.500e-06  Data: 1.200 (1.200)
05/14/2023 04:33:29 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 6.804 (6.86)  Time: 0.276s,  926.65/s  (0.402s,  636.84/s)  LR: 5.500e-06  Data: 0.013 (0.036)
05/14/2023 04:33:43 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 6.848 (6.85)  Time: 0.270s,  948.80/s  (0.337s,  760.14/s)  LR: 5.500e-06  Data: 0.012 (0.024)
05/14/2023 04:33:44 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.789 (6.84)  Time: 0.254s, 1008.17/s  (0.334s,  766.58/s)  LR: 5.500e-06  Data: 0.000 (0.023)
05/14/2023 04:33:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:33:47 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.841 (6.84)  Time: 0.899s,  284.64/s  (0.899s,  284.64/s)  LR: 5.504e-03  Data: 0.622 (0.622)
05/14/2023 04:34:01 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 3.944 (5.39)  Time: 0.268s,  955.89/s  (0.283s,  904.73/s)  LR: 5.504e-03  Data: 0.012 (0.024)
05/14/2023 04:34:14 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 3.658 (4.81)  Time: 0.268s,  954.73/s  (0.277s,  925.18/s)  LR: 5.504e-03  Data: 0.012 (0.018)
05/14/2023 04:34:15 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 3.554 (4.50)  Time: 0.255s, 1002.95/s  (0.276s,  926.84/s)  LR: 5.504e-03  Data: 0.000 (0.018)
05/14/2023 04:34:15 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:34:18 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 3.705 (3.70)  Time: 0.974s,  262.81/s  (0.974s,  262.81/s)  LR: 1.100e-02  Data: 0.711 (0.711)
05/14/2023 04:34:32 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 3.498 (3.60)  Time: 0.272s,  940.41/s  (0.286s,  896.11/s)  LR: 1.100e-02  Data: 0.012 (0.026)
05/14/2023 04:34:45 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 3.280 (3.49)  Time: 0.271s,  945.15/s  (0.278s,  922.40/s)  LR: 1.100e-02  Data: 0.012 (0.019)
05/14/2023 04:34:46 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 3.313 (3.45)  Time: 0.256s, 1001.19/s  (0.277s,  923.85/s)  LR: 1.100e-02  Data: 0.000 (0.019)
05/14/2023 04:34:46 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:34:49 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 3.295 (3.30)  Time: 0.920s,  278.32/s  (0.920s,  278.32/s)  LR: 1.650e-02  Data: 0.665 (0.665)
05/14/2023 04:35:03 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 3.511 (3.40)  Time: 0.279s,  918.78/s  (0.284s,  902.27/s)  LR: 1.650e-02  Data: 0.012 (0.025)
05/14/2023 04:35:16 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 3.061 (3.29)  Time: 0.268s,  954.70/s  (0.277s,  924.32/s)  LR: 1.650e-02  Data: 0.012 (0.018)
05/14/2023 04:35:17 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 2.897 (3.19)  Time: 0.257s,  997.08/s  (0.277s,  925.79/s)  LR: 1.650e-02  Data: 0.000 (0.018)
05/14/2023 04:35:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:35:21 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 3.026 (3.03)  Time: 0.969s,  264.16/s  (0.969s,  264.16/s)  LR: 2.200e-02  Data: 0.683 (0.683)
05/14/2023 04:35:34 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 2.845 (2.94)  Time: 0.266s,  960.66/s  (0.285s,  898.86/s)  LR: 2.200e-02  Data: 0.012 (0.025)
05/14/2023 04:35:48 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 2.879 (2.92)  Time: 0.267s,  960.33/s  (0.277s,  923.21/s)  LR: 2.200e-02  Data: 0.012 (0.019)
05/14/2023 04:35:49 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 2.937 (2.92)  Time: 0.255s, 1002.06/s  (0.277s,  924.94/s)  LR: 2.200e-02  Data: 0.000 (0.018)
05/14/2023 04:35:49 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:35:52 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 2.837 (2.84)  Time: 1.060s,  241.47/s  (1.060s,  241.47/s)  LR: 2.566e-02  Data: 0.796 (0.796)
05/14/2023 04:36:05 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 2.926 (2.88)  Time: 0.268s,  953.85/s  (0.286s,  895.38/s)  LR: 2.566e-02  Data: 0.012 (0.027)
05/14/2023 04:36:19 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 2.892 (2.88)  Time: 0.267s,  958.22/s  (0.278s,  920.13/s)  LR: 2.566e-02  Data: 0.012 (0.020)
05/14/2023 04:36:20 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 2.932 (2.90)  Time: 0.259s,  989.96/s  (0.278s,  921.69/s)  LR: 2.566e-02  Data: 0.000 (0.019)
05/14/2023 04:36:20 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:36:23 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 2.928 (2.93)  Time: 0.974s,  262.93/s  (0.974s,  262.93/s)  LR: 2.487e-02  Data: 0.712 (0.712)
05/14/2023 04:36:36 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 2.866 (2.90)  Time: 0.274s,  933.32/s  (0.284s,  900.98/s)  LR: 2.487e-02  Data: 0.014 (0.026)
05/14/2023 04:36:50 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 2.792 (2.86)  Time: 0.269s,  952.19/s  (0.278s,  922.45/s)  LR: 2.487e-02  Data: 0.012 (0.019)
05/14/2023 04:36:51 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 2.814 (2.85)  Time: 0.255s, 1004.40/s  (0.277s,  923.93/s)  LR: 2.487e-02  Data: 0.000 (0.019)
05/14/2023 04:36:51 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:36:54 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 2.729 (2.73)  Time: 0.913s,  280.49/s  (0.913s,  280.49/s)  LR: 2.397e-02  Data: 0.654 (0.654)
05/14/2023 04:37:07 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 2.715 (2.72)  Time: 0.273s,  938.65/s  (0.287s,  892.34/s)  LR: 2.397e-02  Data: 0.012 (0.026)
05/14/2023 04:37:21 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 2.740 (2.73)  Time: 0.268s,  956.81/s  (0.279s,  917.01/s)  LR: 2.397e-02  Data: 0.013 (0.019)
05/14/2023 04:37:22 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 2.656 (2.71)  Time: 0.256s,  999.75/s  (0.279s,  918.74/s)  LR: 2.397e-02  Data: 0.000 (0.019)
05/14/2023 04:37:22 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:37:25 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 2.722 (2.72)  Time: 1.013s,  252.72/s  (1.013s,  252.72/s)  LR: 2.295e-02  Data: 0.736 (0.736)
05/14/2023 04:37:39 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 2.761 (2.74)  Time: 0.277s,  925.30/s  (0.287s,  891.29/s)  LR: 2.295e-02  Data: 0.012 (0.027)
05/14/2023 04:37:52 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 2.683 (2.72)  Time: 0.272s,  942.00/s  (0.280s,  915.56/s)  LR: 2.295e-02  Data: 0.012 (0.020)
05/14/2023 04:37:53 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 2.724 (2.72)  Time: 0.261s,  981.81/s  (0.279s,  916.94/s)  LR: 2.295e-02  Data: 0.000 (0.019)
05/14/2023 04:37:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:37:56 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 2.767 (2.77)  Time: 0.908s,  282.04/s  (0.908s,  282.04/s)  LR: 2.183e-02  Data: 0.645 (0.645)
05/14/2023 04:38:10 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 2.682 (2.72)  Time: 0.269s,  953.26/s  (0.285s,  899.52/s)  LR: 2.183e-02  Data: 0.012 (0.025)
05/14/2023 04:38:23 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 2.739 (2.73)  Time: 0.269s,  951.27/s  (0.278s,  922.01/s)  LR: 2.183e-02  Data: 0.013 (0.019)
05/14/2023 04:38:25 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 2.910 (2.77)  Time: 0.259s,  986.57/s  (0.277s,  923.38/s)  LR: 2.183e-02  Data: 0.000 (0.018)
05/14/2023 04:38:25 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:38:27 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 2.750 (2.75)  Time: 0.950s,  269.50/s  (0.950s,  269.50/s)  LR: 2.063e-02  Data: 0.684 (0.684)
05/14/2023 04:38:41 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 2.659 (2.70)  Time: 0.267s,  958.52/s  (0.286s,  894.37/s)  LR: 2.063e-02  Data: 0.012 (0.026)
05/14/2023 04:38:55 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 2.645 (2.68)  Time: 0.272s,  940.08/s  (0.281s,  910.78/s)  LR: 2.063e-02  Data: 0.013 (0.019)
05/14/2023 04:38:56 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 2.544 (2.65)  Time: 0.260s,  986.14/s  (0.281s,  912.13/s)  LR: 2.063e-02  Data: 0.000 (0.019)
05/14/2023 04:38:56 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:38:59 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 2.640 (2.64)  Time: 1.061s,  241.24/s  (1.061s,  241.24/s)  LR: 1.934e-02  Data: 0.774 (0.774)
05/14/2023 04:39:13 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 2.633 (2.64)  Time: 0.274s,  932.81/s  (0.292s,  876.57/s)  LR: 1.934e-02  Data: 0.013 (0.027)
05/14/2023 04:39:27 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 2.599 (2.62)  Time: 0.271s,  943.87/s  (0.283s,  903.35/s)  LR: 1.934e-02  Data: 0.012 (0.020)
05/14/2023 04:39:28 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 2.809 (2.67)  Time: 0.259s,  986.83/s  (0.283s,  904.88/s)  LR: 1.934e-02  Data: 0.000 (0.020)
05/14/2023 04:39:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:39:31 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 2.863 (2.86)  Time: 0.970s,  263.96/s  (0.970s,  263.96/s)  LR: 1.800e-02  Data: 0.700 (0.700)
05/14/2023 04:39:45 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 2.616 (2.74)  Time: 0.274s,  932.89/s  (0.291s,  879.98/s)  LR: 1.800e-02  Data: 0.012 (0.026)
05/14/2023 04:39:58 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 2.711 (2.73)  Time: 0.274s,  932.74/s  (0.283s,  903.41/s)  LR: 1.800e-02  Data: 0.012 (0.019)
05/14/2023 04:40:00 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 2.536 (2.68)  Time: 0.261s,  980.56/s  (0.283s,  904.91/s)  LR: 1.800e-02  Data: 0.000 (0.019)
05/14/2023 04:40:00 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:40:02 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 2.554 (2.55)  Time: 1.116s,  229.29/s  (1.116s,  229.29/s)  LR: 1.661e-02  Data: 0.843 (0.843)
05/14/2023 04:40:16 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 2.622 (2.59)  Time: 0.282s,  908.11/s  (0.292s,  875.54/s)  LR: 1.661e-02  Data: 0.016 (0.029)
05/14/2023 04:40:30 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 2.622 (2.60)  Time: 0.277s,  924.24/s  (0.284s,  901.18/s)  LR: 1.661e-02  Data: 0.012 (0.021)
05/14/2023 04:40:31 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 2.730 (2.63)  Time: 0.260s,  983.10/s  (0.284s,  902.89/s)  LR: 1.661e-02  Data: 0.000 (0.020)
05/14/2023 04:40:31 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:40:34 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 2.657 (2.66)  Time: 1.057s,  242.22/s  (1.057s,  242.22/s)  LR: 1.519e-02  Data: 0.778 (0.778)
05/14/2023 04:40:48 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 2.538 (2.60)  Time: 0.269s,  951.36/s  (0.291s,  880.89/s)  LR: 1.519e-02  Data: 0.012 (0.027)
05/14/2023 04:41:02 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 2.567 (2.59)  Time: 0.274s,  933.62/s  (0.283s,  904.03/s)  LR: 1.519e-02  Data: 0.013 (0.020)
05/14/2023 04:41:03 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 2.528 (2.57)  Time: 0.264s,  969.94/s  (0.283s,  905.46/s)  LR: 1.519e-02  Data: 0.000 (0.020)
05/14/2023 04:41:03 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:41:06 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 2.809 (2.81)  Time: 0.968s,  264.44/s  (0.968s,  264.44/s)  LR: 1.375e-02  Data: 0.697 (0.697)
05/14/2023 04:41:19 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 2.569 (2.69)  Time: 0.286s,  895.99/s  (0.292s,  877.65/s)  LR: 1.375e-02  Data: 0.015 (0.027)
05/14/2023 04:41:33 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 2.506 (2.63)  Time: 0.275s,  932.35/s  (0.285s,  897.94/s)  LR: 1.375e-02  Data: 0.012 (0.020)
05/14/2023 04:41:34 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 2.604 (2.62)  Time: 0.263s,  972.70/s  (0.285s,  899.66/s)  LR: 1.375e-02  Data: 0.000 (0.019)
05/14/2023 04:41:34 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:41:38 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 2.686 (2.69)  Time: 1.007s,  254.30/s  (1.007s,  254.30/s)  LR: 1.231e-02  Data: 0.717 (0.717)
05/14/2023 04:41:52 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 2.611 (2.65)  Time: 0.275s,  931.30/s  (0.291s,  881.03/s)  LR: 1.231e-02  Data: 0.013 (0.026)
05/14/2023 04:42:05 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 2.647 (2.65)  Time: 0.274s,  934.38/s  (0.284s,  901.92/s)  LR: 1.231e-02  Data: 0.012 (0.020)
05/14/2023 04:42:07 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 2.626 (2.64)  Time: 0.261s,  980.38/s  (0.283s,  903.50/s)  LR: 1.231e-02  Data: 0.000 (0.019)
05/14/2023 04:42:07 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:42:10 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 2.585 (2.58)  Time: 0.887s,  288.56/s  (0.887s,  288.56/s)  LR: 1.089e-02  Data: 0.624 (0.624)
05/14/2023 04:42:23 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.744 (2.66)  Time: 0.273s,  936.66/s  (0.288s,  890.30/s)  LR: 1.089e-02  Data: 0.012 (0.025)
05/14/2023 04:42:37 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 2.610 (2.65)  Time: 0.274s,  935.52/s  (0.282s,  908.73/s)  LR: 1.089e-02  Data: 0.012 (0.019)
05/14/2023 04:42:38 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 2.550 (2.62)  Time: 0.262s,  976.98/s  (0.281s,  910.16/s)  LR: 1.089e-02  Data: 0.000 (0.018)
05/14/2023 04:42:38 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:42:41 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 2.479 (2.48)  Time: 1.063s,  240.79/s  (1.063s,  240.79/s)  LR: 9.501e-03  Data: 0.782 (0.782)
05/14/2023 04:42:55 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 2.517 (2.50)  Time: 0.273s,  939.05/s  (0.291s,  879.51/s)  LR: 9.501e-03  Data: 0.014 (0.028)
05/14/2023 04:43:09 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 2.588 (2.53)  Time: 0.271s,  943.59/s  (0.284s,  902.28/s)  LR: 9.501e-03  Data: 0.013 (0.020)
05/14/2023 04:43:10 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 2.546 (2.53)  Time: 0.263s,  973.01/s  (0.283s,  903.72/s)  LR: 9.501e-03  Data: 0.000 (0.020)
05/14/2023 04:43:10 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:43:13 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 2.552 (2.55)  Time: 1.040s,  246.26/s  (1.040s,  246.26/s)  LR: 8.157e-03  Data: 0.773 (0.773)
05/14/2023 04:43:27 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 2.634 (2.59)  Time: 0.271s,  944.13/s  (0.291s,  879.79/s)  LR: 8.157e-03  Data: 0.013 (0.027)
05/14/2023 04:43:41 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 2.685 (2.62)  Time: 0.276s,  927.40/s  (0.283s,  905.05/s)  LR: 8.157e-03  Data: 0.012 (0.020)
05/14/2023 04:43:42 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 2.478 (2.59)  Time: 0.263s,  973.21/s  (0.282s,  906.53/s)  LR: 8.157e-03  Data: 0.000 (0.020)
05/14/2023 04:43:42 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:43:45 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 2.556 (2.56)  Time: 1.111s,  230.32/s  (1.111s,  230.32/s)  LR: 6.875e-03  Data: 0.834 (0.834)
05/14/2023 04:43:59 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 2.469 (2.51)  Time: 0.272s,  939.72/s  (0.293s,  872.52/s)  LR: 6.875e-03  Data: 0.012 (0.029)
05/14/2023 04:44:12 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 2.525 (2.52)  Time: 0.271s,  945.30/s  (0.285s,  899.51/s)  LR: 6.875e-03  Data: 0.012 (0.021)
05/14/2023 04:44:13 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 2.461 (2.50)  Time: 0.259s,  987.51/s  (0.284s,  901.31/s)  LR: 6.875e-03  Data: 0.000 (0.020)
05/14/2023 04:44:13 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:44:16 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 2.499 (2.50)  Time: 0.987s,  259.44/s  (0.987s,  259.44/s)  LR: 5.668e-03  Data: 0.721 (0.721)
05/14/2023 04:44:30 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 2.447 (2.47)  Time: 0.279s,  918.23/s  (0.289s,  884.50/s)  LR: 5.668e-03  Data: 0.015 (0.026)
05/14/2023 04:44:44 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 2.619 (2.52)  Time: 0.273s,  937.21/s  (0.283s,  905.28/s)  LR: 5.668e-03  Data: 0.012 (0.020)
05/14/2023 04:44:45 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 2.401 (2.49)  Time: 0.261s,  979.36/s  (0.282s,  906.55/s)  LR: 5.668e-03  Data: 0.000 (0.019)
05/14/2023 04:44:45 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:44:48 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.533 (2.53)  Time: 0.988s,  259.18/s  (0.988s,  259.18/s)  LR: 4.549e-03  Data: 0.701 (0.701)
05/14/2023 04:45:02 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 2.453 (2.49)  Time: 0.277s,  925.63/s  (0.290s,  881.68/s)  LR: 4.549e-03  Data: 0.012 (0.026)
05/14/2023 04:45:16 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 2.569 (2.52)  Time: 0.271s,  943.78/s  (0.283s,  904.95/s)  LR: 4.549e-03  Data: 0.012 (0.019)
05/14/2023 04:45:17 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 2.482 (2.51)  Time: 0.259s,  988.96/s  (0.282s,  906.38/s)  LR: 4.549e-03  Data: 0.000 (0.019)
05/14/2023 04:45:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:45:20 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 2.604 (2.60)  Time: 1.052s,  243.29/s  (1.052s,  243.29/s)  LR: 3.532e-03  Data: 0.786 (0.786)
05/14/2023 04:45:34 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 2.600 (2.60)  Time: 0.278s,  920.19/s  (0.292s,  875.38/s)  LR: 3.532e-03  Data: 0.013 (0.028)
05/14/2023 04:45:48 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 2.564 (2.59)  Time: 0.274s,  935.50/s  (0.284s,  902.56/s)  LR: 3.532e-03  Data: 0.012 (0.020)
05/14/2023 04:45:49 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 2.599 (2.59)  Time: 0.262s,  978.44/s  (0.283s,  904.25/s)  LR: 3.532e-03  Data: 0.000 (0.020)
05/14/2023 04:45:49 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:45:52 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 2.506 (2.51)  Time: 1.020s,  251.08/s  (1.020s,  251.08/s)  LR: 2.626e-03  Data: 0.740 (0.740)
05/14/2023 04:46:06 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 2.662 (2.58)  Time: 0.297s,  862.60/s  (0.292s,  875.84/s)  LR: 2.626e-03  Data: 0.013 (0.027)
05/14/2023 04:46:20 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 2.652 (2.61)  Time: 0.271s,  943.21/s  (0.284s,  900.90/s)  LR: 2.626e-03  Data: 0.012 (0.020)
05/14/2023 04:46:21 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 2.567 (2.60)  Time: 0.258s,  993.98/s  (0.284s,  902.76/s)  LR: 2.626e-03  Data: 0.000 (0.020)
05/14/2023 04:46:21 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:46:24 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 2.548 (2.55)  Time: 1.015s,  252.34/s  (1.015s,  252.34/s)  LR: 1.842e-03  Data: 0.745 (0.745)
05/14/2023 04:46:38 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 2.603 (2.58)  Time: 0.277s,  925.51/s  (0.290s,  881.64/s)  LR: 1.842e-03  Data: 0.013 (0.027)
05/14/2023 04:46:52 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 2.483 (2.54)  Time: 0.273s,  936.26/s  (0.283s,  904.47/s)  LR: 1.842e-03  Data: 0.012 (0.020)
05/14/2023 04:46:53 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 2.463 (2.52)  Time: 0.262s,  975.35/s  (0.283s,  905.95/s)  LR: 1.842e-03  Data: 0.000 (0.020)
05/14/2023 04:46:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:46:56 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 2.676 (2.68)  Time: 0.961s,  266.47/s  (0.961s,  266.47/s)  LR: 1.189e-03  Data: 0.670 (0.670)
05/14/2023 04:47:10 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 2.467 (2.57)  Time: 0.269s,  951.97/s  (0.291s,  879.98/s)  LR: 1.189e-03  Data: 0.012 (0.026)
05/14/2023 04:47:23 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 2.482 (2.54)  Time: 0.279s,  916.79/s  (0.283s,  903.41/s)  LR: 1.189e-03  Data: 0.012 (0.019)
05/14/2023 04:47:25 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 2.510 (2.53)  Time: 0.262s,  975.87/s  (0.283s,  904.86/s)  LR: 1.189e-03  Data: 0.000 (0.019)
05/14/2023 04:47:25 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:47:27 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 2.507 (2.51)  Time: 1.028s,  249.09/s  (1.028s,  249.09/s)  LR: 6.730e-04  Data: 0.764 (0.764)
05/14/2023 04:47:41 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 2.551 (2.53)  Time: 0.282s,  908.86/s  (0.292s,  877.90/s)  LR: 6.730e-04  Data: 0.014 (0.028)
05/14/2023 04:47:55 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 2.554 (2.54)  Time: 0.275s,  930.41/s  (0.284s,  902.40/s)  LR: 6.730e-04  Data: 0.013 (0.020)
05/14/2023 04:47:56 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 2.578 (2.55)  Time: 0.262s,  978.83/s  (0.283s,  903.82/s)  LR: 6.730e-04  Data: 0.000 (0.020)
05/14/2023 04:47:56 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:47:59 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 2.572 (2.57)  Time: 0.999s,  256.30/s  (0.999s,  256.30/s)  LR: 3.005e-04  Data: 0.720 (0.720)
05/14/2023 04:48:13 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 2.595 (2.58)  Time: 0.276s,  927.62/s  (0.291s,  880.35/s)  LR: 3.005e-04  Data: 0.012 (0.027)
05/14/2023 04:48:27 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 2.664 (2.61)  Time: 0.274s,  935.03/s  (0.284s,  902.50/s)  LR: 3.005e-04  Data: 0.012 (0.020)
05/14/2023 04:48:28 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 2.520 (2.59)  Time: 0.264s,  970.00/s  (0.283s,  903.94/s)  LR: 3.005e-04  Data: 0.000 (0.019)
05/14/2023 04:48:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:48:31 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 2.541 (2.54)  Time: 0.935s,  273.69/s  (0.935s,  273.69/s)  LR: 7.532e-05  Data: 0.674 (0.674)
05/14/2023 04:48:45 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 2.562 (2.55)  Time: 0.274s,  933.51/s  (0.290s,  883.43/s)  LR: 7.532e-05  Data: 0.012 (0.026)
05/14/2023 04:48:59 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 2.551 (2.55)  Time: 0.272s,  940.73/s  (0.283s,  904.23/s)  LR: 7.532e-05  Data: 0.014 (0.020)
05/14/2023 04:49:00 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 2.522 (2.54)  Time: 0.258s,  990.35/s  (0.283s,  905.88/s)  LR: 7.532e-05  Data: 0.000 (0.019)
05/14/2023 04:49:00 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 04:49:00 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/14/2023 04:49:02 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/14/2023 04:49:04 - INFO - train -   Test: [   0/39]  Time: 1.297 (1.297)  Loss:  1.0781 (1.0781)  Acc@1: 68.7500 (68.7500)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:06 - INFO - train -   Test: [  39/39]  Time: 0.076 (0.095)  Loss:  1.0547 (1.0521)  Acc@1: 62.5000 (69.0300)  Acc@5: 100.0000 (99.9800)
05/14/2023 04:49:06 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/14/2023 04:49:06 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/14/2023 04:49:07 - INFO - train -   Test: [   0/39]  Time: 0.584 (0.584)  Loss:  0.9375 (0.9375)  Acc@1: 77.3438 (77.3438)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:09 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/14/2023 04:49:09 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.064)  Loss:  0.7607 (0.8975)  Acc@1: 81.2500 (77.9600)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:09 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/14/2023 04:49:10 - INFO - train -   Test: [   0/39]  Time: 0.600 (0.600)  Loss:  0.9468 (0.9468)  Acc@1: 72.6562 (72.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:12 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/14/2023 04:49:12 - INFO - train -   Test: [  39/39]  Time: 0.018 (0.062)  Loss:  0.7637 (0.9216)  Acc@1: 87.5000 (76.1800)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:12 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/14/2023 04:49:13 - INFO - train -   Test: [   0/39]  Time: 0.604 (0.604)  Loss:  0.9219 (0.9219)  Acc@1: 76.1719 (76.1719)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:14 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.064)  Loss:  0.6997 (0.8980)  Acc@1: 81.2500 (76.9600)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:14 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/14/2023 04:49:14 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/14/2023 04:49:15 - INFO - train -   Test: [   0/39]  Time: 0.647 (0.647)  Loss:  0.9541 (0.9541)  Acc@1: 73.4375 (73.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:17 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.067)  Loss:  0.7329 (0.8990)  Acc@1: 81.2500 (76.3100)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:17 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/14/2023 04:49:17 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/14/2023 04:49:18 - INFO - train -   Test: [   0/39]  Time: 0.629 (0.629)  Loss:  0.8809 (0.8809)  Acc@1: 75.7812 (75.7812)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:20 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/14/2023 04:49:20 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.065)  Loss:  0.6465 (0.8682)  Acc@1: 87.5000 (77.8000)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:20 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/14/2023 04:49:21 - INFO - train -   Test: [   0/39]  Time: 0.599 (0.599)  Loss:  0.8853 (0.8853)  Acc@1: 76.9531 (76.9531)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:23 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.065)  Loss:  0.6904 (0.8767)  Acc@1: 81.2500 (77.8100)  Acc@5: 100.0000 (99.9900)
05/14/2023 04:49:23 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/14/2023 04:49:23 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/14/2023 04:49:24 - INFO - train -   Test: [   0/39]  Time: 0.591 (0.591)  Loss:  0.8545 (0.8545)  Acc@1: 79.6875 (79.6875)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:26 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.065)  Loss:  0.6382 (0.8627)  Acc@1: 87.5000 (78.4100)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:26 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/14/2023 04:49:26 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/14/2023 04:49:26 - INFO - train -   Test: [   0/39]  Time: 0.628 (0.628)  Loss:  0.9048 (0.9048)  Acc@1: 76.1719 (76.1719)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:28 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/14/2023 04:49:28 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.065)  Loss:  0.7314 (0.8797)  Acc@1: 81.2500 (76.1700)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:28 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/14/2023 04:49:29 - INFO - train -   Test: [   0/39]  Time: 0.609 (0.609)  Loss:  0.9268 (0.9268)  Acc@1: 72.6562 (72.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 04:49:31 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.066)  Loss:  0.7314 (0.8992)  Acc@1: 87.5000 (75.5600)  Acc@5: 100.0000 (100.0000)
