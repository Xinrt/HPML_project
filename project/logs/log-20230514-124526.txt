05/14/2023 12:45:26 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 12:45:26 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 12:45:29 - INFO - train -   Model resnet18 created, param count:33651792
05/14/2023 12:46:03 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 12:46:03 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 12:46:14 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 12:46:21 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 7.074 (7.07)  Time: 6.819s,   37.54/s  (6.819s,   37.54/s)  LR: 5.500e-06  Data: 1.167 (1.167)
05/14/2023 12:46:33 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 6.869 (6.97)  Time: 0.259s,  989.77/s  (0.381s,  671.62/s)  LR: 5.500e-06  Data: 0.015 (0.035)
05/14/2023 12:46:46 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 6.915 (6.95)  Time: 0.262s,  978.53/s  (0.317s,  807.27/s)  LR: 5.500e-06  Data: 0.012 (0.024)
05/14/2023 12:46:47 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.912 (6.94)  Time: 0.228s, 1121.97/s  (0.314s,  815.06/s)  LR: 5.500e-06  Data: 0.000 (0.023)
05/14/2023 12:46:47 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:46:50 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.894 (6.89)  Time: 1.094s,  234.08/s  (1.094s,  234.08/s)  LR: 5.504e-03  Data: 0.852 (0.852)
05/14/2023 12:47:02 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 4.123 (5.51)  Time: 0.248s, 1034.18/s  (0.267s,  957.08/s)  LR: 5.504e-03  Data: 0.012 (0.028)
05/14/2023 12:47:15 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 3.925 (4.98)  Time: 0.251s, 1018.69/s  (0.259s,  988.63/s)  LR: 5.504e-03  Data: 0.011 (0.020)
05/14/2023 12:47:16 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 3.987 (4.73)  Time: 0.240s, 1065.52/s  (0.258s,  990.76/s)  LR: 5.504e-03  Data: 0.000 (0.020)
05/14/2023 12:47:16 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:47:18 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 3.788 (3.79)  Time: 1.020s,  251.07/s  (1.020s,  251.07/s)  LR: 1.100e-02  Data: 0.777 (0.777)
05/14/2023 12:47:31 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 3.590 (3.69)  Time: 0.268s,  954.09/s  (0.268s,  954.99/s)  LR: 1.100e-02  Data: 0.014 (0.028)
05/14/2023 12:47:44 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 3.317 (3.57)  Time: 0.262s,  977.50/s  (0.259s,  989.82/s)  LR: 1.100e-02  Data: 0.012 (0.020)
05/14/2023 12:47:45 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 3.486 (3.55)  Time: 0.266s,  962.21/s  (0.259s,  989.79/s)  LR: 1.100e-02  Data: 0.000 (0.020)
05/14/2023 12:47:45 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:47:47 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 3.453 (3.45)  Time: 1.016s,  251.87/s  (1.016s,  251.87/s)  LR: 1.650e-02  Data: 0.775 (0.775)
05/14/2023 12:48:00 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 3.593 (3.52)  Time: 0.247s, 1036.39/s  (0.268s,  953.86/s)  LR: 1.650e-02  Data: 0.012 (0.027)
05/14/2023 12:48:12 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 3.057 (3.37)  Time: 0.249s, 1028.41/s  (0.260s,  986.27/s)  LR: 1.650e-02  Data: 0.012 (0.020)
05/14/2023 12:48:13 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 3.154 (3.31)  Time: 0.240s, 1067.72/s  (0.259s,  988.43/s)  LR: 1.650e-02  Data: 0.000 (0.019)
05/14/2023 12:48:13 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:48:16 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 3.118 (3.12)  Time: 1.109s,  230.85/s  (1.109s,  230.85/s)  LR: 2.200e-02  Data: 0.861 (0.861)
05/14/2023 12:48:29 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 3.077 (3.10)  Time: 0.240s, 1064.49/s  (0.270s,  947.15/s)  LR: 2.200e-02  Data: 0.011 (0.029)
05/14/2023 12:48:41 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 2.998 (3.06)  Time: 0.252s, 1016.24/s  (0.262s,  978.23/s)  LR: 2.200e-02  Data: 0.012 (0.021)
05/14/2023 12:48:42 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 3.057 (3.06)  Time: 0.229s, 1116.72/s  (0.261s,  980.45/s)  LR: 2.200e-02  Data: 0.000 (0.020)
05/14/2023 12:48:42 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:48:45 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 3.111 (3.11)  Time: 1.150s,  222.56/s  (1.150s,  222.56/s)  LR: 2.566e-02  Data: 0.912 (0.912)
05/14/2023 12:48:57 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 2.933 (3.02)  Time: 0.240s, 1067.16/s  (0.270s,  946.70/s)  LR: 2.566e-02  Data: 0.011 (0.030)
05/14/2023 12:49:10 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 2.969 (3.00)  Time: 0.253s, 1013.82/s  (0.262s,  977.81/s)  LR: 2.566e-02  Data: 0.012 (0.022)
05/14/2023 12:49:11 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 2.820 (2.96)  Time: 0.250s, 1022.71/s  (0.261s,  979.44/s)  LR: 2.566e-02  Data: 0.000 (0.021)
05/14/2023 12:49:11 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:49:14 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 2.942 (2.94)  Time: 1.003s,  255.12/s  (1.003s,  255.12/s)  LR: 2.487e-02  Data: 0.756 (0.756)
05/14/2023 12:49:26 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 3.027 (2.98)  Time: 0.264s,  969.98/s  (0.267s,  960.40/s)  LR: 2.487e-02  Data: 0.013 (0.028)
05/14/2023 12:49:39 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 2.952 (2.97)  Time: 0.249s, 1029.41/s  (0.259s,  987.49/s)  LR: 2.487e-02  Data: 0.011 (0.020)
05/14/2023 12:49:40 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 2.924 (2.96)  Time: 0.229s, 1118.14/s  (0.259s,  988.68/s)  LR: 2.487e-02  Data: 0.000 (0.020)
05/14/2023 12:49:40 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:49:43 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 2.952 (2.95)  Time: 1.082s,  236.70/s  (1.082s,  236.70/s)  LR: 2.397e-02  Data: 0.834 (0.834)
05/14/2023 12:49:55 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 2.865 (2.91)  Time: 0.253s, 1013.63/s  (0.269s,  952.19/s)  LR: 2.397e-02  Data: 0.012 (0.028)
05/14/2023 12:50:08 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 2.869 (2.90)  Time: 0.244s, 1050.86/s  (0.259s,  986.93/s)  LR: 2.397e-02  Data: 0.012 (0.020)
05/14/2023 12:50:09 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 2.896 (2.90)  Time: 0.229s, 1117.38/s  (0.259s,  989.11/s)  LR: 2.397e-02  Data: 0.000 (0.020)
05/14/2023 12:50:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:50:11 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 2.861 (2.86)  Time: 1.147s,  223.26/s  (1.147s,  223.26/s)  LR: 2.295e-02  Data: 0.902 (0.902)
05/14/2023 12:50:24 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 2.771 (2.82)  Time: 0.262s,  977.57/s  (0.272s,  940.97/s)  LR: 2.295e-02  Data: 0.012 (0.030)
05/14/2023 12:50:37 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 2.752 (2.79)  Time: 0.263s,  975.04/s  (0.262s,  976.67/s)  LR: 2.295e-02  Data: 0.012 (0.021)
05/14/2023 12:50:38 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 2.844 (2.81)  Time: 0.242s, 1059.06/s  (0.262s,  978.40/s)  LR: 2.295e-02  Data: 0.000 (0.021)
05/14/2023 12:50:38 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:50:40 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 2.865 (2.87)  Time: 1.071s,  238.99/s  (1.071s,  238.99/s)  LR: 2.183e-02  Data: 0.837 (0.837)
05/14/2023 12:50:53 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 2.891 (2.88)  Time: 0.239s, 1073.10/s  (0.267s,  957.30/s)  LR: 2.183e-02  Data: 0.014 (0.028)
05/14/2023 12:51:05 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 2.782 (2.85)  Time: 0.253s, 1009.96/s  (0.259s,  987.59/s)  LR: 2.183e-02  Data: 0.012 (0.020)
05/14/2023 12:51:06 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 2.949 (2.87)  Time: 0.250s, 1022.99/s  (0.259s,  988.34/s)  LR: 2.183e-02  Data: 0.000 (0.020)
05/14/2023 12:51:06 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:51:09 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 2.846 (2.85)  Time: 1.118s,  228.91/s  (1.118s,  228.91/s)  LR: 2.063e-02  Data: 0.871 (0.871)
05/14/2023 12:51:22 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 2.798 (2.82)  Time: 0.245s, 1045.56/s  (0.273s,  939.04/s)  LR: 2.063e-02  Data: 0.013 (0.029)
05/14/2023 12:51:34 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 2.714 (2.79)  Time: 0.254s, 1006.94/s  (0.262s,  976.51/s)  LR: 2.063e-02  Data: 0.012 (0.021)
05/14/2023 12:51:35 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 2.697 (2.76)  Time: 0.232s, 1103.84/s  (0.262s,  978.74/s)  LR: 2.063e-02  Data: 0.000 (0.020)
05/14/2023 12:51:35 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:51:38 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 2.776 (2.78)  Time: 1.268s,  201.82/s  (1.268s,  201.82/s)  LR: 1.934e-02  Data: 1.032 (1.032)
05/14/2023 12:51:51 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 2.710 (2.74)  Time: 0.256s,  998.42/s  (0.275s,  932.19/s)  LR: 1.934e-02  Data: 0.012 (0.033)
05/14/2023 12:52:04 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 2.738 (2.74)  Time: 0.253s, 1011.95/s  (0.266s,  963.99/s)  LR: 1.934e-02  Data: 0.012 (0.023)
05/14/2023 12:52:05 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 2.803 (2.76)  Time: 0.248s, 1034.30/s  (0.265s,  964.97/s)  LR: 1.934e-02  Data: 0.000 (0.023)
05/14/2023 12:52:05 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:52:07 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 2.941 (2.94)  Time: 1.132s,  226.10/s  (1.132s,  226.10/s)  LR: 1.800e-02  Data: 0.880 (0.880)
05/14/2023 12:52:20 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 2.915 (2.93)  Time: 0.251s, 1019.04/s  (0.271s,  944.36/s)  LR: 1.800e-02  Data: 0.015 (0.030)
05/14/2023 12:52:33 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 2.904 (2.92)  Time: 0.237s, 1079.68/s  (0.265s,  966.53/s)  LR: 1.800e-02  Data: 0.013 (0.021)
05/14/2023 12:52:34 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 2.701 (2.87)  Time: 0.245s, 1044.02/s  (0.264s,  967.97/s)  LR: 1.800e-02  Data: 0.000 (0.021)
05/14/2023 12:52:34 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:52:36 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 2.646 (2.65)  Time: 1.075s,  238.05/s  (1.075s,  238.05/s)  LR: 1.661e-02  Data: 0.819 (0.819)
05/14/2023 12:52:49 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 2.630 (2.64)  Time: 0.268s,  955.49/s  (0.272s,  941.38/s)  LR: 1.661e-02  Data: 0.013 (0.028)
05/14/2023 12:53:02 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 2.678 (2.65)  Time: 0.268s,  955.82/s  (0.264s,  968.77/s)  LR: 1.661e-02  Data: 0.013 (0.021)
05/14/2023 12:53:03 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 2.764 (2.68)  Time: 0.244s, 1047.44/s  (0.264s,  970.41/s)  LR: 1.661e-02  Data: 0.000 (0.020)
05/14/2023 12:53:03 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:53:06 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 2.806 (2.81)  Time: 1.143s,  224.01/s  (1.143s,  224.01/s)  LR: 1.519e-02  Data: 0.894 (0.894)
05/14/2023 12:53:18 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 2.602 (2.70)  Time: 0.249s, 1028.84/s  (0.274s,  935.91/s)  LR: 1.519e-02  Data: 0.014 (0.030)
05/14/2023 12:53:31 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 2.805 (2.74)  Time: 0.252s, 1016.05/s  (0.263s,  974.48/s)  LR: 1.519e-02  Data: 0.013 (0.021)
05/14/2023 12:53:32 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 2.620 (2.71)  Time: 0.253s, 1011.58/s  (0.262s,  975.87/s)  LR: 1.519e-02  Data: 0.000 (0.021)
05/14/2023 12:53:32 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:53:35 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 2.865 (2.87)  Time: 1.134s,  225.65/s  (1.134s,  225.65/s)  LR: 1.375e-02  Data: 0.890 (0.890)
05/14/2023 12:53:47 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 2.670 (2.77)  Time: 0.258s,  991.45/s  (0.274s,  934.42/s)  LR: 1.375e-02  Data: 0.012 (0.030)
05/14/2023 12:54:00 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 2.636 (2.72)  Time: 0.250s, 1024.78/s  (0.266s,  961.53/s)  LR: 1.375e-02  Data: 0.012 (0.021)
05/14/2023 12:54:01 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 2.674 (2.71)  Time: 0.246s, 1038.80/s  (0.266s,  963.79/s)  LR: 1.375e-02  Data: 0.000 (0.021)
05/14/2023 12:54:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:54:04 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 2.635 (2.64)  Time: 1.114s,  229.87/s  (1.114s,  229.87/s)  LR: 1.231e-02  Data: 0.849 (0.849)
05/14/2023 12:54:17 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 2.611 (2.62)  Time: 0.248s, 1031.54/s  (0.273s,  936.92/s)  LR: 1.231e-02  Data: 0.012 (0.029)
05/14/2023 12:54:30 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 2.705 (2.65)  Time: 0.242s, 1057.60/s  (0.264s,  970.15/s)  LR: 1.231e-02  Data: 0.012 (0.021)
05/14/2023 12:54:31 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 2.589 (2.64)  Time: 0.237s, 1077.92/s  (0.263s,  971.81/s)  LR: 1.231e-02  Data: 0.000 (0.021)
05/14/2023 12:54:31 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:54:33 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 2.724 (2.72)  Time: 1.084s,  236.11/s  (1.084s,  236.11/s)  LR: 1.089e-02  Data: 0.821 (0.821)
05/14/2023 12:54:46 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.916 (2.82)  Time: 0.256s,  998.85/s  (0.275s,  931.55/s)  LR: 1.089e-02  Data: 0.013 (0.029)
05/14/2023 12:54:59 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 2.679 (2.77)  Time: 0.246s, 1039.61/s  (0.266s,  961.47/s)  LR: 1.089e-02  Data: 0.012 (0.021)
05/14/2023 12:55:00 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 2.686 (2.75)  Time: 0.233s, 1098.57/s  (0.265s,  964.26/s)  LR: 1.089e-02  Data: 0.000 (0.021)
05/14/2023 12:55:00 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:55:03 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 2.699 (2.70)  Time: 1.147s,  223.20/s  (1.147s,  223.20/s)  LR: 9.501e-03  Data: 0.877 (0.877)
05/14/2023 12:55:16 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 2.584 (2.64)  Time: 0.259s,  986.60/s  (0.276s,  926.01/s)  LR: 9.501e-03  Data: 0.015 (0.030)
05/14/2023 12:55:29 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 2.906 (2.73)  Time: 0.239s, 1069.96/s  (0.266s,  961.95/s)  LR: 9.501e-03  Data: 0.012 (0.021)
05/14/2023 12:55:30 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 2.710 (2.72)  Time: 0.256s, 1000.01/s  (0.265s,  964.26/s)  LR: 9.501e-03  Data: 0.000 (0.021)
05/14/2023 12:55:30 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:55:32 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 2.599 (2.60)  Time: 1.060s,  241.55/s  (1.060s,  241.55/s)  LR: 8.157e-03  Data: 0.806 (0.806)
05/14/2023 12:55:45 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 2.735 (2.67)  Time: 0.257s,  996.94/s  (0.273s,  939.19/s)  LR: 8.157e-03  Data: 0.013 (0.028)
05/14/2023 12:55:58 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 2.824 (2.72)  Time: 0.259s,  989.17/s  (0.267s,  959.50/s)  LR: 8.157e-03  Data: 0.013 (0.022)
05/14/2023 12:55:59 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 2.647 (2.70)  Time: 0.232s, 1101.12/s  (0.266s,  961.67/s)  LR: 8.157e-03  Data: 0.000 (0.021)
05/14/2023 12:55:59 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:56:02 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 2.726 (2.73)  Time: 1.081s,  236.79/s  (1.081s,  236.79/s)  LR: 6.875e-03  Data: 0.830 (0.830)
05/14/2023 12:56:15 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 2.626 (2.68)  Time: 0.260s,  983.62/s  (0.274s,  932.79/s)  LR: 6.875e-03  Data: 0.013 (0.029)
05/14/2023 12:56:28 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 2.618 (2.66)  Time: 0.245s, 1044.56/s  (0.266s,  962.13/s)  LR: 6.875e-03  Data: 0.012 (0.021)
05/14/2023 12:56:29 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 2.665 (2.66)  Time: 0.238s, 1074.38/s  (0.266s,  964.16/s)  LR: 6.875e-03  Data: 0.000 (0.021)
05/14/2023 12:56:29 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:56:31 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 2.594 (2.59)  Time: 1.086s,  235.77/s  (1.086s,  235.77/s)  LR: 5.668e-03  Data: 0.837 (0.837)
05/14/2023 12:56:44 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 2.668 (2.63)  Time: 0.263s,  974.57/s  (0.274s,  935.44/s)  LR: 5.668e-03  Data: 0.015 (0.029)
05/14/2023 12:56:57 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 2.842 (2.70)  Time: 0.247s, 1035.19/s  (0.265s,  967.84/s)  LR: 5.668e-03  Data: 0.012 (0.021)
05/14/2023 12:56:58 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 2.556 (2.66)  Time: 0.235s, 1089.92/s  (0.264s,  969.91/s)  LR: 5.668e-03  Data: 0.000 (0.021)
05/14/2023 12:56:58 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:57:00 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.599 (2.60)  Time: 1.071s,  239.01/s  (1.071s,  239.01/s)  LR: 4.549e-03  Data: 0.833 (0.833)
05/14/2023 12:57:13 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 2.605 (2.60)  Time: 0.258s,  992.79/s  (0.272s,  939.95/s)  LR: 4.549e-03  Data: 0.013 (0.029)
05/14/2023 12:57:26 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 2.656 (2.62)  Time: 0.256s, 1001.31/s  (0.265s,  965.04/s)  LR: 4.549e-03  Data: 0.012 (0.021)
05/14/2023 12:57:27 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 2.567 (2.61)  Time: 0.245s, 1045.68/s  (0.265s,  967.07/s)  LR: 4.549e-03  Data: 0.000 (0.020)
05/14/2023 12:57:27 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:57:30 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 2.546 (2.55)  Time: 1.091s,  234.67/s  (1.091s,  234.67/s)  LR: 3.532e-03  Data: 0.844 (0.844)
05/14/2023 12:57:43 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 2.836 (2.69)  Time: 0.239s, 1069.40/s  (0.276s,  927.76/s)  LR: 3.532e-03  Data: 0.015 (0.029)
05/14/2023 12:57:56 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 2.590 (2.66)  Time: 0.242s, 1059.64/s  (0.266s,  961.91/s)  LR: 3.532e-03  Data: 0.012 (0.021)
05/14/2023 12:57:57 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 2.690 (2.67)  Time: 0.245s, 1044.15/s  (0.265s,  964.51/s)  LR: 3.532e-03  Data: 0.000 (0.021)
05/14/2023 12:57:57 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:57:59 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 2.593 (2.59)  Time: 1.075s,  238.22/s  (1.075s,  238.22/s)  LR: 2.626e-03  Data: 0.826 (0.826)
05/14/2023 12:58:12 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 2.633 (2.61)  Time: 0.261s,  979.98/s  (0.272s,  941.52/s)  LR: 2.626e-03  Data: 0.015 (0.029)
05/14/2023 12:58:25 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 2.701 (2.64)  Time: 0.259s,  989.00/s  (0.264s,  970.22/s)  LR: 2.626e-03  Data: 0.013 (0.021)
05/14/2023 12:58:26 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 2.572 (2.62)  Time: 0.247s, 1037.10/s  (0.263s,  972.14/s)  LR: 2.626e-03  Data: 0.000 (0.020)
05/14/2023 12:58:26 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:58:28 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 2.606 (2.61)  Time: 1.083s,  236.40/s  (1.083s,  236.40/s)  LR: 1.842e-03  Data: 0.834 (0.834)
05/14/2023 12:58:41 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 2.614 (2.61)  Time: 0.262s,  975.58/s  (0.274s,  936.01/s)  LR: 1.842e-03  Data: 0.012 (0.029)
05/14/2023 12:58:54 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 2.548 (2.59)  Time: 0.259s,  987.30/s  (0.265s,  964.23/s)  LR: 1.842e-03  Data: 0.012 (0.021)
05/14/2023 12:58:55 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 2.556 (2.58)  Time: 0.236s, 1086.99/s  (0.265s,  966.31/s)  LR: 1.842e-03  Data: 0.000 (0.020)
05/14/2023 12:58:55 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:58:57 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 2.942 (2.94)  Time: 1.028s,  249.01/s  (1.028s,  249.01/s)  LR: 1.189e-03  Data: 0.803 (0.803)
05/14/2023 12:59:10 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 2.550 (2.75)  Time: 0.250s, 1025.54/s  (0.273s,  936.93/s)  LR: 1.189e-03  Data: 0.012 (0.028)
05/14/2023 12:59:23 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 2.676 (2.72)  Time: 0.266s,  963.64/s  (0.265s,  964.51/s)  LR: 1.189e-03  Data: 0.012 (0.021)
05/14/2023 12:59:24 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 2.622 (2.70)  Time: 0.268s,  954.10/s  (0.265s,  965.19/s)  LR: 1.189e-03  Data: 0.000 (0.020)
05/14/2023 12:59:24 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:59:27 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 2.589 (2.59)  Time: 1.086s,  235.72/s  (1.086s,  235.72/s)  LR: 6.730e-04  Data: 0.849 (0.849)
05/14/2023 12:59:40 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 2.667 (2.63)  Time: 0.268s,  954.64/s  (0.275s,  931.61/s)  LR: 6.730e-04  Data: 0.012 (0.029)
05/14/2023 12:59:52 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 2.629 (2.63)  Time: 0.258s,  991.27/s  (0.266s,  964.18/s)  LR: 6.730e-04  Data: 0.013 (0.021)
05/14/2023 12:59:53 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 2.675 (2.64)  Time: 0.241s, 1060.86/s  (0.265s,  965.23/s)  LR: 6.730e-04  Data: 0.000 (0.020)
05/14/2023 12:59:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 12:59:56 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 2.880 (2.88)  Time: 1.044s,  245.25/s  (1.044s,  245.25/s)  LR: 3.005e-04  Data: 0.804 (0.804)
05/14/2023 13:00:09 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 2.668 (2.77)  Time: 0.259s,  989.17/s  (0.277s,  924.49/s)  LR: 3.005e-04  Data: 0.012 (0.029)
05/14/2023 13:00:22 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 2.765 (2.77)  Time: 0.255s, 1005.54/s  (0.267s,  960.49/s)  LR: 3.005e-04  Data: 0.012 (0.021)
05/14/2023 13:00:23 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 2.675 (2.75)  Time: 0.255s, 1004.78/s  (0.266s,  961.87/s)  LR: 3.005e-04  Data: 0.000 (0.020)
05/14/2023 13:00:23 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:00:26 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 2.633 (2.63)  Time: 1.137s,  225.08/s  (1.137s,  225.08/s)  LR: 7.532e-05  Data: 0.876 (0.876)
05/14/2023 13:00:38 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 2.629 (2.63)  Time: 0.262s,  977.34/s  (0.275s,  930.27/s)  LR: 7.532e-05  Data: 0.012 (0.030)
05/14/2023 13:00:51 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 2.618 (2.63)  Time: 0.256s,  998.76/s  (0.265s,  965.60/s)  LR: 7.532e-05  Data: 0.012 (0.021)
05/14/2023 13:00:52 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 2.685 (2.64)  Time: 0.245s, 1043.81/s  (0.264s,  968.31/s)  LR: 7.532e-05  Data: 0.000 (0.021)
05/14/2023 13:00:52 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 13:00:52 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/14/2023 13:00:54 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/14/2023 13:00:55 - INFO - train -   Test: [   0/39]  Time: 1.455 (1.455)  Loss:  1.0547 (1.0547)  Acc@1: 76.1719 (76.1719)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:00:58 - INFO - train -   Test: [  39/39]  Time: 0.064 (0.110)  Loss:  1.5586 (1.0390)  Acc@1: 50.0000 (77.3900)  Acc@5: 100.0000 (99.8500)
05/14/2023 13:00:58 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/14/2023 13:00:58 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/14/2023 13:00:59 - INFO - train -   Test: [   0/39]  Time: 0.610 (0.610)  Loss:  0.7598 (0.7598)  Acc@1: 87.8906 (87.8906)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:01 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/14/2023 13:01:01 - INFO - train -   Test: [  39/39]  Time: 0.013 (0.065)  Loss:  0.7451 (0.7040)  Acc@1: 87.5000 (88.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:01 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/14/2023 13:01:02 - INFO - train -   Test: [   0/39]  Time: 0.646 (0.646)  Loss:  0.7998 (0.7998)  Acc@1: 80.0781 (80.0781)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:04 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/14/2023 13:01:04 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.068)  Loss:  0.6831 (0.7659)  Acc@1: 87.5000 (83.4800)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:04 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/14/2023 13:01:05 - INFO - train -   Test: [   0/39]  Time: 0.628 (0.628)  Loss:  0.8237 (0.8237)  Acc@1: 80.8594 (80.8594)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:07 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.066)  Loss:  0.6689 (0.7644)  Acc@1: 87.5000 (83.7600)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:07 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/14/2023 13:01:07 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/14/2023 13:01:07 - INFO - train -   Test: [   0/39]  Time: 0.626 (0.626)  Loss:  0.8696 (0.8696)  Acc@1: 80.0781 (80.0781)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:09 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.064)  Loss:  0.7080 (0.8103)  Acc@1: 87.5000 (81.3700)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:09 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/14/2023 13:01:09 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/14/2023 13:01:10 - INFO - train -   Test: [   0/39]  Time: 0.606 (0.606)  Loss:  0.8506 (0.8506)  Acc@1: 83.5938 (83.5938)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:12 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.064)  Loss:  0.6152 (0.8406)  Acc@1: 100.0000 (85.7500)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:12 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/14/2023 13:01:12 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/14/2023 13:01:13 - INFO - train -   Test: [   0/39]  Time: 0.607 (0.607)  Loss:  0.9043 (0.9043)  Acc@1: 77.3438 (77.3438)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:15 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/14/2023 13:01:15 - INFO - train -   Test: [  39/39]  Time: 0.008 (0.063)  Loss:  0.7729 (0.8701)  Acc@1: 81.2500 (80.0300)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:15 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/14/2023 13:01:15 - INFO - train -   Test: [   0/39]  Time: 0.586 (0.586)  Loss:  0.9170 (0.9170)  Acc@1: 76.1719 (76.1719)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:17 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/14/2023 13:01:17 - INFO - train -   Test: [  39/39]  Time: 0.008 (0.063)  Loss:  0.8535 (0.8831)  Acc@1: 68.7500 (79.0200)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:17 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/14/2023 13:01:18 - INFO - train -   Test: [   0/39]  Time: 0.628 (0.628)  Loss:  1.0293 (1.0293)  Acc@1: 76.9531 (76.9531)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:20 - INFO - train -   Test: [  39/39]  Time: 0.009 (0.063)  Loss:  0.9277 (1.0012)  Acc@1: 75.0000 (76.3300)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:20 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/14/2023 13:01:20 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/14/2023 13:01:21 - INFO - train -   Test: [   0/39]  Time: 0.633 (0.633)  Loss:  1.1279 (1.1279)  Acc@1: 74.2188 (74.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 13:01:23 - INFO - train -   Test: [  39/39]  Time: 0.008 (0.065)  Loss:  1.1006 (1.1421)  Acc@1: 81.2500 (73.0500)  Acc@5: 100.0000 (100.0000)
