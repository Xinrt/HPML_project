04/26/2023 17:30:40 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
04/26/2023 17:30:40 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
04/26/2023 17:30:41 - INFO - train -   Model resnet34 created, param count:33651792
04/26/2023 17:30:49 - INFO - train -   Using native Torch AMP. Training in mixed precision.
04/26/2023 17:30:49 - INFO - train -   Using native Torch DistributedDataParallel.
04/26/2023 17:30:53 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
04/26/2023 17:30:58 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 7.074 (7.07)  Time: 4.897s,   52.27/s  (4.897s,   52.27/s)  LR: 5.500e-06  Data: 1.383 (1.383)
04/26/2023 17:31:11 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 6.869 (6.97)  Time: 0.260s,  984.73/s  (0.344s,  745.10/s)  LR: 5.500e-06  Data: 0.010 (0.039)
04/26/2023 17:31:24 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 6.915 (6.95)  Time: 0.260s,  983.94/s  (0.298s,  859.62/s)  LR: 5.500e-06  Data: 0.011 (0.026)
04/26/2023 17:31:24 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.912 (6.94)  Time: 0.227s, 1129.27/s  (0.295s,  866.39/s)  LR: 5.500e-06  Data: 0.000 (0.025)
04/26/2023 17:31:24 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:31:27 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.894 (6.89)  Time: 1.020s,  251.09/s  (1.020s,  251.09/s)  LR: 5.504e-03  Data: 0.778 (0.778)
04/26/2023 17:31:40 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 4.120 (5.51)  Time: 0.249s, 1026.84/s  (0.267s,  958.28/s)  LR: 5.504e-03  Data: 0.012 (0.027)
04/26/2023 17:31:52 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 3.883 (4.97)  Time: 0.252s, 1016.13/s  (0.259s,  986.59/s)  LR: 5.504e-03  Data: 0.011 (0.020)
04/26/2023 17:31:53 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 3.981 (4.72)  Time: 0.241s, 1062.35/s  (0.259s,  988.80/s)  LR: 5.504e-03  Data: 0.000 (0.019)
04/26/2023 17:31:53 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:31:58 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 3.801 (3.80)  Time: 3.245s,   78.88/s  (3.245s,   78.88/s)  LR: 1.100e-02  Data: 2.992 (2.992)
04/26/2023 17:32:11 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 3.587 (3.69)  Time: 0.263s,  972.65/s  (0.313s,  818.14/s)  LR: 1.100e-02  Data: 0.012 (0.071)
04/26/2023 17:32:24 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 3.321 (3.57)  Time: 0.261s,  979.55/s  (0.282s,  909.16/s)  LR: 1.100e-02  Data: 0.012 (0.042)
04/26/2023 17:32:25 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 3.496 (3.55)  Time: 0.242s, 1058.27/s  (0.280s,  912.72/s)  LR: 1.100e-02  Data: 0.000 (0.040)
04/26/2023 17:32:25 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:32:27 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 3.448 (3.45)  Time: 0.994s,  257.51/s  (0.994s,  257.51/s)  LR: 1.650e-02  Data: 0.749 (0.749)
04/26/2023 17:32:40 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 3.590 (3.52)  Time: 0.249s, 1026.19/s  (0.269s,  952.08/s)  LR: 1.650e-02  Data: 0.011 (0.026)
04/26/2023 17:32:53 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 3.080 (3.37)  Time: 0.247s, 1035.17/s  (0.259s,  986.55/s)  LR: 1.650e-02  Data: 0.012 (0.019)
04/26/2023 17:32:54 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 3.188 (3.33)  Time: 0.244s, 1050.07/s  (0.259s,  988.69/s)  LR: 1.650e-02  Data: 0.000 (0.019)
04/26/2023 17:32:54 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:32:56 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 3.114 (3.11)  Time: 1.039s,  246.29/s  (1.039s,  246.29/s)  LR: 2.200e-02  Data: 0.793 (0.793)
04/26/2023 17:33:09 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 3.081 (3.10)  Time: 0.244s, 1050.60/s  (0.268s,  953.55/s)  LR: 2.200e-02  Data: 0.013 (0.027)
04/26/2023 17:33:21 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 3.019 (3.07)  Time: 0.253s, 1013.75/s  (0.261s,  981.34/s)  LR: 2.200e-02  Data: 0.012 (0.019)
04/26/2023 17:33:22 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 3.049 (3.07)  Time: 0.228s, 1123.04/s  (0.260s,  983.71/s)  LR: 2.200e-02  Data: 0.000 (0.019)
04/26/2023 17:33:22 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:33:27 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 3.093 (3.09)  Time: 2.722s,   94.03/s  (2.722s,   94.03/s)  LR: 2.566e-02  Data: 2.468 (2.468)
04/26/2023 17:33:40 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 2.931 (3.01)  Time: 0.241s, 1062.45/s  (0.299s,  856.13/s)  LR: 2.566e-02  Data: 0.011 (0.060)
04/26/2023 17:33:53 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 2.970 (3.00)  Time: 0.252s, 1014.32/s  (0.277s,  925.85/s)  LR: 2.566e-02  Data: 0.011 (0.036)
04/26/2023 17:33:54 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 2.826 (2.96)  Time: 0.250s, 1024.60/s  (0.275s,  929.44/s)  LR: 2.566e-02  Data: 0.000 (0.035)
04/26/2023 17:33:54 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:33:56 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 2.933 (2.93)  Time: 1.014s,  252.48/s  (1.014s,  252.48/s)  LR: 2.487e-02  Data: 0.761 (0.761)
04/26/2023 17:34:09 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 3.035 (2.98)  Time: 0.262s,  977.85/s  (0.264s,  968.22/s)  LR: 2.487e-02  Data: 0.012 (0.027)
04/26/2023 17:34:21 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 2.977 (2.98)  Time: 0.246s, 1039.21/s  (0.259s,  990.19/s)  LR: 2.487e-02  Data: 0.011 (0.020)
04/26/2023 17:34:22 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 2.898 (2.96)  Time: 0.229s, 1118.54/s  (0.258s,  991.29/s)  LR: 2.487e-02  Data: 0.000 (0.019)
04/26/2023 17:34:22 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:34:25 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 2.966 (2.97)  Time: 1.019s,  251.26/s  (1.019s,  251.26/s)  LR: 2.397e-02  Data: 0.771 (0.771)
04/26/2023 17:34:38 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 2.867 (2.92)  Time: 0.252s, 1017.61/s  (0.269s,  952.51/s)  LR: 2.397e-02  Data: 0.010 (0.027)
04/26/2023 17:34:50 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 2.844 (2.89)  Time: 0.243s, 1052.73/s  (0.260s,  986.39/s)  LR: 2.397e-02  Data: 0.012 (0.020)
04/26/2023 17:34:51 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 2.858 (2.88)  Time: 0.229s, 1117.74/s  (0.259s,  988.52/s)  LR: 2.397e-02  Data: 0.000 (0.019)
04/26/2023 17:34:51 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:34:54 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 2.848 (2.85)  Time: 1.004s,  255.00/s  (1.004s,  255.00/s)  LR: 2.295e-02  Data: 0.755 (0.755)
04/26/2023 17:35:06 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 2.780 (2.81)  Time: 0.263s,  973.09/s  (0.268s,  955.55/s)  LR: 2.295e-02  Data: 0.012 (0.027)
04/26/2023 17:35:19 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 2.740 (2.79)  Time: 0.261s,  981.46/s  (0.260s,  986.27/s)  LR: 2.295e-02  Data: 0.012 (0.019)
04/26/2023 17:35:20 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 2.850 (2.80)  Time: 0.240s, 1065.09/s  (0.259s,  987.81/s)  LR: 2.295e-02  Data: 0.000 (0.019)
04/26/2023 17:35:20 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:35:23 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 2.849 (2.85)  Time: 1.252s,  204.55/s  (1.252s,  204.55/s)  LR: 2.183e-02  Data: 0.992 (0.992)
04/26/2023 17:35:36 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 2.911 (2.88)  Time: 0.233s, 1097.81/s  (0.271s,  946.27/s)  LR: 2.183e-02  Data: 0.011 (0.031)
04/26/2023 17:35:48 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 2.799 (2.85)  Time: 0.253s, 1013.82/s  (0.261s,  980.85/s)  LR: 2.183e-02  Data: 0.011 (0.022)
04/26/2023 17:35:49 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 2.939 (2.87)  Time: 0.249s, 1026.29/s  (0.261s,  982.08/s)  LR: 2.183e-02  Data: 0.000 (0.022)
04/26/2023 17:35:49 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:35:52 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 2.842 (2.84)  Time: 1.047s,  244.44/s  (1.047s,  244.44/s)  LR: 2.063e-02  Data: 0.803 (0.803)
04/26/2023 17:36:05 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 2.810 (2.83)  Time: 0.241s, 1064.38/s  (0.284s,  901.42/s)  LR: 2.063e-02  Data: 0.012 (0.041)
04/26/2023 17:36:18 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 2.718 (2.79)  Time: 0.252s, 1014.79/s  (0.268s,  954.21/s)  LR: 2.063e-02  Data: 0.013 (0.027)
04/26/2023 17:36:19 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 2.698 (2.77)  Time: 0.233s, 1100.49/s  (0.267s,  957.18/s)  LR: 2.063e-02  Data: 0.000 (0.026)
04/26/2023 17:36:19 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:36:22 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 2.774 (2.77)  Time: 1.046s,  244.63/s  (1.046s,  244.63/s)  LR: 1.934e-02  Data: 0.805 (0.805)
04/26/2023 17:36:34 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 2.695 (2.73)  Time: 0.258s,  991.42/s  (0.267s,  958.49/s)  LR: 1.934e-02  Data: 0.013 (0.028)
04/26/2023 17:36:47 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 2.728 (2.73)  Time: 0.247s, 1036.71/s  (0.259s,  989.82/s)  LR: 1.934e-02  Data: 0.011 (0.020)
04/26/2023 17:36:48 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 2.822 (2.75)  Time: 0.242s, 1057.83/s  (0.258s,  991.77/s)  LR: 1.934e-02  Data: 0.000 (0.019)
04/26/2023 17:36:48 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:36:50 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 2.962 (2.96)  Time: 1.131s,  226.28/s  (1.131s,  226.28/s)  LR: 1.800e-02  Data: 0.880 (0.880)
04/26/2023 17:37:03 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 2.911 (2.94)  Time: 0.240s, 1066.20/s  (0.267s,  957.40/s)  LR: 1.800e-02  Data: 0.011 (0.029)
04/26/2023 17:37:15 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 2.916 (2.93)  Time: 0.234s, 1093.02/s  (0.259s,  986.61/s)  LR: 1.800e-02  Data: 0.012 (0.021)
04/26/2023 17:37:16 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 2.700 (2.87)  Time: 0.242s, 1055.92/s  (0.259s,  988.27/s)  LR: 1.800e-02  Data: 0.000 (0.020)
04/26/2023 17:37:16 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:37:19 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 2.622 (2.62)  Time: 1.040s,  246.22/s  (1.040s,  246.22/s)  LR: 1.661e-02  Data: 0.781 (0.781)
04/26/2023 17:37:32 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 2.652 (2.64)  Time: 0.262s,  977.77/s  (0.269s,  950.77/s)  LR: 1.661e-02  Data: 0.012 (0.027)
04/26/2023 17:37:44 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 2.676 (2.65)  Time: 0.262s,  977.78/s  (0.260s,  982.81/s)  LR: 1.661e-02  Data: 0.012 (0.020)
04/26/2023 17:37:45 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 2.756 (2.68)  Time: 0.241s, 1063.30/s  (0.260s,  984.65/s)  LR: 1.661e-02  Data: 0.000 (0.019)
04/26/2023 17:37:45 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:37:48 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 2.803 (2.80)  Time: 1.068s,  239.77/s  (1.068s,  239.77/s)  LR: 1.519e-02  Data: 0.828 (0.828)
04/26/2023 17:38:01 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 2.601 (2.70)  Time: 0.256s,  998.32/s  (0.270s,  949.80/s)  LR: 1.519e-02  Data: 0.016 (0.029)
04/26/2023 17:38:13 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 2.807 (2.74)  Time: 0.250s, 1025.16/s  (0.259s,  986.94/s)  LR: 1.519e-02  Data: 0.012 (0.021)
04/26/2023 17:38:14 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 2.607 (2.70)  Time: 0.252s, 1017.25/s  (0.259s,  988.35/s)  LR: 1.519e-02  Data: 0.000 (0.020)
04/26/2023 17:38:14 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:38:19 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 2.878 (2.88)  Time: 2.972s,   86.13/s  (2.972s,   86.13/s)  LR: 1.375e-02  Data: 2.728 (2.728)
04/26/2023 17:38:32 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 2.675 (2.78)  Time: 0.255s, 1003.50/s  (0.306s,  835.66/s)  LR: 1.375e-02  Data: 0.012 (0.066)
04/26/2023 17:38:44 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 2.649 (2.73)  Time: 0.247s, 1038.46/s  (0.281s,  911.77/s)  LR: 1.375e-02  Data: 0.012 (0.040)
04/26/2023 17:38:45 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 2.670 (2.72)  Time: 0.241s, 1063.18/s  (0.279s,  916.43/s)  LR: 1.375e-02  Data: 0.000 (0.038)
04/26/2023 17:38:45 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:38:48 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 2.611 (2.61)  Time: 1.088s,  235.27/s  (1.088s,  235.27/s)  LR: 1.231e-02  Data: 0.827 (0.827)
04/26/2023 17:39:01 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 2.607 (2.61)  Time: 0.242s, 1055.85/s  (0.270s,  949.29/s)  LR: 1.231e-02  Data: 0.012 (0.028)
04/26/2023 17:39:13 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 2.694 (2.64)  Time: 0.241s, 1061.62/s  (0.261s,  981.94/s)  LR: 1.231e-02  Data: 0.011 (0.020)
04/26/2023 17:39:15 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 2.599 (2.63)  Time: 0.237s, 1081.89/s  (0.260s,  982.92/s)  LR: 1.231e-02  Data: 0.000 (0.020)
04/26/2023 17:39:15 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:39:17 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 2.717 (2.72)  Time: 1.100s,  232.63/s  (1.100s,  232.63/s)  LR: 1.089e-02  Data: 0.842 (0.842)
04/26/2023 17:39:30 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 2.924 (2.82)  Time: 0.252s, 1016.30/s  (0.270s,  947.86/s)  LR: 1.089e-02  Data: 0.013 (0.029)
04/26/2023 17:39:43 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 2.689 (2.78)  Time: 0.241s, 1063.19/s  (0.262s,  977.14/s)  LR: 1.089e-02  Data: 0.011 (0.021)
04/26/2023 17:39:44 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 2.674 (2.75)  Time: 0.228s, 1122.13/s  (0.261s,  980.20/s)  LR: 1.089e-02  Data: 0.000 (0.020)
04/26/2023 17:39:44 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:39:49 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 2.698 (2.70)  Time: 3.261s,   78.51/s  (3.261s,   78.51/s)  LR: 9.501e-03  Data: 2.996 (2.996)
04/26/2023 17:40:02 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 2.588 (2.64)  Time: 0.255s, 1003.35/s  (0.313s,  817.12/s)  LR: 9.501e-03  Data: 0.015 (0.071)
04/26/2023 17:40:14 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 2.920 (2.74)  Time: 0.234s, 1091.69/s  (0.283s,  903.33/s)  LR: 9.501e-03  Data: 0.013 (0.042)
04/26/2023 17:40:15 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 2.737 (2.74)  Time: 0.250s, 1023.48/s  (0.282s,  907.94/s)  LR: 9.501e-03  Data: 0.000 (0.041)
04/26/2023 17:40:15 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:40:18 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 2.601 (2.60)  Time: 1.013s,  252.79/s  (1.013s,  252.79/s)  LR: 8.157e-03  Data: 0.764 (0.764)
04/26/2023 17:40:31 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 2.732 (2.67)  Time: 0.253s, 1010.31/s  (0.270s,  948.40/s)  LR: 8.157e-03  Data: 0.012 (0.027)
04/26/2023 17:40:43 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 2.838 (2.72)  Time: 0.254s, 1006.45/s  (0.262s,  977.08/s)  LR: 8.157e-03  Data: 0.012 (0.020)
04/26/2023 17:40:44 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 2.651 (2.71)  Time: 0.230s, 1112.98/s  (0.261s,  979.15/s)  LR: 8.157e-03  Data: 0.000 (0.020)
04/26/2023 17:40:44 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:40:47 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 2.727 (2.73)  Time: 0.988s,  259.21/s  (0.988s,  259.21/s)  LR: 6.875e-03  Data: 0.740 (0.740)
04/26/2023 17:41:00 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 2.648 (2.69)  Time: 0.253s, 1013.07/s  (0.268s,  956.67/s)  LR: 6.875e-03  Data: 0.011 (0.027)
04/26/2023 17:41:12 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 2.617 (2.66)  Time: 0.241s, 1064.19/s  (0.260s,  984.29/s)  LR: 6.875e-03  Data: 0.011 (0.020)
04/26/2023 17:41:13 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 2.677 (2.67)  Time: 0.231s, 1107.67/s  (0.260s,  986.30/s)  LR: 6.875e-03  Data: 0.000 (0.019)
04/26/2023 17:41:13 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:41:16 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 2.594 (2.59)  Time: 1.019s,  251.18/s  (1.019s,  251.18/s)  LR: 5.668e-03  Data: 0.775 (0.775)
04/26/2023 17:41:28 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 2.656 (2.63)  Time: 0.262s,  976.98/s  (0.267s,  957.72/s)  LR: 5.668e-03  Data: 0.011 (0.027)
04/26/2023 17:41:41 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 2.849 (2.70)  Time: 0.242s, 1058.46/s  (0.259s,  988.83/s)  LR: 5.668e-03  Data: 0.011 (0.019)
04/26/2023 17:41:42 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 2.543 (2.66)  Time: 0.229s, 1117.33/s  (0.258s,  990.95/s)  LR: 5.668e-03  Data: 0.000 (0.019)
04/26/2023 17:41:42 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:41:44 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 2.603 (2.60)  Time: 1.038s,  246.66/s  (1.038s,  246.66/s)  LR: 4.549e-03  Data: 0.807 (0.807)
04/26/2023 17:41:57 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 2.604 (2.60)  Time: 0.257s,  994.84/s  (0.268s,  954.34/s)  LR: 4.549e-03  Data: 0.012 (0.028)
04/26/2023 17:42:10 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 2.650 (2.62)  Time: 0.253s, 1013.27/s  (0.261s,  982.64/s)  LR: 4.549e-03  Data: 0.012 (0.020)
04/26/2023 17:42:11 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 2.552 (2.60)  Time: 0.241s, 1062.73/s  (0.260s,  984.68/s)  LR: 4.549e-03  Data: 0.000 (0.019)
04/26/2023 17:42:11 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:42:13 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 2.545 (2.55)  Time: 1.098s,  233.17/s  (1.098s,  233.17/s)  LR: 3.532e-03  Data: 0.856 (0.856)
04/26/2023 17:42:26 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 2.836 (2.69)  Time: 0.236s, 1082.76/s  (0.270s,  947.67/s)  LR: 3.532e-03  Data: 0.012 (0.029)
04/26/2023 17:42:39 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 2.582 (2.65)  Time: 0.242s, 1059.11/s  (0.261s,  981.35/s)  LR: 3.532e-03  Data: 0.012 (0.020)
04/26/2023 17:42:40 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 2.687 (2.66)  Time: 0.242s, 1056.31/s  (0.260s,  983.47/s)  LR: 3.532e-03  Data: 0.000 (0.020)
04/26/2023 17:42:40 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:42:42 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 2.598 (2.60)  Time: 1.089s,  235.07/s  (1.089s,  235.07/s)  LR: 2.626e-03  Data: 0.850 (0.850)
04/26/2023 17:42:55 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 2.637 (2.62)  Time: 0.253s, 1013.12/s  (0.266s,  961.07/s)  LR: 2.626e-03  Data: 0.014 (0.028)
04/26/2023 17:43:07 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 2.711 (2.65)  Time: 0.253s, 1013.30/s  (0.259s,  990.09/s)  LR: 2.626e-03  Data: 0.012 (0.020)
04/26/2023 17:43:08 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 2.565 (2.63)  Time: 0.243s, 1052.09/s  (0.258s,  991.96/s)  LR: 2.626e-03  Data: 0.000 (0.020)
04/26/2023 17:43:08 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:43:11 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 2.602 (2.60)  Time: 1.052s,  243.36/s  (1.052s,  243.36/s)  LR: 1.842e-03  Data: 0.803 (0.803)
04/26/2023 17:43:24 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 2.614 (2.61)  Time: 0.254s, 1007.25/s  (0.268s,  954.93/s)  LR: 1.842e-03  Data: 0.012 (0.028)
04/26/2023 17:43:36 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 2.540 (2.59)  Time: 0.255s, 1003.73/s  (0.260s,  983.93/s)  LR: 1.842e-03  Data: 0.012 (0.020)
04/26/2023 17:43:37 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 2.558 (2.58)  Time: 0.230s, 1111.73/s  (0.259s,  986.81/s)  LR: 1.842e-03  Data: 0.000 (0.020)
04/26/2023 17:43:37 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:43:40 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 2.926 (2.93)  Time: 1.082s,  236.69/s  (1.082s,  236.69/s)  LR: 1.189e-03  Data: 0.854 (0.854)
04/26/2023 17:43:53 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 2.558 (2.74)  Time: 0.245s, 1044.94/s  (0.270s,  948.58/s)  LR: 1.189e-03  Data: 0.014 (0.029)
04/26/2023 17:44:05 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 2.680 (2.72)  Time: 0.261s,  979.33/s  (0.261s,  979.14/s)  LR: 1.189e-03  Data: 0.012 (0.021)
04/26/2023 17:44:06 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 2.634 (2.70)  Time: 0.238s, 1075.83/s  (0.261s,  981.15/s)  LR: 1.189e-03  Data: 0.000 (0.020)
04/26/2023 17:44:06 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:44:09 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 2.589 (2.59)  Time: 0.997s,  256.80/s  (0.997s,  256.80/s)  LR: 6.730e-04  Data: 0.759 (0.759)
04/26/2023 17:44:22 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 2.666 (2.63)  Time: 0.262s,  976.19/s  (0.269s,  952.71/s)  LR: 6.730e-04  Data: 0.012 (0.027)
04/26/2023 17:44:34 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 2.613 (2.62)  Time: 0.255s, 1003.51/s  (0.260s,  985.43/s)  LR: 6.730e-04  Data: 0.012 (0.020)
04/26/2023 17:44:35 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 2.676 (2.64)  Time: 0.236s, 1084.58/s  (0.260s,  986.34/s)  LR: 6.730e-04  Data: 0.000 (0.019)
04/26/2023 17:44:35 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:44:38 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 2.893 (2.89)  Time: 0.981s,  260.96/s  (0.981s,  260.96/s)  LR: 3.005e-04  Data: 0.747 (0.747)
04/26/2023 17:44:50 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 2.662 (2.78)  Time: 0.256s, 1001.66/s  (0.269s,  952.42/s)  LR: 3.005e-04  Data: 0.011 (0.027)
04/26/2023 17:45:03 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 2.759 (2.77)  Time: 0.249s, 1029.60/s  (0.260s,  985.85/s)  LR: 3.005e-04  Data: 0.011 (0.019)
04/26/2023 17:45:04 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 2.696 (2.75)  Time: 0.250s, 1023.53/s  (0.259s,  987.09/s)  LR: 3.005e-04  Data: 0.000 (0.019)
04/26/2023 17:45:04 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:45:07 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 2.636 (2.64)  Time: 1.039s,  246.37/s  (1.039s,  246.37/s)  LR: 7.532e-05  Data: 0.782 (0.782)
04/26/2023 17:45:19 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 2.626 (2.63)  Time: 0.249s, 1027.89/s  (0.269s,  951.02/s)  LR: 7.532e-05  Data: 0.012 (0.028)
04/26/2023 17:45:32 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 2.631 (2.63)  Time: 0.252s, 1014.31/s  (0.260s,  985.07/s)  LR: 7.532e-05  Data: 0.012 (0.020)
04/26/2023 17:45:33 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 2.685 (2.64)  Time: 0.241s, 1062.19/s  (0.259s,  987.83/s)  LR: 7.532e-05  Data: 0.000 (0.019)
04/26/2023 17:45:33 - INFO - train -   Distributing BatchNorm running means and vars
04/26/2023 17:45:33 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
04/26/2023 17:45:34 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
04/26/2023 17:45:35 - INFO - train -   Test: [   0/39]  Time: 0.987 (0.987)  Loss:  1.0664 (1.0664)  Acc@1: 76.9531 (76.9531)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:37 - INFO - train -   Test: [  39/39]  Time: 0.062 (0.066)  Loss:  1.6035 (1.0482)  Acc@1: 50.0000 (76.8400)  Acc@5: 100.0000 (99.7800)
04/26/2023 17:45:37 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
04/26/2023 17:45:37 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
04/26/2023 17:45:38 - INFO - train -   Test: [   0/39]  Time: 0.572 (0.572)  Loss:  0.7261 (0.7261)  Acc@1: 87.1094 (87.1094)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:40 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.062)  Loss:  0.7158 (0.6897)  Acc@1: 87.5000 (88.4900)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:40 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
04/26/2023 17:45:40 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
04/26/2023 17:45:41 - INFO - train -   Test: [   0/39]  Time: 0.605 (0.605)  Loss:  0.7939 (0.7939)  Acc@1: 81.6406 (81.6406)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:42 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
04/26/2023 17:45:42 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.063)  Loss:  0.6738 (0.7605)  Acc@1: 87.5000 (83.7500)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:42 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
04/26/2023 17:45:43 - INFO - train -   Test: [   0/39]  Time: 0.598 (0.598)  Loss:  0.7993 (0.7993)  Acc@1: 81.6406 (81.6406)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:45 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
04/26/2023 17:45:45 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.064)  Loss:  0.7109 (0.7590)  Acc@1: 81.2500 (84.0300)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:45 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
04/26/2023 17:45:46 - INFO - train -   Test: [   0/39]  Time: 0.591 (0.591)  Loss:  0.8623 (0.8623)  Acc@1: 80.0781 (80.0781)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:48 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
04/26/2023 17:45:48 - INFO - train -   Test: [  39/39]  Time: 0.009 (0.062)  Loss:  0.7563 (0.8100)  Acc@1: 81.2500 (81.5800)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:48 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
04/26/2023 17:45:48 - INFO - train -   Test: [   0/39]  Time: 0.593 (0.593)  Loss:  0.8467 (0.8467)  Acc@1: 85.1562 (85.1562)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:50 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
04/26/2023 17:45:50 - INFO - train -   Test: [  39/39]  Time: 0.016 (0.061)  Loss:  0.6040 (0.8399)  Acc@1: 100.0000 (85.7000)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:50 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
04/26/2023 17:45:51 - INFO - train -   Test: [   0/39]  Time: 0.584 (0.584)  Loss:  0.9092 (0.9092)  Acc@1: 76.5625 (76.5625)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:53 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
04/26/2023 17:45:53 - INFO - train -   Test: [  39/39]  Time: 0.008 (0.061)  Loss:  0.7910 (0.8761)  Acc@1: 87.5000 (79.8900)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:53 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
04/26/2023 17:45:54 - INFO - train -   Test: [   0/39]  Time: 0.574 (0.574)  Loss:  0.9277 (0.9277)  Acc@1: 75.3906 (75.3906)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:55 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
04/26/2023 17:45:55 - INFO - train -   Test: [  39/39]  Time: 0.008 (0.061)  Loss:  0.8438 (0.8891)  Acc@1: 75.0000 (79.0200)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:55 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
04/26/2023 17:45:56 - INFO - train -   Test: [   0/39]  Time: 0.611 (0.611)  Loss:  1.0322 (1.0322)  Acc@1: 75.3906 (75.3906)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:58 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
04/26/2023 17:45:58 - INFO - train -   Test: [  39/39]  Time: 0.007 (0.060)  Loss:  0.9531 (1.0077)  Acc@1: 75.0000 (76.3000)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:45:58 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
04/26/2023 17:45:59 - INFO - train -   Test: [   0/39]  Time: 0.572 (0.572)  Loss:  1.1289 (1.1289)  Acc@1: 73.0469 (73.0469)  Acc@5: 100.0000 (100.0000)
04/26/2023 17:46:00 - INFO - train -   Test: [  39/39]  Time: 0.007 (0.059)  Loss:  1.0898 (1.1436)  Acc@1: 68.7500 (73.1800)  Acc@5: 100.0000 (100.0000)
