05/14/2023 02:08:24 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 02:08:24 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 02:08:31 - INFO - train -   Model resnet18 created, param count:68123984
05/14/2023 02:09:18 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 02:09:18 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 02:09:30 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 02:09:38 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 6.835 (6.83)  Time: 8.517s,   30.06/s  (8.517s,   30.06/s)  LR: 5.500e-06  Data: 1.302 (1.302)
05/14/2023 02:09:59 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 7.348 (7.09)  Time: 0.414s,  618.93/s  (0.579s,  442.36/s)  LR: 5.500e-06  Data: 0.011 (0.037)
05/14/2023 02:10:19 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 8.188 (7.46)  Time: 0.392s,  653.11/s  (0.489s,  523.15/s)  LR: 5.500e-06  Data: 0.012 (0.025)
05/14/2023 02:10:21 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.779 (7.29)  Time: 0.392s,  653.27/s  (0.484s,  528.69/s)  LR: 5.500e-06  Data: 0.000 (0.024)
05/14/2023 02:10:21 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:10:25 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.752 (6.75)  Time: 1.059s,  241.76/s  (1.059s,  241.76/s)  LR: 5.504e-03  Data: 0.646 (0.646)
05/14/2023 02:10:45 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 6.060 (6.41)  Time: 0.327s,  783.66/s  (0.409s,  626.15/s)  LR: 5.504e-03  Data: 0.012 (0.024)
05/14/2023 02:11:05 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 5.713 (6.18)  Time: 0.426s,  601.05/s  (0.404s,  634.06/s)  LR: 5.504e-03  Data: 0.012 (0.019)
05/14/2023 02:11:06 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 5.765 (6.07)  Time: 0.390s,  655.73/s  (0.403s,  634.96/s)  LR: 5.504e-03  Data: 0.000 (0.018)
05/14/2023 02:11:06 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:11:10 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 5.707 (5.71)  Time: 1.035s,  247.26/s  (1.035s,  247.26/s)  LR: 1.100e-02  Data: 0.590 (0.590)
05/14/2023 02:11:30 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.979 (5.34)  Time: 0.435s,  588.47/s  (0.405s,  632.39/s)  LR: 1.100e-02  Data: 0.011 (0.024)
05/14/2023 02:11:50 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.534 (5.07)  Time: 0.474s,  540.26/s  (0.400s,  639.53/s)  LR: 1.100e-02  Data: 0.012 (0.018)
05/14/2023 02:11:51 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.775 (5.00)  Time: 0.424s,  603.22/s  (0.400s,  639.92/s)  LR: 1.100e-02  Data: 0.000 (0.018)
05/14/2023 02:11:51 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:11:56 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 4.726 (4.73)  Time: 1.019s,  251.32/s  (1.019s,  251.32/s)  LR: 1.650e-02  Data: 0.613 (0.613)
05/14/2023 02:12:16 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 4.670 (4.70)  Time: 0.405s,  631.37/s  (0.410s,  623.98/s)  LR: 1.650e-02  Data: 0.015 (0.024)
05/14/2023 02:12:35 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 4.502 (4.63)  Time: 0.372s,  687.94/s  (0.404s,  633.53/s)  LR: 1.650e-02  Data: 0.012 (0.018)
05/14/2023 02:12:37 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 4.298 (4.55)  Time: 0.473s,  541.20/s  (0.405s,  632.51/s)  LR: 1.650e-02  Data: 0.000 (0.018)
05/14/2023 02:12:37 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:12:41 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 4.494 (4.49)  Time: 1.033s,  247.84/s  (1.033s,  247.84/s)  LR: 2.200e-02  Data: 0.690 (0.690)
05/14/2023 02:13:01 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 4.454 (4.47)  Time: 0.371s,  690.38/s  (0.408s,  628.15/s)  LR: 2.200e-02  Data: 0.012 (0.025)
05/14/2023 02:13:21 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 4.528 (4.49)  Time: 0.427s,  598.92/s  (0.402s,  636.28/s)  LR: 2.200e-02  Data: 0.013 (0.019)
05/14/2023 02:13:22 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 4.427 (4.48)  Time: 0.374s,  684.46/s  (0.402s,  636.74/s)  LR: 2.200e-02  Data: 0.000 (0.019)
05/14/2023 02:13:22 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:13:26 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 4.384 (4.38)  Time: 0.974s,  262.90/s  (0.974s,  262.90/s)  LR: 2.566e-02  Data: 0.583 (0.583)
05/14/2023 02:13:46 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 4.316 (4.35)  Time: 0.346s,  740.31/s  (0.409s,  626.49/s)  LR: 2.566e-02  Data: 0.012 (0.023)
05/14/2023 02:14:06 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 4.354 (4.35)  Time: 0.405s,  632.61/s  (0.404s,  634.30/s)  LR: 2.566e-02  Data: 0.013 (0.018)
05/14/2023 02:14:08 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 4.326 (4.35)  Time: 0.424s,  603.09/s  (0.404s,  633.65/s)  LR: 2.566e-02  Data: 0.000 (0.018)
05/14/2023 02:14:08 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:14:12 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 4.273 (4.27)  Time: 1.025s,  249.72/s  (1.025s,  249.72/s)  LR: 2.487e-02  Data: 0.640 (0.640)
05/14/2023 02:14:32 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 4.270 (4.27)  Time: 0.363s,  706.04/s  (0.404s,  633.49/s)  LR: 2.487e-02  Data: 0.012 (0.024)
05/14/2023 02:14:51 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 4.308 (4.28)  Time: 0.418s,  612.56/s  (0.401s,  638.67/s)  LR: 2.487e-02  Data: 0.012 (0.019)
05/14/2023 02:14:53 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 4.311 (4.29)  Time: 0.401s,  638.80/s  (0.400s,  640.78/s)  LR: 2.487e-02  Data: 0.000 (0.018)
05/14/2023 02:14:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:14:57 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 4.219 (4.22)  Time: 0.892s,  286.99/s  (0.892s,  286.99/s)  LR: 2.397e-02  Data: 0.537 (0.537)
05/14/2023 02:15:17 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 4.218 (4.22)  Time: 0.392s,  653.75/s  (0.406s,  630.76/s)  LR: 2.397e-02  Data: 0.012 (0.023)
05/14/2023 02:15:37 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 4.066 (4.17)  Time: 0.333s,  769.77/s  (0.401s,  637.87/s)  LR: 2.397e-02  Data: 0.013 (0.018)
05/14/2023 02:15:38 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 4.099 (4.15)  Time: 0.341s,  749.90/s  (0.399s,  641.44/s)  LR: 2.397e-02  Data: 0.000 (0.018)
05/14/2023 02:15:38 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:15:42 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 4.049 (4.05)  Time: 1.004s,  254.91/s  (1.004s,  254.91/s)  LR: 2.295e-02  Data: 0.626 (0.626)
05/14/2023 02:16:02 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 4.045 (4.05)  Time: 0.385s,  665.64/s  (0.410s,  625.12/s)  LR: 2.295e-02  Data: 0.013 (0.025)
05/14/2023 02:16:22 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 4.066 (4.05)  Time: 0.360s,  710.81/s  (0.400s,  640.33/s)  LR: 2.295e-02  Data: 0.012 (0.019)
05/14/2023 02:16:23 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 3.961 (4.03)  Time: 0.350s,  731.18/s  (0.398s,  642.49/s)  LR: 2.295e-02  Data: 0.000 (0.018)
05/14/2023 02:16:23 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:16:27 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 4.078 (4.08)  Time: 1.035s,  247.34/s  (1.035s,  247.34/s)  LR: 2.183e-02  Data: 0.580 (0.580)
05/14/2023 02:16:47 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 4.037 (4.06)  Time: 0.343s,  746.36/s  (0.408s,  627.27/s)  LR: 2.183e-02  Data: 0.013 (0.024)
05/14/2023 02:17:07 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 3.975 (4.03)  Time: 0.325s,  787.42/s  (0.402s,  637.51/s)  LR: 2.183e-02  Data: 0.012 (0.018)
05/14/2023 02:17:08 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 4.068 (4.04)  Time: 0.391s,  653.91/s  (0.402s,  637.17/s)  LR: 2.183e-02  Data: 0.000 (0.018)
05/14/2023 02:17:08 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:17:12 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 4.021 (4.02)  Time: 0.950s,  269.35/s  (0.950s,  269.35/s)  LR: 2.063e-02  Data: 0.575 (0.575)
05/14/2023 02:17:32 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 3.980 (4.00)  Time: 0.392s,  653.40/s  (0.404s,  634.41/s)  LR: 2.063e-02  Data: 0.011 (0.024)
05/14/2023 02:17:52 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 4.037 (4.01)  Time: 0.434s,  589.37/s  (0.397s,  644.39/s)  LR: 2.063e-02  Data: 0.013 (0.018)
05/14/2023 02:17:53 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 4.030 (4.02)  Time: 0.363s,  706.03/s  (0.397s,  644.64/s)  LR: 2.063e-02  Data: 0.000 (0.018)
05/14/2023 02:17:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:17:57 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 3.982 (3.98)  Time: 1.040s,  246.27/s  (1.040s,  246.27/s)  LR: 1.934e-02  Data: 0.600 (0.600)
05/14/2023 02:18:18 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 3.931 (3.96)  Time: 0.335s,  764.82/s  (0.418s,  612.50/s)  LR: 1.934e-02  Data: 0.012 (0.024)
05/14/2023 02:18:37 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 3.966 (3.96)  Time: 0.403s,  635.28/s  (0.406s,  630.92/s)  LR: 1.934e-02  Data: 0.012 (0.018)
05/14/2023 02:18:39 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 3.966 (3.96)  Time: 0.328s,  779.73/s  (0.404s,  632.94/s)  LR: 1.934e-02  Data: 0.000 (0.018)
05/14/2023 02:18:39 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:18:43 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 4.043 (4.04)  Time: 1.065s,  240.35/s  (1.065s,  240.35/s)  LR: 1.800e-02  Data: 0.615 (0.615)
05/14/2023 02:19:03 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 3.955 (4.00)  Time: 0.402s,  637.47/s  (0.414s,  617.86/s)  LR: 1.800e-02  Data: 0.013 (0.024)
05/14/2023 02:19:23 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 3.994 (4.00)  Time: 0.346s,  740.59/s  (0.403s,  634.76/s)  LR: 1.800e-02  Data: 0.012 (0.019)
05/14/2023 02:19:24 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 4.098 (4.02)  Time: 0.425s,  602.02/s  (0.402s,  636.44/s)  LR: 1.800e-02  Data: 0.000 (0.018)
05/14/2023 02:19:24 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:19:28 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 4.081 (4.08)  Time: 1.039s,  246.43/s  (1.039s,  246.43/s)  LR: 1.661e-02  Data: 0.625 (0.625)
05/14/2023 02:19:48 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 3.949 (4.01)  Time: 0.361s,  708.82/s  (0.403s,  634.48/s)  LR: 1.661e-02  Data: 0.013 (0.025)
05/14/2023 02:20:08 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 3.937 (3.99)  Time: 0.368s,  695.59/s  (0.401s,  638.35/s)  LR: 1.661e-02  Data: 0.012 (0.019)
05/14/2023 02:20:09 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 3.979 (3.99)  Time: 0.306s,  835.61/s  (0.401s,  637.92/s)  LR: 1.661e-02  Data: 0.000 (0.018)
05/14/2023 02:20:09 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:20:14 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 3.985 (3.98)  Time: 1.049s,  243.95/s  (1.049s,  243.95/s)  LR: 1.519e-02  Data: 0.654 (0.654)
05/14/2023 02:20:34 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 3.905 (3.94)  Time: 0.399s,  641.26/s  (0.416s,  615.25/s)  LR: 1.519e-02  Data: 0.013 (0.025)
05/14/2023 02:20:54 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 3.955 (3.95)  Time: 0.348s,  735.82/s  (0.408s,  627.95/s)  LR: 1.519e-02  Data: 0.012 (0.019)
05/14/2023 02:20:55 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 3.957 (3.95)  Time: 0.391s,  654.14/s  (0.406s,  629.78/s)  LR: 1.519e-02  Data: 0.000 (0.019)
05/14/2023 02:20:55 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:20:59 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 3.869 (3.87)  Time: 1.135s,  225.55/s  (1.135s,  225.55/s)  LR: 1.375e-02  Data: 0.639 (0.639)
05/14/2023 02:21:19 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 3.876 (3.87)  Time: 0.361s,  709.05/s  (0.412s,  622.00/s)  LR: 1.375e-02  Data: 0.012 (0.025)
05/14/2023 02:21:39 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 3.974 (3.91)  Time: 0.430s,  595.50/s  (0.405s,  632.24/s)  LR: 1.375e-02  Data: 0.013 (0.019)
05/14/2023 02:21:41 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 3.971 (3.92)  Time: 0.394s,  650.02/s  (0.404s,  633.09/s)  LR: 1.375e-02  Data: 0.000 (0.018)
05/14/2023 02:21:41 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:21:45 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 3.907 (3.91)  Time: 0.967s,  264.82/s  (0.967s,  264.82/s)  LR: 1.231e-02  Data: 0.608 (0.608)
05/14/2023 02:22:05 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 4.057 (3.98)  Time: 0.418s,  611.80/s  (0.407s,  629.07/s)  LR: 1.231e-02  Data: 0.013 (0.025)
05/14/2023 02:22:24 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 4.024 (4.00)  Time: 0.436s,  586.93/s  (0.399s,  641.67/s)  LR: 1.231e-02  Data: 0.013 (0.019)
05/14/2023 02:22:26 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 3.947 (3.98)  Time: 0.340s,  752.65/s  (0.398s,  642.65/s)  LR: 1.231e-02  Data: 0.000 (0.018)
05/14/2023 02:22:26 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:22:31 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 3.994 (3.99)  Time: 1.373s,  186.42/s  (1.373s,  186.42/s)  LR: 1.089e-02  Data: 0.925 (0.925)
05/14/2023 02:22:51 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 3.934 (3.96)  Time: 0.373s,  685.52/s  (0.419s,  610.89/s)  LR: 1.089e-02  Data: 0.012 (0.031)
05/14/2023 02:23:11 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 3.751 (3.89)  Time: 0.487s,  525.48/s  (0.414s,  618.91/s)  LR: 1.089e-02  Data: 0.012 (0.022)
05/14/2023 02:23:13 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 4.006 (3.92)  Time: 0.410s,  624.71/s  (0.413s,  619.97/s)  LR: 1.089e-02  Data: 0.000 (0.021)
05/14/2023 02:23:13 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:23:16 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 3.870 (3.87)  Time: 1.039s,  246.48/s  (1.039s,  246.48/s)  LR: 9.501e-03  Data: 0.654 (0.654)
05/14/2023 02:23:36 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 3.922 (3.90)  Time: 0.356s,  719.53/s  (0.411s,  622.24/s)  LR: 9.501e-03  Data: 0.013 (0.027)
05/14/2023 02:23:56 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 3.814 (3.87)  Time: 0.334s,  765.82/s  (0.407s,  628.46/s)  LR: 9.501e-03  Data: 0.012 (0.020)
05/14/2023 02:23:58 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 3.915 (3.88)  Time: 0.354s,  723.87/s  (0.405s,  631.94/s)  LR: 9.501e-03  Data: 0.000 (0.019)
05/14/2023 02:23:58 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:24:02 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 3.858 (3.86)  Time: 1.041s,  246.02/s  (1.041s,  246.02/s)  LR: 8.157e-03  Data: 0.669 (0.669)
05/14/2023 02:24:22 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 3.971 (3.91)  Time: 0.417s,  614.38/s  (0.410s,  624.14/s)  LR: 8.157e-03  Data: 0.012 (0.026)
05/14/2023 02:24:41 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 3.984 (3.94)  Time: 0.415s,  616.19/s  (0.403s,  635.62/s)  LR: 8.157e-03  Data: 0.011 (0.019)
05/14/2023 02:24:43 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 3.891 (3.93)  Time: 0.371s,  689.59/s  (0.401s,  637.64/s)  LR: 8.157e-03  Data: 0.000 (0.019)
05/14/2023 02:24:43 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:24:47 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 3.903 (3.90)  Time: 0.994s,  257.60/s  (0.994s,  257.60/s)  LR: 6.875e-03  Data: 0.590 (0.590)
05/14/2023 02:25:06 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 3.804 (3.85)  Time: 0.360s,  711.20/s  (0.401s,  638.07/s)  LR: 6.875e-03  Data: 0.012 (0.024)
05/14/2023 02:25:27 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 3.970 (3.89)  Time: 0.418s,  612.77/s  (0.403s,  634.58/s)  LR: 6.875e-03  Data: 0.013 (0.019)
05/14/2023 02:25:28 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 3.817 (3.87)  Time: 0.369s,  694.57/s  (0.403s,  634.70/s)  LR: 6.875e-03  Data: 0.000 (0.018)
05/14/2023 02:25:28 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:25:32 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 3.888 (3.89)  Time: 0.991s,  258.28/s  (0.991s,  258.28/s)  LR: 5.668e-03  Data: 0.624 (0.624)
05/14/2023 02:25:52 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 3.830 (3.86)  Time: 0.489s,  523.82/s  (0.407s,  629.40/s)  LR: 5.668e-03  Data: 0.013 (0.025)
05/14/2023 02:26:12 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 3.845 (3.85)  Time: 0.490s,  522.32/s  (0.402s,  636.33/s)  LR: 5.668e-03  Data: 0.014 (0.019)
05/14/2023 02:26:14 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 3.824 (3.85)  Time: 0.340s,  751.92/s  (0.403s,  635.98/s)  LR: 5.668e-03  Data: 0.000 (0.018)
05/14/2023 02:26:14 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:26:18 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 3.826 (3.83)  Time: 1.075s,  238.17/s  (1.075s,  238.17/s)  LR: 4.549e-03  Data: 0.654 (0.654)
05/14/2023 02:26:38 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 3.798 (3.81)  Time: 0.343s,  746.63/s  (0.413s,  619.12/s)  LR: 4.549e-03  Data: 0.012 (0.025)
05/14/2023 02:26:57 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 3.811 (3.81)  Time: 0.367s,  698.09/s  (0.404s,  633.57/s)  LR: 4.549e-03  Data: 0.012 (0.019)
05/14/2023 02:26:59 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 3.943 (3.84)  Time: 0.387s,  661.58/s  (0.404s,  634.29/s)  LR: 4.549e-03  Data: 0.000 (0.019)
05/14/2023 02:26:59 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:27:03 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 3.904 (3.90)  Time: 1.001s,  255.68/s  (1.001s,  255.68/s)  LR: 3.532e-03  Data: 0.636 (0.636)
05/14/2023 02:27:23 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 3.920 (3.91)  Time: 0.419s,  610.73/s  (0.402s,  637.36/s)  LR: 3.532e-03  Data: 0.013 (0.025)
05/14/2023 02:27:43 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 3.907 (3.91)  Time: 0.427s,  600.03/s  (0.400s,  640.29/s)  LR: 3.532e-03  Data: 0.013 (0.019)
05/14/2023 02:27:44 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 3.814 (3.89)  Time: 0.469s,  546.36/s  (0.401s,  638.09/s)  LR: 3.532e-03  Data: 0.000 (0.019)
05/14/2023 02:27:44 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:27:48 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 3.794 (3.79)  Time: 0.966s,  264.91/s  (0.966s,  264.91/s)  LR: 2.626e-03  Data: 0.587 (0.587)
05/14/2023 02:28:08 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 3.914 (3.85)  Time: 0.440s,  581.73/s  (0.402s,  637.56/s)  LR: 2.626e-03  Data: 0.015 (0.024)
05/14/2023 02:28:27 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 3.987 (3.90)  Time: 0.434s,  590.11/s  (0.394s,  649.30/s)  LR: 2.626e-03  Data: 0.012 (0.019)
05/14/2023 02:28:29 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 3.945 (3.91)  Time: 0.420s,  608.89/s  (0.394s,  649.92/s)  LR: 2.626e-03  Data: 0.000 (0.018)
05/14/2023 02:28:29 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:28:33 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 3.910 (3.91)  Time: 1.034s,  247.59/s  (1.034s,  247.59/s)  LR: 1.842e-03  Data: 0.622 (0.622)
05/14/2023 02:28:53 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 3.940 (3.92)  Time: 0.373s,  685.72/s  (0.404s,  632.99/s)  LR: 1.842e-03  Data: 0.013 (0.025)
05/14/2023 02:29:13 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 3.879 (3.91)  Time: 0.384s,  666.86/s  (0.406s,  630.81/s)  LR: 1.842e-03  Data: 0.013 (0.019)
05/14/2023 02:29:15 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 3.753 (3.87)  Time: 0.462s,  553.73/s  (0.406s,  629.85/s)  LR: 1.842e-03  Data: 0.000 (0.018)
05/14/2023 02:29:15 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:29:19 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 3.850 (3.85)  Time: 1.350s,  189.59/s  (1.350s,  189.59/s)  LR: 1.189e-03  Data: 0.950 (0.950)
05/14/2023 02:29:39 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 3.862 (3.86)  Time: 0.345s,  741.70/s  (0.420s,  608.96/s)  LR: 1.189e-03  Data: 0.015 (0.031)
05/14/2023 02:29:59 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 3.856 (3.86)  Time: 0.341s,  750.49/s  (0.410s,  623.81/s)  LR: 1.189e-03  Data: 0.013 (0.022)
05/14/2023 02:30:01 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 3.585 (3.79)  Time: 0.470s,  544.65/s  (0.410s,  624.64/s)  LR: 1.189e-03  Data: 0.000 (0.022)
05/14/2023 02:30:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:30:05 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 3.918 (3.92)  Time: 1.070s,  239.22/s  (1.070s,  239.22/s)  LR: 6.730e-04  Data: 0.680 (0.680)
05/14/2023 02:30:25 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 3.865 (3.89)  Time: 0.347s,  737.92/s  (0.422s,  606.51/s)  LR: 6.730e-04  Data: 0.011 (0.026)
05/14/2023 02:30:45 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 3.834 (3.87)  Time: 0.380s,  673.72/s  (0.411s,  622.23/s)  LR: 6.730e-04  Data: 0.013 (0.019)
05/14/2023 02:30:47 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 3.879 (3.87)  Time: 0.368s,  695.72/s  (0.410s,  623.93/s)  LR: 6.730e-04  Data: 0.000 (0.019)
05/14/2023 02:30:47 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:30:51 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 3.787 (3.79)  Time: 1.037s,  246.93/s  (1.037s,  246.93/s)  LR: 3.005e-04  Data: 0.668 (0.668)
05/14/2023 02:31:11 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 3.939 (3.86)  Time: 0.426s,  601.62/s  (0.410s,  625.15/s)  LR: 3.005e-04  Data: 0.012 (0.025)
05/14/2023 02:31:30 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 3.864 (3.86)  Time: 0.367s,  697.66/s  (0.398s,  643.36/s)  LR: 3.005e-04  Data: 0.013 (0.019)
05/14/2023 02:31:32 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 3.919 (3.88)  Time: 0.390s,  657.16/s  (0.398s,  643.25/s)  LR: 3.005e-04  Data: 0.000 (0.019)
05/14/2023 02:31:32 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:31:36 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 3.667 (3.67)  Time: 1.107s,  231.19/s  (1.107s,  231.19/s)  LR: 7.532e-05  Data: 0.618 (0.618)
05/14/2023 02:31:55 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 3.871 (3.77)  Time: 0.356s,  719.63/s  (0.404s,  633.59/s)  LR: 7.532e-05  Data: 0.013 (0.024)
05/14/2023 02:32:15 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 3.856 (3.80)  Time: 0.385s,  665.36/s  (0.396s,  646.52/s)  LR: 7.532e-05  Data: 0.012 (0.018)
05/14/2023 02:32:16 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 3.873 (3.82)  Time: 0.413s,  619.58/s  (0.396s,  647.23/s)  LR: 7.532e-05  Data: 0.000 (0.018)
05/14/2023 02:32:16 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:32:16 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 02:32:19 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 02:32:21 - INFO - train -   Test: [   0/39]  Time: 1.241 (1.241)  Loss:  0.8037 (0.8037)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:23 - INFO - train -   Test: [  39/39]  Time: 0.070 (0.095)  Loss:  0.7705 (0.8036)  Acc@1: 100.0000 (99.8900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:23 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 02:32:23 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 02:32:25 - INFO - train -   Test: [   0/39]  Time: 0.653 (0.653)  Loss:  1.5703 (1.5703)  Acc@1: 93.7500 (93.7500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:29 - INFO - train -   Test: [  39/39]  Time: 0.202 (0.120)  Loss:  2.0293 (1.6446)  Acc@1: 87.5000 (92.0800)  Acc@5: 100.0000 (99.9800)
05/14/2023 02:32:29 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 02:32:29 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 02:32:30 - INFO - train -   Test: [   0/39]  Time: 0.669 (0.669)  Loss:  1.2207 (1.2207)  Acc@1: 98.0469 (98.0469)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:33 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 02:32:33 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.107)  Loss:  1.1699 (1.2038)  Acc@1: 100.0000 (94.9500)  Acc@5: 100.0000 (99.9900)
05/14/2023 02:32:33 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 02:32:34 - INFO - train -   Test: [   0/39]  Time: 0.630 (0.630)  Loss:  0.7510 (0.7510)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:38 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 02:32:38 - INFO - train -   Test: [  39/39]  Time: 0.026 (0.105)  Loss:  0.7534 (0.7541)  Acc@1: 100.0000 (99.1100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:38 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 02:32:39 - INFO - train -   Test: [   0/39]  Time: 0.672 (0.672)  Loss:  1.5723 (1.5723)  Acc@1: 97.6562 (97.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:43 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 02:32:43 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.105)  Loss:  1.0312 (1.5647)  Acc@1: 100.0000 (98.8100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:43 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 02:32:44 - INFO - train -   Test: [   0/39]  Time: 0.660 (0.660)  Loss:  1.8340 (1.8340)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:47 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.102)  Loss:  1.7656 (1.8533)  Acc@1: 100.0000 (99.9100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:47 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 02:32:47 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 02:32:48 - INFO - train -   Test: [   0/39]  Time: 0.661 (0.661)  Loss:  1.5850 (1.5850)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:51 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 02:32:51 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.099)  Loss:  1.4590 (1.5947)  Acc@1: 100.0000 (99.5400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:51 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 02:32:53 - INFO - train -   Test: [   0/39]  Time: 0.807 (0.807)  Loss:  1.6211 (1.6211)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:56 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 02:32:56 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.101)  Loss:  1.6104 (1.6973)  Acc@1: 93.7500 (99.3800)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:32:56 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 02:32:57 - INFO - train -   Test: [   0/39]  Time: 0.615 (0.615)  Loss:  1.5127 (1.5127)  Acc@1: 98.8281 (98.8281)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:00 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 02:33:00 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.095)  Loss:  1.5137 (1.5041)  Acc@1: 100.0000 (99.4200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:00 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 02:33:01 - INFO - train -   Test: [   0/39]  Time: 0.641 (0.641)  Loss:  0.8066 (0.8066)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:04 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 02:33:04 - INFO - train -   Test: [  39/39]  Time: 0.022 (0.094)  Loss:  0.8687 (0.8138)  Acc@1: 100.0000 (98.8800)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:04 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 02:33:05 - INFO - train -   Test: [   0/39]  Time: 0.619 (0.619)  Loss:  1.1387 (1.1387)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:08 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 02:33:08 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.092)  Loss:  1.1211 (1.1424)  Acc@1: 100.0000 (99.8400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:08 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 02:33:09 - INFO - train -   Test: [   0/39]  Time: 0.649 (0.649)  Loss:  0.7500 (0.7500)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:12 - INFO - train -   Test: [  39/39]  Time: 0.020 (0.089)  Loss:  0.7891 (0.7613)  Acc@1: 100.0000 (99.8300)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:12 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 02:33:12 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 02:33:13 - INFO - train -   Test: [   0/39]  Time: 0.590 (0.590)  Loss:  1.0215 (1.0215)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:16 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.087)  Loss:  1.1416 (1.0336)  Acc@1: 93.7500 (99.8300)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:16 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 02:33:16 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 02:33:17 - INFO - train -   Test: [   0/39]  Time: 0.685 (0.685)  Loss:  0.8921 (0.8921)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:20 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 02:33:20 - INFO - train -   Test: [  39/39]  Time: 0.018 (0.086)  Loss:  0.8799 (0.8990)  Acc@1: 100.0000 (99.9900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:20 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 02:33:21 - INFO - train -   Test: [   0/39]  Time: 0.657 (0.657)  Loss:  1.1445 (1.1445)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:23 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.083)  Loss:  1.2168 (1.1361)  Acc@1: 100.0000 (99.8900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:23 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 02:33:23 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 02:33:24 - INFO - train -   Test: [   0/39]  Time: 0.612 (0.612)  Loss:  0.9424 (0.9424)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:27 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.081)  Loss:  0.9893 (0.9207)  Acc@1: 100.0000 (99.8200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:27 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 02:33:27 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 02:33:28 - INFO - train -   Test: [   0/39]  Time: 0.626 (0.626)  Loss:  0.9478 (0.9478)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:30 - INFO - train -   Test: [  39/39]  Time: 0.018 (0.080)  Loss:  0.9717 (0.9326)  Acc@1: 100.0000 (99.7000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:30 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 02:33:30 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 02:33:31 - INFO - train -   Test: [   0/39]  Time: 0.720 (0.720)  Loss:  0.8794 (0.8794)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:34 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.080)  Loss:  0.8848 (0.8724)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:34 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 02:33:34 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 02:33:35 - INFO - train -   Test: [   0/39]  Time: 0.617 (0.617)  Loss:  0.9624 (0.9624)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:38 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 02:33:38 - INFO - train -   Test: [  39/39]  Time: 0.018 (0.098)  Loss:  0.9746 (0.9596)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:38 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 02:33:39 - INFO - train -   Test: [   0/39]  Time: 0.606 (0.606)  Loss:  1.0195 (1.0195)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:41 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 02:33:41 - INFO - train -   Test: [  39/39]  Time: 0.016 (0.074)  Loss:  1.0215 (1.0288)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:41 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 02:33:42 - INFO - train -   Test: [   0/39]  Time: 0.617 (0.617)  Loss:  0.7339 (0.7339)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:45 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 02:33:45 - INFO - train -   Test: [  39/39]  Time: 0.016 (0.073)  Loss:  0.7480 (0.7392)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:45 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 02:33:45 - INFO - train -   Test: [   0/39]  Time: 0.616 (0.616)  Loss:  0.8149 (0.8149)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:48 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 02:33:48 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.071)  Loss:  0.8667 (0.8253)  Acc@1: 100.0000 (99.9300)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:48 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 02:33:48 - INFO - train -   Test: [   0/39]  Time: 0.572 (0.572)  Loss:  1.0352 (1.0352)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:51 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.071)  Loss:  1.0557 (1.0519)  Acc@1: 100.0000 (99.9400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:51 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 02:33:51 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 02:33:52 - INFO - train -   Test: [   0/39]  Time: 0.612 (0.612)  Loss:  0.9170 (0.9170)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:54 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 02:33:54 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.067)  Loss:  0.9526 (0.9292)  Acc@1: 100.0000 (99.9900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:54 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 02:33:55 - INFO - train -   Test: [   0/39]  Time: 0.620 (0.620)  Loss:  0.7471 (0.7471)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:57 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 02:33:57 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.067)  Loss:  0.7310 (0.7544)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:33:57 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 02:33:58 - INFO - train -   Test: [   0/39]  Time: 0.600 (0.600)  Loss:  0.9302 (0.9302)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:34:00 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 02:34:00 - INFO - train -   Test: [  39/39]  Time: 0.012 (0.066)  Loss:  0.8872 (0.9178)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:34:00 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 02:34:00 - INFO - train -   Test: [   0/39]  Time: 0.590 (0.590)  Loss:  0.7900 (0.7900)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:34:03 - INFO - train -   Test: [  39/39]  Time: 0.013 (0.067)  Loss:  0.7988 (0.7856)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
