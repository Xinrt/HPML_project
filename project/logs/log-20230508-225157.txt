05/08/2023 22:51:57 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 4, device cuda:1.
05/08/2023 22:51:57 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 3, total 4, device cuda:3.
05/08/2023 22:51:57 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 2, total 4, device cuda:2.
05/08/2023 22:51:57 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 4, device cuda:0.
05/08/2023 22:52:01 - INFO - train -   Model resnet18 created, param count:48868688
05/08/2023 22:52:13 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/08/2023 22:52:13 - INFO - train -   Using native Torch DistributedDataParallel.
05/08/2023 22:52:25 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/08/2023 22:52:32 - INFO - train -   Train: 0 [   0/52 (  0%)]  Loss: 8.250 (8.25)  Time: 7.529s,   68.01/s  (7.529s,   68.01/s)  LR: 5.500e-06  Data: 2.367 (2.367)
05/08/2023 22:52:50 - INFO - train -   Train: 0 [  50/52 ( 98%)]  Loss: 8.182 (8.22)  Time: 0.325s, 1577.15/s  (0.509s, 1006.39/s)  LR: 5.500e-06  Data: 0.014 (0.059)
05/08/2023 22:52:51 - INFO - train -   Train: 0 [  51/52 (100%)]  Loss: 7.746 (8.06)  Time: 0.309s, 1655.32/s  (0.505s, 1014.03/s)  LR: 5.500e-06  Data: 0.000 (0.058)
05/08/2023 22:52:51 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:52:55 - INFO - train -   Train: 1 [   0/52 (  0%)]  Loss: 7.892 (7.89)  Time: 1.221s,  419.29/s  (1.221s,  419.29/s)  LR: 5.504e-03  Data: 0.902 (0.902)
05/08/2023 22:53:11 - INFO - train -   Train: 1 [  50/52 ( 98%)]  Loss: 5.979 (6.94)  Time: 0.371s, 1379.38/s  (0.352s, 1455.74/s)  LR: 5.504e-03  Data: 0.014 (0.030)
05/08/2023 22:53:12 - INFO - train -   Train: 1 [  51/52 (100%)]  Loss: 5.717 (6.53)  Time: 0.357s, 1432.26/s  (0.352s, 1455.28/s)  LR: 5.504e-03  Data: 0.000 (0.030)
05/08/2023 22:53:12 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:53:15 - INFO - train -   Train: 2 [   0/52 (  0%)]  Loss: 5.522 (5.52)  Time: 1.253s,  408.63/s  (1.253s,  408.63/s)  LR: 1.100e-02  Data: 0.932 (0.932)
05/08/2023 22:53:33 - INFO - train -   Train: 2 [  50/52 ( 98%)]  Loss: 4.275 (4.90)  Time: 0.320s, 1599.66/s  (0.364s, 1404.98/s)  LR: 1.100e-02  Data: 0.012 (0.031)
05/08/2023 22:53:33 - INFO - train -   Train: 2 [  51/52 (100%)]  Loss: 4.537 (4.78)  Time: 0.358s, 1430.32/s  (0.364s, 1405.46/s)  LR: 1.100e-02  Data: 0.000 (0.031)
05/08/2023 22:53:33 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:53:37 - INFO - train -   Train: 3 [   0/52 (  0%)]  Loss: 4.315 (4.31)  Time: 1.294s,  395.82/s  (1.294s,  395.82/s)  LR: 1.650e-02  Data: 0.979 (0.979)
05/08/2023 22:53:54 - INFO - train -   Train: 3 [  50/52 ( 98%)]  Loss: 4.039 (4.18)  Time: 0.319s, 1603.14/s  (0.360s, 1421.64/s)  LR: 1.650e-02  Data: 0.014 (0.032)
05/08/2023 22:53:54 - INFO - train -   Train: 3 [  51/52 (100%)]  Loss: 4.071 (4.14)  Time: 0.310s, 1649.05/s  (0.359s, 1425.42/s)  LR: 1.650e-02  Data: 0.000 (0.031)
05/08/2023 22:53:54 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:53:58 - INFO - train -   Train: 4 [   0/52 (  0%)]  Loss: 4.430 (4.43)  Time: 1.351s,  378.97/s  (1.351s,  378.97/s)  LR: 2.200e-02  Data: 0.973 (0.973)
05/08/2023 22:54:15 - INFO - train -   Train: 4 [  50/52 ( 98%)]  Loss: 3.912 (4.17)  Time: 0.301s, 1699.91/s  (0.364s, 1407.24/s)  LR: 2.200e-02  Data: 0.015 (0.032)
05/08/2023 22:54:16 - INFO - train -   Train: 4 [  51/52 (100%)]  Loss: 3.910 (4.08)  Time: 0.357s, 1435.90/s  (0.364s, 1407.78/s)  LR: 2.200e-02  Data: 0.000 (0.032)
05/08/2023 22:54:16 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:54:20 - INFO - train -   Train: 5 [   0/52 (  0%)]  Loss: 4.127 (4.13)  Time: 1.369s,  374.00/s  (1.369s,  374.00/s)  LR: 2.566e-02  Data: 0.997 (0.997)
05/08/2023 22:54:37 - INFO - train -   Train: 5 [  50/52 ( 98%)]  Loss: 4.001 (4.06)  Time: 0.374s, 1367.70/s  (0.361s, 1416.69/s)  LR: 2.566e-02  Data: 0.014 (0.032)
05/08/2023 22:54:37 - INFO - train -   Train: 5 [  51/52 (100%)]  Loss: 3.845 (3.99)  Time: 0.361s, 1416.39/s  (0.361s, 1416.68/s)  LR: 2.566e-02  Data: 0.000 (0.031)
05/08/2023 22:54:37 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:54:41 - INFO - train -   Train: 6 [   0/52 (  0%)]  Loss: 3.774 (3.77)  Time: 1.339s,  382.47/s  (1.339s,  382.47/s)  LR: 2.487e-02  Data: 1.018 (1.018)
05/08/2023 22:54:58 - INFO - train -   Train: 6 [  50/52 ( 98%)]  Loss: 3.734 (3.75)  Time: 0.375s, 1366.81/s  (0.357s, 1433.82/s)  LR: 2.487e-02  Data: 0.013 (0.032)
05/08/2023 22:54:58 - INFO - train -   Train: 6 [  51/52 (100%)]  Loss: 3.699 (3.74)  Time: 0.310s, 1653.95/s  (0.356s, 1437.50/s)  LR: 2.487e-02  Data: 0.000 (0.032)
05/08/2023 22:54:58 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:55:01 - INFO - train -   Train: 7 [   0/52 (  0%)]  Loss: 3.759 (3.76)  Time: 1.318s,  388.55/s  (1.318s,  388.55/s)  LR: 2.397e-02  Data: 0.961 (0.961)
05/08/2023 22:55:18 - INFO - train -   Train: 7 [  50/52 ( 98%)]  Loss: 3.486 (3.62)  Time: 0.320s, 1598.90/s  (0.354s, 1447.34/s)  LR: 2.397e-02  Data: 0.014 (0.032)
05/08/2023 22:55:18 - INFO - train -   Train: 7 [  51/52 (100%)]  Loss: 3.556 (3.60)  Time: 0.302s, 1695.14/s  (0.353s, 1451.42/s)  LR: 2.397e-02  Data: 0.000 (0.031)
05/08/2023 22:55:18 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:55:22 - INFO - train -   Train: 8 [   0/52 (  0%)]  Loss: 3.553 (3.55)  Time: 1.191s,  430.00/s  (1.191s,  430.00/s)  LR: 2.295e-02  Data: 0.887 (0.887)
05/08/2023 22:55:39 - INFO - train -   Train: 8 [  50/52 ( 98%)]  Loss: 3.396 (3.47)  Time: 0.326s, 1570.20/s  (0.359s, 1428.13/s)  LR: 2.295e-02  Data: 0.012 (0.030)
05/08/2023 22:55:39 - INFO - train -   Train: 8 [  51/52 (100%)]  Loss: 3.723 (3.56)  Time: 0.360s, 1423.29/s  (0.359s, 1428.04/s)  LR: 2.295e-02  Data: 0.000 (0.029)
05/08/2023 22:55:39 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:55:43 - INFO - train -   Train: 9 [   0/52 (  0%)]  Loss: 3.453 (3.45)  Time: 1.093s,  468.27/s  (1.093s,  468.27/s)  LR: 2.183e-02  Data: 0.779 (0.779)
05/08/2023 22:56:00 - INFO - train -   Train: 9 [  50/52 ( 98%)]  Loss: 3.657 (3.56)  Time: 0.377s, 1358.38/s  (0.354s, 1445.74/s)  LR: 2.183e-02  Data: 0.012 (0.027)
05/08/2023 22:56:01 - INFO - train -   Train: 9 [  51/52 (100%)]  Loss: 3.501 (3.54)  Time: 0.360s, 1421.51/s  (0.354s, 1445.26/s)  LR: 2.183e-02  Data: 0.000 (0.027)
05/08/2023 22:56:01 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:56:05 - INFO - train -   Train: 10 [   0/52 (  0%)]  Loss: 3.428 (3.43)  Time: 1.314s,  389.60/s  (1.314s,  389.60/s)  LR: 2.063e-02  Data: 1.003 (1.003)
05/08/2023 22:56:21 - INFO - train -   Train: 10 [  50/52 ( 98%)]  Loss: 3.534 (3.48)  Time: 0.365s, 1401.94/s  (0.358s, 1429.91/s)  LR: 2.063e-02  Data: 0.011 (0.032)
05/08/2023 22:56:22 - INFO - train -   Train: 10 [  51/52 (100%)]  Loss: 3.379 (3.45)  Time: 0.354s, 1446.42/s  (0.358s, 1430.22/s)  LR: 2.063e-02  Data: 0.000 (0.032)
05/08/2023 22:56:22 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:56:25 - INFO - train -   Train: 11 [   0/52 (  0%)]  Loss: 3.343 (3.34)  Time: 1.190s,  430.17/s  (1.190s,  430.17/s)  LR: 1.934e-02  Data: 0.861 (0.861)
05/08/2023 22:56:42 - INFO - train -   Train: 11 [  50/52 ( 98%)]  Loss: 3.327 (3.33)  Time: 0.323s, 1587.23/s  (0.352s, 1454.56/s)  LR: 1.934e-02  Data: 0.013 (0.030)
05/08/2023 22:56:43 - INFO - train -   Train: 11 [  51/52 (100%)]  Loss: 3.304 (3.32)  Time: 0.306s, 1675.21/s  (0.351s, 1458.26/s)  LR: 1.934e-02  Data: 0.000 (0.029)
05/08/2023 22:56:43 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:56:46 - INFO - train -   Train: 12 [   0/52 (  0%)]  Loss: 3.183 (3.18)  Time: 1.236s,  414.27/s  (1.236s,  414.27/s)  LR: 1.800e-02  Data: 0.931 (0.931)
05/08/2023 22:57:03 - INFO - train -   Train: 12 [  50/52 ( 98%)]  Loss: 3.222 (3.20)  Time: 0.320s, 1601.89/s  (0.352s, 1455.23/s)  LR: 1.800e-02  Data: 0.012 (0.031)
05/08/2023 22:57:03 - INFO - train -   Train: 12 [  51/52 (100%)]  Loss: 3.390 (3.27)  Time: 0.309s, 1657.57/s  (0.351s, 1458.66/s)  LR: 1.800e-02  Data: 0.000 (0.030)
05/08/2023 22:57:03 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:57:07 - INFO - train -   Train: 13 [   0/52 (  0%)]  Loss: 3.212 (3.21)  Time: 1.214s,  421.90/s  (1.214s,  421.90/s)  LR: 1.661e-02  Data: 0.898 (0.898)
05/08/2023 22:57:24 - INFO - train -   Train: 13 [  50/52 ( 98%)]  Loss: 3.253 (3.23)  Time: 0.312s, 1643.41/s  (0.360s, 1422.29/s)  LR: 1.661e-02  Data: 0.014 (0.030)
05/08/2023 22:57:24 - INFO - train -   Train: 13 [  51/52 (100%)]  Loss: 3.197 (3.22)  Time: 0.306s, 1673.14/s  (0.359s, 1426.40/s)  LR: 1.661e-02  Data: 0.000 (0.030)
05/08/2023 22:57:24 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:57:28 - INFO - train -   Train: 14 [   0/52 (  0%)]  Loss: 3.511 (3.51)  Time: 1.343s,  381.23/s  (1.343s,  381.23/s)  LR: 1.519e-02  Data: 0.957 (0.957)
05/08/2023 22:57:45 - INFO - train -   Train: 14 [  50/52 ( 98%)]  Loss: 3.289 (3.40)  Time: 0.372s, 1378.09/s  (0.364s, 1406.48/s)  LR: 1.519e-02  Data: 0.013 (0.031)
05/08/2023 22:57:45 - INFO - train -   Train: 14 [  51/52 (100%)]  Loss: 3.286 (3.36)  Time: 0.355s, 1442.86/s  (0.364s, 1407.16/s)  LR: 1.519e-02  Data: 0.000 (0.031)
05/08/2023 22:57:45 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:57:49 - INFO - train -   Train: 15 [   0/52 (  0%)]  Loss: 3.265 (3.27)  Time: 1.269s,  403.55/s  (1.269s,  403.55/s)  LR: 1.375e-02  Data: 0.953 (0.953)
05/08/2023 22:58:06 - INFO - train -   Train: 15 [  50/52 ( 98%)]  Loss: 3.157 (3.21)  Time: 0.297s, 1724.39/s  (0.357s, 1435.40/s)  LR: 1.375e-02  Data: 0.013 (0.031)
05/08/2023 22:58:06 - INFO - train -   Train: 15 [  51/52 (100%)]  Loss: 3.207 (3.21)  Time: 0.302s, 1694.74/s  (0.356s, 1439.64/s)  LR: 1.375e-02  Data: 0.000 (0.031)
05/08/2023 22:58:06 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:58:10 - INFO - train -   Train: 16 [   0/52 (  0%)]  Loss: 3.308 (3.31)  Time: 1.213s,  422.02/s  (1.213s,  422.02/s)  LR: 1.231e-02  Data: 0.876 (0.876)
05/08/2023 22:58:26 - INFO - train -   Train: 16 [  50/52 ( 98%)]  Loss: 3.253 (3.28)  Time: 0.365s, 1401.19/s  (0.354s, 1446.82/s)  LR: 1.231e-02  Data: 0.011 (0.030)
05/08/2023 22:58:27 - INFO - train -   Train: 16 [  51/52 (100%)]  Loss: 3.203 (3.25)  Time: 0.300s, 1705.50/s  (0.353s, 1451.06/s)  LR: 1.231e-02  Data: 0.000 (0.029)
05/08/2023 22:58:27 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:58:30 - INFO - train -   Train: 17 [   0/52 (  0%)]  Loss: 3.206 (3.21)  Time: 1.236s,  414.22/s  (1.236s,  414.22/s)  LR: 1.089e-02  Data: 0.938 (0.938)
05/08/2023 22:58:47 - INFO - train -   Train: 17 [  50/52 ( 98%)]  Loss: 3.186 (3.20)  Time: 0.300s, 1707.19/s  (0.348s, 1470.86/s)  LR: 1.089e-02  Data: 0.012 (0.031)
05/08/2023 22:58:47 - INFO - train -   Train: 17 [  51/52 (100%)]  Loss: 3.227 (3.21)  Time: 0.317s, 1617.59/s  (0.347s, 1473.43/s)  LR: 1.089e-02  Data: 0.000 (0.030)
05/08/2023 22:58:47 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:58:51 - INFO - train -   Train: 18 [   0/52 (  0%)]  Loss: 3.224 (3.22)  Time: 1.388s,  368.97/s  (1.388s,  368.97/s)  LR: 9.501e-03  Data: 1.022 (1.022)
05/08/2023 22:59:08 - INFO - train -   Train: 18 [  50/52 ( 98%)]  Loss: 3.124 (3.17)  Time: 0.294s, 1738.84/s  (0.356s, 1437.68/s)  LR: 9.501e-03  Data: 0.014 (0.033)
05/08/2023 22:59:08 - INFO - train -   Train: 18 [  51/52 (100%)]  Loss: 3.133 (3.16)  Time: 0.308s, 1661.05/s  (0.355s, 1441.41/s)  LR: 9.501e-03  Data: 0.000 (0.032)
05/08/2023 22:59:08 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:59:12 - INFO - train -   Train: 19 [   0/52 (  0%)]  Loss: 3.193 (3.19)  Time: 1.245s,  411.39/s  (1.245s,  411.39/s)  LR: 8.157e-03  Data: 0.926 (0.926)
05/08/2023 22:59:28 - INFO - train -   Train: 19 [  50/52 ( 98%)]  Loss: 3.185 (3.19)  Time: 0.330s, 1553.87/s  (0.350s, 1464.71/s)  LR: 8.157e-03  Data: 0.012 (0.031)
05/08/2023 22:59:29 - INFO - train -   Train: 19 [  51/52 (100%)]  Loss: 3.254 (3.21)  Time: 0.358s, 1431.00/s  (0.350s, 1464.05/s)  LR: 8.157e-03  Data: 0.000 (0.030)
05/08/2023 22:59:29 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:59:32 - INFO - train -   Train: 20 [   0/52 (  0%)]  Loss: 3.230 (3.23)  Time: 1.270s,  403.27/s  (1.270s,  403.27/s)  LR: 6.875e-03  Data: 0.917 (0.917)
05/08/2023 22:59:50 - INFO - train -   Train: 20 [  50/52 ( 98%)]  Loss: 3.234 (3.23)  Time: 0.373s, 1374.07/s  (0.372s, 1375.00/s)  LR: 6.875e-03  Data: 0.013 (0.031)
05/08/2023 22:59:50 - INFO - train -   Train: 20 [  51/52 (100%)]  Loss: 3.153 (3.21)  Time: 0.307s, 1666.78/s  (0.371s, 1379.65/s)  LR: 6.875e-03  Data: 0.000 (0.030)
05/08/2023 22:59:50 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:59:54 - INFO - train -   Train: 21 [   0/52 (  0%)]  Loss: 3.165 (3.16)  Time: 1.189s,  430.78/s  (1.189s,  430.78/s)  LR: 5.668e-03  Data: 0.877 (0.877)
05/08/2023 23:00:11 - INFO - train -   Train: 21 [  50/52 ( 98%)]  Loss: 3.143 (3.15)  Time: 0.317s, 1612.69/s  (0.356s, 1439.59/s)  LR: 5.668e-03  Data: 0.014 (0.031)
05/08/2023 23:00:11 - INFO - train -   Train: 21 [  51/52 (100%)]  Loss: 3.221 (3.18)  Time: 0.311s, 1643.76/s  (0.355s, 1443.04/s)  LR: 5.668e-03  Data: 0.000 (0.030)
05/08/2023 23:00:11 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:00:15 - INFO - train -   Train: 22 [   0/52 (  0%)]  Loss: 3.105 (3.11)  Time: 1.349s,  379.63/s  (1.349s,  379.63/s)  LR: 4.549e-03  Data: 1.029 (1.029)
05/08/2023 23:00:32 - INFO - train -   Train: 22 [  50/52 ( 98%)]  Loss: 3.124 (3.11)  Time: 0.329s, 1554.25/s  (0.367s, 1396.07/s)  LR: 4.549e-03  Data: 0.015 (0.033)
05/08/2023 23:00:33 - INFO - train -   Train: 22 [  51/52 (100%)]  Loss: 3.107 (3.11)  Time: 0.310s, 1650.79/s  (0.366s, 1400.22/s)  LR: 4.549e-03  Data: 0.000 (0.032)
05/08/2023 23:00:33 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:00:36 - INFO - train -   Train: 23 [   0/52 (  0%)]  Loss: 3.178 (3.18)  Time: 1.151s,  444.94/s  (1.151s,  444.94/s)  LR: 3.532e-03  Data: 0.841 (0.841)
05/08/2023 23:00:53 - INFO - train -   Train: 23 [  50/52 ( 98%)]  Loss: 3.049 (3.11)  Time: 0.319s, 1603.87/s  (0.351s, 1457.11/s)  LR: 3.532e-03  Data: 0.014 (0.029)
05/08/2023 23:00:53 - INFO - train -   Train: 23 [  51/52 (100%)]  Loss: 3.158 (3.13)  Time: 0.361s, 1420.20/s  (0.352s, 1456.39/s)  LR: 3.532e-03  Data: 0.000 (0.029)
05/08/2023 23:00:53 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:00:57 - INFO - train -   Train: 24 [   0/52 (  0%)]  Loss: 3.030 (3.03)  Time: 1.232s,  415.75/s  (1.232s,  415.75/s)  LR: 2.626e-03  Data: 0.916 (0.916)
05/08/2023 23:01:14 - INFO - train -   Train: 24 [  50/52 ( 98%)]  Loss: 3.170 (3.10)  Time: 0.323s, 1583.41/s  (0.353s, 1449.43/s)  LR: 2.626e-03  Data: 0.013 (0.031)
05/08/2023 23:01:14 - INFO - train -   Train: 24 [  51/52 (100%)]  Loss: 3.011 (3.07)  Time: 0.286s, 1791.81/s  (0.352s, 1454.78/s)  LR: 2.626e-03  Data: 0.000 (0.030)
05/08/2023 23:01:14 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:01:18 - INFO - train -   Train: 25 [   0/52 (  0%)]  Loss: 3.194 (3.19)  Time: 1.213s,  422.17/s  (1.213s,  422.17/s)  LR: 1.842e-03  Data: 0.856 (0.856)
05/08/2023 23:01:35 - INFO - train -   Train: 25 [  50/52 ( 98%)]  Loss: 3.189 (3.19)  Time: 0.375s, 1367.12/s  (0.359s, 1428.09/s)  LR: 1.842e-03  Data: 0.012 (0.030)
05/08/2023 23:01:35 - INFO - train -   Train: 25 [  51/52 (100%)]  Loss: 3.175 (3.19)  Time: 0.306s, 1672.46/s  (0.358s, 1432.12/s)  LR: 1.842e-03  Data: 0.000 (0.029)
05/08/2023 23:01:35 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:01:39 - INFO - train -   Train: 26 [   0/52 (  0%)]  Loss: 3.131 (3.13)  Time: 1.162s,  440.50/s  (1.162s,  440.50/s)  LR: 1.189e-03  Data: 0.852 (0.852)
05/08/2023 23:01:55 - INFO - train -   Train: 26 [  50/52 ( 98%)]  Loss: 3.160 (3.15)  Time: 0.317s, 1615.27/s  (0.353s, 1452.21/s)  LR: 1.189e-03  Data: 0.014 (0.029)
05/08/2023 23:01:56 - INFO - train -   Train: 26 [  51/52 (100%)]  Loss: 3.151 (3.15)  Time: 0.302s, 1695.06/s  (0.352s, 1456.23/s)  LR: 1.189e-03  Data: 0.000 (0.029)
05/08/2023 23:01:56 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:01:59 - INFO - train -   Train: 27 [   0/52 (  0%)]  Loss: 3.219 (3.22)  Time: 1.278s,  400.59/s  (1.278s,  400.59/s)  LR: 6.730e-04  Data: 0.921 (0.921)
05/08/2023 23:02:16 - INFO - train -   Train: 27 [  50/52 ( 98%)]  Loss: 3.209 (3.21)  Time: 0.322s, 1589.03/s  (0.355s, 1444.06/s)  LR: 6.730e-04  Data: 0.013 (0.031)
05/08/2023 23:02:17 - INFO - train -   Train: 27 [  51/52 (100%)]  Loss: 3.226 (3.22)  Time: 0.358s, 1431.43/s  (0.355s, 1443.82/s)  LR: 6.730e-04  Data: 0.000 (0.030)
05/08/2023 23:02:17 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:02:20 - INFO - train -   Train: 28 [   0/52 (  0%)]  Loss: 3.040 (3.04)  Time: 1.324s,  386.82/s  (1.324s,  386.82/s)  LR: 3.005e-04  Data: 1.018 (1.018)
05/08/2023 23:02:37 - INFO - train -   Train: 28 [  50/52 ( 98%)]  Loss: 3.017 (3.03)  Time: 0.297s, 1725.37/s  (0.355s, 1441.19/s)  LR: 3.005e-04  Data: 0.011 (0.032)
05/08/2023 23:02:37 - INFO - train -   Train: 28 [  51/52 (100%)]  Loss: 3.102 (3.05)  Time: 0.306s, 1671.68/s  (0.354s, 1445.03/s)  LR: 3.005e-04  Data: 0.000 (0.032)
05/08/2023 23:02:37 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:02:41 - INFO - train -   Train: 29 [   0/52 (  0%)]  Loss: 3.062 (3.06)  Time: 1.290s,  396.83/s  (1.290s,  396.83/s)  LR: 7.532e-05  Data: 0.982 (0.982)
05/08/2023 23:02:58 - INFO - train -   Train: 29 [  50/52 ( 98%)]  Loss: 3.287 (3.17)  Time: 0.371s, 1380.45/s  (0.360s, 1423.42/s)  LR: 7.532e-05  Data: 0.011 (0.032)
05/08/2023 23:02:59 - INFO - train -   Train: 29 [  51/52 (100%)]  Loss: 3.145 (3.16)  Time: 0.352s, 1455.58/s  (0.360s, 1424.03/s)  LR: 7.532e-05  Data: 0.000 (0.031)
05/08/2023 23:02:59 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 23:02:59 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 23:02:59 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 23:02:59 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 23:03:01 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 23:03:03 - INFO - train -   Test: [   0/19]  Time: 1.535 (1.535)  Loss:  1.1074 (1.1074)  Acc@1: 72.2656 (72.2656)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:04 - INFO - train -   Test: [  19/19]  Time: 0.156 (0.130)  Loss:  1.1602 (1.1421)  Acc@1: 66.9118 (68.2700)  Acc@5: 100.0000 (99.9600)
05/08/2023 23:03:04 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 23:03:04 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 23:03:04 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 23:03:04 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 23:03:05 - INFO - train -   Test: [   0/19]  Time: 0.855 (0.855)  Loss:  0.8677 (0.8677)  Acc@1: 90.2344 (90.2344)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:07 - INFO - train -   Test: [  19/19]  Time: 0.376 (0.124)  Loss:  0.8467 (0.8626)  Acc@1: 91.1765 (90.3500)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:07 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 23:03:07 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 23:03:07 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 23:03:07 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 23:03:08 - INFO - train -   Test: [   0/19]  Time: 0.772 (0.772)  Loss:  0.8423 (0.8423)  Acc@1: 85.7422 (85.7422)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:09 - INFO - train -   Test: [  19/19]  Time: 0.023 (0.101)  Loss:  0.8354 (0.8320)  Acc@1: 84.5588 (86.2900)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:09 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 23:03:09 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 23:03:09 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 23:03:09 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 23:03:10 - INFO - train -   Test: [   0/19]  Time: 0.753 (0.753)  Loss:  0.9336 (0.9336)  Acc@1: 86.5234 (86.5234)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:12 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 23:03:12 - INFO - train -   Test: [  19/19]  Time: 0.022 (0.092)  Loss:  0.8955 (0.8624)  Acc@1: 87.8676 (89.2500)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:12 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 23:03:12 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 23:03:12 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 23:03:13 - INFO - train -   Test: [   0/19]  Time: 0.814 (0.814)  Loss:  1.2959 (1.2959)  Acc@1: 83.9844 (83.9844)  Acc@5: 99.8047 (99.8047)
05/08/2023 23:03:14 - INFO - train -   Test: [  19/19]  Time: 0.022 (0.100)  Loss:  1.3057 (1.3074)  Acc@1: 84.1912 (83.3000)  Acc@5: 100.0000 (99.9900)
05/08/2023 23:03:14 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 23:03:14 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 23:03:14 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 23:03:14 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 23:03:15 - INFO - train -   Test: [   0/19]  Time: 0.790 (0.790)  Loss:  0.9287 (0.9287)  Acc@1: 81.8359 (81.8359)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:16 - INFO - train -   Test: [  19/19]  Time: 0.020 (0.098)  Loss:  0.8975 (0.9307)  Acc@1: 80.8824 (80.5900)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:16 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 23:03:16 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 23:03:16 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 23:03:16 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 23:03:17 - INFO - train -   Test: [   0/19]  Time: 0.810 (0.810)  Loss:  0.9854 (0.9854)  Acc@1: 86.1328 (86.1328)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:18 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.096)  Loss:  0.9077 (0.9373)  Acc@1: 86.3971 (86.7200)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:18 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 23:03:18 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 23:03:18 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 23:03:18 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 23:03:19 - INFO - train -   Test: [   0/19]  Time: 0.786 (0.786)  Loss:  1.2842 (1.2842)  Acc@1: 74.2188 (74.2188)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:20 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.099)  Loss:  1.2588 (1.2773)  Acc@1: 73.5294 (74.0600)  Acc@5: 100.0000 (99.9900)
05/08/2023 23:03:20 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 23:03:20 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 23:03:20 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 23:03:20 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 23:03:21 - INFO - train -   Test: [   0/19]  Time: 0.812 (0.812)  Loss:  1.2021 (1.2021)  Acc@1: 75.9766 (75.9766)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:22 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.096)  Loss:  1.1201 (1.1508)  Acc@1: 76.4706 (74.6300)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:22 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 23:03:22 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 23:03:22 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 23:03:22 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 23:03:23 - INFO - train -   Test: [   0/19]  Time: 0.857 (0.857)  Loss:  0.7388 (0.7388)  Acc@1: 90.6250 (90.6250)  Acc@5: 100.0000 (100.0000)
05/08/2023 23:03:24 - INFO - train -   Test: [  19/19]  Time: 0.018 (0.097)  Loss:  0.7183 (0.7381)  Acc@1: 91.5441 (90.8500)  Acc@5: 100.0000 (100.0000)
