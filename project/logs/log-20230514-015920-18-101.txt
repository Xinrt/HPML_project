05/14/2023 01:59:21 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
05/14/2023 01:59:21 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
05/14/2023 01:59:23 - INFO - train -   Model resnet18 created, param count:56897104
05/14/2023 01:59:42 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/14/2023 01:59:42 - INFO - train -   Using native Torch DistributedDataParallel.
05/14/2023 01:59:47 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/14/2023 01:59:53 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 6.705 (6.71)  Time: 6.432s,   39.80/s  (6.432s,   39.80/s)  LR: 5.500e-06  Data: 1.391 (1.391)
05/14/2023 02:00:13 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 7.268 (6.99)  Time: 0.389s,  658.07/s  (0.513s,  498.89/s)  LR: 5.500e-06  Data: 0.012 (0.039)
05/14/2023 02:00:32 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 8.406 (7.46)  Time: 0.360s,  710.62/s  (0.443s,  578.00/s)  LR: 5.500e-06  Data: 0.012 (0.026)
05/14/2023 02:00:33 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 6.853 (7.31)  Time: 0.364s,  702.34/s  (0.438s,  584.04/s)  LR: 5.500e-06  Data: 0.000 (0.025)
05/14/2023 02:00:33 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:00:36 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 6.787 (6.79)  Time: 0.985s,  259.93/s  (0.985s,  259.93/s)  LR: 5.504e-03  Data: 0.603 (0.603)
05/14/2023 02:00:55 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 6.267 (6.53)  Time: 0.295s,  866.35/s  (0.383s,  667.63/s)  LR: 5.504e-03  Data: 0.012 (0.024)
05/14/2023 02:01:14 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 5.804 (6.29)  Time: 0.408s,  627.60/s  (0.379s,  675.80/s)  LR: 5.504e-03  Data: 0.012 (0.018)
05/14/2023 02:01:15 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 5.811 (6.17)  Time: 0.364s,  704.21/s  (0.378s,  676.88/s)  LR: 5.504e-03  Data: 0.000 (0.018)
05/14/2023 02:01:15 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:01:19 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 5.476 (5.48)  Time: 1.091s,  234.61/s  (1.091s,  234.61/s)  LR: 1.100e-02  Data: 0.675 (0.675)
05/14/2023 02:01:37 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 5.060 (5.27)  Time: 0.411s,  623.49/s  (0.382s,  670.23/s)  LR: 1.100e-02  Data: 0.012 (0.025)
05/14/2023 02:01:56 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.531 (5.02)  Time: 0.461s,  555.01/s  (0.377s,  679.37/s)  LR: 1.100e-02  Data: 0.011 (0.019)
05/14/2023 02:01:57 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.753 (4.96)  Time: 0.412s,  622.00/s  (0.377s,  679.57/s)  LR: 1.100e-02  Data: 0.000 (0.018)
05/14/2023 02:01:57 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:02:01 - INFO - train -   Train: 3 [   0/105 (  0%)]  Loss: 4.783 (4.78)  Time: 1.042s,  245.73/s  (1.042s,  245.73/s)  LR: 1.650e-02  Data: 0.627 (0.627)
05/14/2023 02:02:19 - INFO - train -   Train: 3 [  50/105 ( 48%)]  Loss: 4.654 (4.72)  Time: 0.378s,  677.58/s  (0.387s,  662.06/s)  LR: 1.650e-02  Data: 0.012 (0.025)
05/14/2023 02:02:38 - INFO - train -   Train: 3 [ 100/105 ( 96%)]  Loss: 4.650 (4.70)  Time: 0.340s,  752.19/s  (0.379s,  674.61/s)  LR: 1.650e-02  Data: 0.012 (0.019)
05/14/2023 02:02:40 - INFO - train -   Train: 3 [ 104/105 (100%)]  Loss: 4.273 (4.59)  Time: 0.462s,  554.24/s  (0.380s,  673.11/s)  LR: 1.650e-02  Data: 0.000 (0.018)
05/14/2023 02:02:40 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:02:43 - INFO - train -   Train: 4 [   0/105 (  0%)]  Loss: 4.504 (4.50)  Time: 0.950s,  269.48/s  (0.950s,  269.48/s)  LR: 2.200e-02  Data: 0.641 (0.641)
05/14/2023 02:03:02 - INFO - train -   Train: 4 [  50/105 ( 48%)]  Loss: 4.600 (4.55)  Time: 0.341s,  751.68/s  (0.382s,  670.92/s)  LR: 2.200e-02  Data: 0.012 (0.024)
05/14/2023 02:03:20 - INFO - train -   Train: 4 [ 100/105 ( 96%)]  Loss: 4.701 (4.60)  Time: 0.408s,  627.94/s  (0.378s,  678.07/s)  LR: 2.200e-02  Data: 0.011 (0.018)
05/14/2023 02:03:22 - INFO - train -   Train: 4 [ 104/105 (100%)]  Loss: 4.503 (4.58)  Time: 0.342s,  748.36/s  (0.377s,  678.74/s)  LR: 2.200e-02  Data: 0.000 (0.018)
05/14/2023 02:03:22 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:03:26 - INFO - train -   Train: 5 [   0/105 (  0%)]  Loss: 4.507 (4.51)  Time: 1.025s,  249.71/s  (1.025s,  249.71/s)  LR: 2.566e-02  Data: 0.660 (0.660)
05/14/2023 02:03:45 - INFO - train -   Train: 5 [  50/105 ( 48%)]  Loss: 4.454 (4.48)  Time: 0.314s,  815.93/s  (0.384s,  666.34/s)  LR: 2.566e-02  Data: 0.012 (0.025)
05/14/2023 02:04:03 - INFO - train -   Train: 5 [ 100/105 ( 96%)]  Loss: 4.452 (4.47)  Time: 0.377s,  678.60/s  (0.379s,  675.81/s)  LR: 2.566e-02  Data: 0.012 (0.019)
05/14/2023 02:04:05 - INFO - train -   Train: 5 [ 104/105 (100%)]  Loss: 4.409 (4.46)  Time: 0.409s,  625.96/s  (0.379s,  674.73/s)  LR: 2.566e-02  Data: 0.000 (0.018)
05/14/2023 02:04:05 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:04:08 - INFO - train -   Train: 6 [   0/105 (  0%)]  Loss: 4.424 (4.42)  Time: 1.004s,  254.95/s  (1.004s,  254.95/s)  LR: 2.487e-02  Data: 0.668 (0.668)
05/14/2023 02:04:27 - INFO - train -   Train: 6 [  50/105 ( 48%)]  Loss: 4.376 (4.40)  Time: 0.335s,  764.59/s  (0.379s,  676.34/s)  LR: 2.487e-02  Data: 0.011 (0.025)
05/14/2023 02:04:45 - INFO - train -   Train: 6 [ 100/105 ( 96%)]  Loss: 4.350 (4.38)  Time: 0.396s,  646.38/s  (0.374s,  683.58/s)  LR: 2.487e-02  Data: 0.012 (0.019)
05/14/2023 02:04:47 - INFO - train -   Train: 6 [ 104/105 (100%)]  Loss: 4.332 (4.37)  Time: 0.379s,  675.85/s  (0.373s,  686.11/s)  LR: 2.487e-02  Data: 0.000 (0.018)
05/14/2023 02:04:47 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:04:50 - INFO - train -   Train: 7 [   0/105 (  0%)]  Loss: 4.329 (4.33)  Time: 1.015s,  252.24/s  (1.015s,  252.24/s)  LR: 2.397e-02  Data: 0.667 (0.667)
05/14/2023 02:05:09 - INFO - train -   Train: 7 [  50/105 ( 48%)]  Loss: 4.305 (4.32)  Time: 0.365s,  701.72/s  (0.382s,  670.03/s)  LR: 2.397e-02  Data: 0.012 (0.025)
05/14/2023 02:05:27 - INFO - train -   Train: 7 [ 100/105 ( 96%)]  Loss: 4.178 (4.27)  Time: 0.295s,  867.54/s  (0.377s,  679.31/s)  LR: 2.397e-02  Data: 0.012 (0.019)
05/14/2023 02:05:29 - INFO - train -   Train: 7 [ 104/105 (100%)]  Loss: 4.222 (4.26)  Time: 0.312s,  821.05/s  (0.374s,  684.82/s)  LR: 2.397e-02  Data: 0.000 (0.018)
05/14/2023 02:05:29 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:05:32 - INFO - train -   Train: 8 [   0/105 (  0%)]  Loss: 4.255 (4.26)  Time: 0.893s,  286.74/s  (0.893s,  286.74/s)  LR: 2.295e-02  Data: 0.550 (0.550)
05/14/2023 02:05:51 - INFO - train -   Train: 8 [  50/105 ( 48%)]  Loss: 4.225 (4.24)  Time: 0.344s,  743.29/s  (0.381s,  671.60/s)  LR: 2.295e-02  Data: 0.012 (0.023)
05/14/2023 02:06:09 - INFO - train -   Train: 8 [ 100/105 ( 96%)]  Loss: 4.139 (4.21)  Time: 0.329s,  778.33/s  (0.373s,  686.36/s)  LR: 2.295e-02  Data: 0.012 (0.018)
05/14/2023 02:06:10 - INFO - train -   Train: 8 [ 104/105 (100%)]  Loss: 4.157 (4.19)  Time: 0.316s,  811.31/s  (0.371s,  689.14/s)  LR: 2.295e-02  Data: 0.000 (0.017)
05/14/2023 02:06:10 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:06:14 - INFO - train -   Train: 9 [   0/105 (  0%)]  Loss: 4.157 (4.16)  Time: 1.023s,  250.30/s  (1.023s,  250.30/s)  LR: 2.183e-02  Data: 0.603 (0.603)
05/14/2023 02:06:33 - INFO - train -   Train: 9 [  50/105 ( 48%)]  Loss: 4.117 (4.14)  Time: 0.305s,  839.14/s  (0.384s,  666.19/s)  LR: 2.183e-02  Data: 0.012 (0.024)
05/14/2023 02:06:51 - INFO - train -   Train: 9 [ 100/105 ( 96%)]  Loss: 4.070 (4.11)  Time: 0.293s,  875.16/s  (0.376s,  681.14/s)  LR: 2.183e-02  Data: 0.012 (0.018)
05/14/2023 02:06:53 - INFO - train -   Train: 9 [ 104/105 (100%)]  Loss: 4.127 (4.12)  Time: 0.366s,  699.97/s  (0.376s,  680.57/s)  LR: 2.183e-02  Data: 0.000 (0.018)
05/14/2023 02:06:53 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:06:56 - INFO - train -   Train: 10 [   0/105 (  0%)]  Loss: 4.224 (4.22)  Time: 0.982s,  260.62/s  (0.982s,  260.62/s)  LR: 2.063e-02  Data: 0.651 (0.651)
05/14/2023 02:07:14 - INFO - train -   Train: 10 [  50/105 ( 48%)]  Loss: 4.115 (4.17)  Time: 0.366s,  699.65/s  (0.378s,  676.79/s)  LR: 2.063e-02  Data: 0.012 (0.025)
05/14/2023 02:07:33 - INFO - train -   Train: 10 [ 100/105 ( 96%)]  Loss: 4.111 (4.15)  Time: 0.419s,  611.55/s  (0.372s,  688.78/s)  LR: 2.063e-02  Data: 0.012 (0.019)
05/14/2023 02:07:34 - INFO - train -   Train: 10 [ 104/105 (100%)]  Loss: 4.135 (4.15)  Time: 0.334s,  765.92/s  (0.372s,  689.06/s)  LR: 2.063e-02  Data: 0.000 (0.018)
05/14/2023 02:07:34 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:07:38 - INFO - train -   Train: 11 [   0/105 (  0%)]  Loss: 4.144 (4.14)  Time: 1.057s,  242.25/s  (1.057s,  242.25/s)  LR: 1.934e-02  Data: 0.650 (0.650)
05/14/2023 02:07:57 - INFO - train -   Train: 11 [  50/105 ( 48%)]  Loss: 4.046 (4.10)  Time: 0.289s,  886.14/s  (0.396s,  646.40/s)  LR: 1.934e-02  Data: 0.011 (0.025)
05/14/2023 02:08:15 - INFO - train -   Train: 11 [ 100/105 ( 96%)]  Loss: 4.032 (4.07)  Time: 0.379s,  675.94/s  (0.383s,  668.79/s)  LR: 1.934e-02  Data: 0.012 (0.019)
05/14/2023 02:08:17 - INFO - train -   Train: 11 [ 104/105 (100%)]  Loss: 4.107 (4.08)  Time: 0.296s,  863.75/s  (0.381s,  671.31/s)  LR: 1.934e-02  Data: 0.000 (0.018)
05/14/2023 02:08:17 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:08:21 - INFO - train -   Train: 12 [   0/105 (  0%)]  Loss: 4.070 (4.07)  Time: 1.118s,  228.91/s  (1.118s,  228.91/s)  LR: 1.800e-02  Data: 0.696 (0.696)
05/14/2023 02:08:40 - INFO - train -   Train: 12 [  50/105 ( 48%)]  Loss: 4.096 (4.08)  Time: 0.364s,  704.17/s  (0.391s,  654.94/s)  LR: 1.800e-02  Data: 0.011 (0.025)
05/14/2023 02:08:58 - INFO - train -   Train: 12 [ 100/105 ( 96%)]  Loss: 4.083 (4.08)  Time: 0.315s,  811.55/s  (0.379s,  675.48/s)  LR: 1.800e-02  Data: 0.012 (0.019)
05/14/2023 02:09:00 - INFO - train -   Train: 12 [ 104/105 (100%)]  Loss: 3.987 (4.06)  Time: 0.410s,  624.83/s  (0.378s,  677.47/s)  LR: 1.800e-02  Data: 0.000 (0.019)
05/14/2023 02:09:00 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:09:03 - INFO - train -   Train: 13 [   0/105 (  0%)]  Loss: 3.956 (3.96)  Time: 1.004s,  254.99/s  (1.004s,  254.99/s)  LR: 1.661e-02  Data: 0.605 (0.605)
05/14/2023 02:09:22 - INFO - train -   Train: 13 [  50/105 ( 48%)]  Loss: 4.037 (4.00)  Time: 0.327s,  784.03/s  (0.376s,  680.30/s)  LR: 1.661e-02  Data: 0.012 (0.024)
05/14/2023 02:09:40 - INFO - train -   Train: 13 [ 100/105 ( 96%)]  Loss: 4.048 (4.01)  Time: 0.340s,  752.67/s  (0.375s,  683.37/s)  LR: 1.661e-02  Data: 0.012 (0.018)
05/14/2023 02:09:42 - INFO - train -   Train: 13 [ 104/105 (100%)]  Loss: 4.019 (4.02)  Time: 0.273s,  937.86/s  (0.375s,  682.43/s)  LR: 1.661e-02  Data: 0.000 (0.018)
05/14/2023 02:09:42 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:09:46 - INFO - train -   Train: 14 [   0/105 (  0%)]  Loss: 4.070 (4.07)  Time: 1.080s,  237.09/s  (1.080s,  237.09/s)  LR: 1.519e-02  Data: 0.691 (0.691)
05/14/2023 02:10:05 - INFO - train -   Train: 14 [  50/105 ( 48%)]  Loss: 4.045 (4.06)  Time: 0.374s,  685.20/s  (0.392s,  652.31/s)  LR: 1.519e-02  Data: 0.012 (0.026)
05/14/2023 02:10:23 - INFO - train -   Train: 14 [ 100/105 ( 96%)]  Loss: 3.996 (4.04)  Time: 0.321s,  796.60/s  (0.384s,  666.40/s)  LR: 1.519e-02  Data: 0.012 (0.019)
05/14/2023 02:10:25 - INFO - train -   Train: 14 [ 104/105 (100%)]  Loss: 4.028 (4.03)  Time: 0.372s,  687.98/s  (0.383s,  668.60/s)  LR: 1.519e-02  Data: 0.000 (0.019)
05/14/2023 02:10:25 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:10:29 - INFO - train -   Train: 15 [   0/105 (  0%)]  Loss: 3.874 (3.87)  Time: 1.164s,  219.99/s  (1.164s,  219.99/s)  LR: 1.375e-02  Data: 0.684 (0.684)
05/14/2023 02:10:48 - INFO - train -   Train: 15 [  50/105 ( 48%)]  Loss: 3.980 (3.93)  Time: 0.339s,  754.95/s  (0.388s,  659.35/s)  LR: 1.375e-02  Data: 0.012 (0.025)
05/14/2023 02:11:06 - INFO - train -   Train: 15 [ 100/105 ( 96%)]  Loss: 3.886 (3.91)  Time: 0.408s,  626.70/s  (0.382s,  669.96/s)  LR: 1.375e-02  Data: 0.012 (0.019)
05/14/2023 02:11:08 - INFO - train -   Train: 15 [ 104/105 (100%)]  Loss: 4.038 (3.94)  Time: 0.371s,  690.90/s  (0.381s,  671.10/s)  LR: 1.375e-02  Data: 0.000 (0.019)
05/14/2023 02:11:08 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:11:12 - INFO - train -   Train: 16 [   0/105 (  0%)]  Loss: 4.015 (4.02)  Time: 0.979s,  261.37/s  (0.979s,  261.37/s)  LR: 1.231e-02  Data: 0.642 (0.642)
05/14/2023 02:11:30 - INFO - train -   Train: 16 [  50/105 ( 48%)]  Loss: 3.999 (4.01)  Time: 0.395s,  648.90/s  (0.382s,  670.03/s)  LR: 1.231e-02  Data: 0.014 (0.025)
05/14/2023 02:11:48 - INFO - train -   Train: 16 [ 100/105 ( 96%)]  Loss: 4.073 (4.03)  Time: 0.423s,  605.77/s  (0.375s,  683.31/s)  LR: 1.231e-02  Data: 0.012 (0.019)
05/14/2023 02:11:50 - INFO - train -   Train: 16 [ 104/105 (100%)]  Loss: 4.065 (4.04)  Time: 0.310s,  826.90/s  (0.374s,  684.68/s)  LR: 1.231e-02  Data: 0.000 (0.018)
05/14/2023 02:11:50 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:11:54 - INFO - train -   Train: 17 [   0/105 (  0%)]  Loss: 4.004 (4.00)  Time: 1.143s,  224.00/s  (1.143s,  224.00/s)  LR: 1.089e-02  Data: 0.715 (0.715)
05/14/2023 02:12:12 - INFO - train -   Train: 17 [  50/105 ( 48%)]  Loss: 4.017 (4.01)  Time: 0.340s,  753.84/s  (0.390s,  657.08/s)  LR: 1.089e-02  Data: 0.015 (0.027)
05/14/2023 02:12:32 - INFO - train -   Train: 17 [ 100/105 ( 96%)]  Loss: 3.820 (3.95)  Time: 0.479s,  534.41/s  (0.387s,  661.46/s)  LR: 1.089e-02  Data: 0.013 (0.020)
05/14/2023 02:12:33 - INFO - train -   Train: 17 [ 104/105 (100%)]  Loss: 3.989 (3.96)  Time: 0.392s,  652.89/s  (0.386s,  662.53/s)  LR: 1.089e-02  Data: 0.000 (0.020)
05/14/2023 02:12:33 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:12:37 - INFO - train -   Train: 18 [   0/105 (  0%)]  Loss: 3.924 (3.92)  Time: 0.989s,  258.73/s  (0.989s,  258.73/s)  LR: 9.501e-03  Data: 0.636 (0.636)
05/14/2023 02:12:56 - INFO - train -   Train: 18 [  50/105 ( 48%)]  Loss: 3.979 (3.95)  Time: 0.332s,  771.79/s  (0.386s,  662.66/s)  LR: 9.501e-03  Data: 0.012 (0.025)
05/14/2023 02:13:14 - INFO - train -   Train: 18 [ 100/105 ( 96%)]  Loss: 3.917 (3.94)  Time: 0.305s,  838.25/s  (0.383s,  669.06/s)  LR: 9.501e-03  Data: 0.013 (0.019)
05/14/2023 02:13:16 - INFO - train -   Train: 18 [ 104/105 (100%)]  Loss: 3.986 (3.95)  Time: 0.321s,  798.10/s  (0.380s,  673.51/s)  LR: 9.501e-03  Data: 0.000 (0.018)
05/14/2023 02:13:16 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:13:19 - INFO - train -   Train: 19 [   0/105 (  0%)]  Loss: 3.934 (3.93)  Time: 0.945s,  270.87/s  (0.945s,  270.87/s)  LR: 8.157e-03  Data: 0.612 (0.612)
05/14/2023 02:13:38 - INFO - train -   Train: 19 [  50/105 ( 48%)]  Loss: 4.013 (3.97)  Time: 0.398s,  642.49/s  (0.382s,  670.89/s)  LR: 8.157e-03  Data: 0.012 (0.024)
05/14/2023 02:13:56 - INFO - train -   Train: 19 [ 100/105 ( 96%)]  Loss: 3.991 (3.98)  Time: 0.396s,  645.85/s  (0.376s,  680.69/s)  LR: 8.157e-03  Data: 0.012 (0.018)
05/14/2023 02:13:58 - INFO - train -   Train: 19 [ 104/105 (100%)]  Loss: 3.945 (3.97)  Time: 0.338s,  756.55/s  (0.375s,  683.27/s)  LR: 8.157e-03  Data: 0.000 (0.018)
05/14/2023 02:13:58 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:14:01 - INFO - train -   Train: 20 [   0/105 (  0%)]  Loss: 3.984 (3.98)  Time: 1.035s,  247.40/s  (1.035s,  247.40/s)  LR: 6.875e-03  Data: 0.668 (0.668)
05/14/2023 02:14:19 - INFO - train -   Train: 20 [  50/105 ( 48%)]  Loss: 3.945 (3.96)  Time: 0.324s,  789.72/s  (0.374s,  683.81/s)  LR: 6.875e-03  Data: 0.012 (0.025)
05/14/2023 02:14:38 - INFO - train -   Train: 20 [ 100/105 ( 96%)]  Loss: 3.961 (3.96)  Time: 0.395s,  648.11/s  (0.378s,  676.79/s)  LR: 6.875e-03  Data: 0.012 (0.019)
05/14/2023 02:14:40 - INFO - train -   Train: 20 [ 104/105 (100%)]  Loss: 3.937 (3.96)  Time: 0.339s,  754.30/s  (0.378s,  676.79/s)  LR: 6.875e-03  Data: 0.000 (0.018)
05/14/2023 02:14:40 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:14:44 - INFO - train -   Train: 21 [   0/105 (  0%)]  Loss: 3.994 (3.99)  Time: 0.969s,  264.26/s  (0.969s,  264.26/s)  LR: 5.668e-03  Data: 0.642 (0.642)
05/14/2023 02:15:02 - INFO - train -   Train: 21 [  50/105 ( 48%)]  Loss: 3.781 (3.89)  Time: 0.479s,  534.98/s  (0.383s,  668.70/s)  LR: 5.668e-03  Data: 0.012 (0.025)
05/14/2023 02:15:21 - INFO - train -   Train: 21 [ 100/105 ( 96%)]  Loss: 3.763 (3.85)  Time: 0.482s,  530.62/s  (0.378s,  677.08/s)  LR: 5.668e-03  Data: 0.012 (0.019)
05/14/2023 02:15:22 - INFO - train -   Train: 21 [ 104/105 (100%)]  Loss: 3.910 (3.86)  Time: 0.308s,  831.81/s  (0.378s,  676.41/s)  LR: 5.668e-03  Data: 0.000 (0.018)
05/14/2023 02:15:22 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:15:26 - INFO - train -   Train: 22 [   0/105 (  0%)]  Loss: 3.969 (3.97)  Time: 1.049s,  244.10/s  (1.049s,  244.10/s)  LR: 4.549e-03  Data: 0.666 (0.666)
05/14/2023 02:15:45 - INFO - train -   Train: 22 [  50/105 ( 48%)]  Loss: 3.965 (3.97)  Time: 0.311s,  823.47/s  (0.390s,  656.79/s)  LR: 4.549e-03  Data: 0.012 (0.025)
05/14/2023 02:16:03 - INFO - train -   Train: 22 [ 100/105 ( 96%)]  Loss: 3.983 (3.97)  Time: 0.338s,  758.31/s  (0.381s,  672.35/s)  LR: 4.549e-03  Data: 0.011 (0.019)
05/14/2023 02:16:05 - INFO - train -   Train: 22 [ 104/105 (100%)]  Loss: 4.008 (3.98)  Time: 0.359s,  712.75/s  (0.380s,  673.16/s)  LR: 4.549e-03  Data: 0.000 (0.018)
05/14/2023 02:16:05 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:16:09 - INFO - train -   Train: 23 [   0/105 (  0%)]  Loss: 3.896 (3.90)  Time: 1.124s,  227.80/s  (1.124s,  227.80/s)  LR: 3.532e-03  Data: 0.778 (0.778)
05/14/2023 02:16:27 - INFO - train -   Train: 23 [  50/105 ( 48%)]  Loss: 3.923 (3.91)  Time: 0.403s,  635.31/s  (0.380s,  672.97/s)  LR: 3.532e-03  Data: 0.015 (0.028)
05/14/2023 02:16:46 - INFO - train -   Train: 23 [ 100/105 ( 96%)]  Loss: 3.934 (3.92)  Time: 0.410s,  625.01/s  (0.377s,  678.41/s)  LR: 3.532e-03  Data: 0.013 (0.020)
05/14/2023 02:16:48 - INFO - train -   Train: 23 [ 104/105 (100%)]  Loss: 3.686 (3.86)  Time: 0.460s,  556.06/s  (0.379s,  675.46/s)  LR: 3.532e-03  Data: 0.000 (0.020)
05/14/2023 02:16:48 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:16:51 - INFO - train -   Train: 24 [   0/105 (  0%)]  Loss: 3.954 (3.95)  Time: 1.001s,  255.79/s  (1.001s,  255.79/s)  LR: 2.626e-03  Data: 0.650 (0.650)
05/14/2023 02:17:09 - INFO - train -   Train: 24 [  50/105 ( 48%)]  Loss: 3.944 (3.95)  Time: 0.424s,  603.58/s  (0.376s,  681.54/s)  LR: 2.626e-03  Data: 0.015 (0.025)
05/14/2023 02:17:27 - INFO - train -   Train: 24 [ 100/105 ( 96%)]  Loss: 3.979 (3.96)  Time: 0.417s,  613.59/s  (0.368s,  695.55/s)  LR: 2.626e-03  Data: 0.012 (0.019)
05/14/2023 02:17:29 - INFO - train -   Train: 24 [ 104/105 (100%)]  Loss: 3.890 (3.94)  Time: 0.407s,  629.72/s  (0.368s,  696.16/s)  LR: 2.626e-03  Data: 0.000 (0.019)
05/14/2023 02:17:29 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:17:33 - INFO - train -   Train: 25 [   0/105 (  0%)]  Loss: 3.888 (3.89)  Time: 1.017s,  251.72/s  (1.017s,  251.72/s)  LR: 1.842e-03  Data: 0.614 (0.614)
05/14/2023 02:17:51 - INFO - train -   Train: 25 [  50/105 ( 48%)]  Loss: 3.926 (3.91)  Time: 0.346s,  740.44/s  (0.383s,  667.70/s)  LR: 1.842e-03  Data: 0.012 (0.024)
05/14/2023 02:18:10 - INFO - train -   Train: 25 [ 100/105 ( 96%)]  Loss: 3.966 (3.93)  Time: 0.360s,  711.44/s  (0.383s,  668.27/s)  LR: 1.842e-03  Data: 0.013 (0.019)
05/14/2023 02:18:12 - INFO - train -   Train: 25 [ 104/105 (100%)]  Loss: 3.758 (3.88)  Time: 0.455s,  562.41/s  (0.384s,  666.83/s)  LR: 1.842e-03  Data: 0.000 (0.018)
05/14/2023 02:18:12 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:18:16 - INFO - train -   Train: 26 [   0/105 (  0%)]  Loss: 4.001 (4.00)  Time: 1.087s,  235.43/s  (1.087s,  235.43/s)  LR: 1.189e-03  Data: 0.699 (0.699)
05/14/2023 02:18:34 - INFO - train -   Train: 26 [  50/105 ( 48%)]  Loss: 3.907 (3.95)  Time: 0.312s,  820.38/s  (0.389s,  658.92/s)  LR: 1.189e-03  Data: 0.012 (0.026)
05/14/2023 02:18:53 - INFO - train -   Train: 26 [ 100/105 ( 96%)]  Loss: 3.960 (3.96)  Time: 0.310s,  826.47/s  (0.382s,  670.12/s)  LR: 1.189e-03  Data: 0.011 (0.019)
05/14/2023 02:18:55 - INFO - train -   Train: 26 [ 104/105 (100%)]  Loss: 3.712 (3.89)  Time: 0.466s,  549.04/s  (0.382s,  670.67/s)  LR: 1.189e-03  Data: 0.000 (0.019)
05/14/2023 02:18:55 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:18:58 - INFO - train -   Train: 27 [   0/105 (  0%)]  Loss: 3.925 (3.92)  Time: 1.051s,  243.62/s  (1.051s,  243.62/s)  LR: 6.730e-04  Data: 0.691 (0.691)
05/14/2023 02:19:18 - INFO - train -   Train: 27 [  50/105 ( 48%)]  Loss: 3.940 (3.93)  Time: 0.318s,  805.35/s  (0.400s,  640.22/s)  LR: 6.730e-04  Data: 0.012 (0.025)
05/14/2023 02:19:36 - INFO - train -   Train: 27 [ 100/105 ( 96%)]  Loss: 3.873 (3.91)  Time: 0.352s,  728.15/s  (0.387s,  661.97/s)  LR: 6.730e-04  Data: 0.012 (0.019)
05/14/2023 02:19:38 - INFO - train -   Train: 27 [ 104/105 (100%)]  Loss: 3.899 (3.91)  Time: 0.337s,  760.43/s  (0.386s,  664.05/s)  LR: 6.730e-04  Data: 0.000 (0.019)
05/14/2023 02:19:38 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:19:42 - INFO - train -   Train: 28 [   0/105 (  0%)]  Loss: 3.830 (3.83)  Time: 0.884s,  289.57/s  (0.884s,  289.57/s)  LR: 3.005e-04  Data: 0.582 (0.582)
05/14/2023 02:20:00 - INFO - train -   Train: 28 [  50/105 ( 48%)]  Loss: 3.958 (3.89)  Time: 0.424s,  603.31/s  (0.380s,  672.97/s)  LR: 3.005e-04  Data: 0.014 (0.023)
05/14/2023 02:20:18 - INFO - train -   Train: 28 [ 100/105 ( 96%)]  Loss: 3.942 (3.91)  Time: 0.334s,  766.13/s  (0.371s,  690.15/s)  LR: 3.005e-04  Data: 0.012 (0.018)
05/14/2023 02:20:20 - INFO - train -   Train: 28 [ 104/105 (100%)]  Loss: 3.976 (3.93)  Time: 0.364s,  703.20/s  (0.371s,  689.86/s)  LR: 3.005e-04  Data: 0.000 (0.018)
05/14/2023 02:20:20 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:20:23 - INFO - train -   Train: 29 [   0/105 (  0%)]  Loss: 3.707 (3.71)  Time: 1.167s,  219.34/s  (1.167s,  219.34/s)  LR: 7.532e-05  Data: 0.680 (0.680)
05/14/2023 02:20:42 - INFO - train -   Train: 29 [  50/105 ( 48%)]  Loss: 3.887 (3.80)  Time: 0.321s,  798.42/s  (0.380s,  674.10/s)  LR: 7.532e-05  Data: 0.014 (0.025)
05/14/2023 02:21:00 - INFO - train -   Train: 29 [ 100/105 ( 96%)]  Loss: 4.000 (3.86)  Time: 0.358s,  715.43/s  (0.372s,  688.78/s)  LR: 7.532e-05  Data: 0.013 (0.019)
05/14/2023 02:21:01 - INFO - train -   Train: 29 [ 104/105 (100%)]  Loss: 3.917 (3.88)  Time: 0.398s,  643.33/s  (0.371s,  689.63/s)  LR: 7.532e-05  Data: 0.000 (0.019)
05/14/2023 02:21:01 - INFO - train -   Distributing BatchNorm running means and vars
05/14/2023 02:21:01 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 02:21:04 - INFO - train -   ------------- Evaluting stitch config 0/27 -------------
05/14/2023 02:21:06 - INFO - train -   Test: [   0/39]  Time: 0.953 (0.953)  Loss:  0.8320 (0.8320)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:07 - INFO - train -   Test: [  39/39]  Time: 0.064 (0.065)  Loss:  0.8301 (0.8388)  Acc@1: 100.0000 (99.4200)  Acc@5: 100.0000 (99.9800)
05/14/2023 02:21:07 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 02:21:07 - INFO - train -   ------------- Evaluting stitch config 1/27 -------------
05/14/2023 02:21:08 - INFO - train -   Test: [   0/39]  Time: 0.668 (0.668)  Loss:  1.1025 (1.1025)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:13 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 02:21:13 - INFO - train -   Test: [  39/39]  Time: 0.162 (0.120)  Loss:  1.1719 (1.0742)  Acc@1: 100.0000 (99.6500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:13 - INFO - train -   ------------- Evaluting stitch config 2/27 -------------
05/14/2023 02:21:14 - INFO - train -   Test: [   0/39]  Time: 0.655 (0.655)  Loss:  1.0693 (1.0693)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:17 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 02:21:17 - INFO - train -   Test: [  39/39]  Time: 0.024 (0.106)  Loss:  1.0625 (1.0614)  Acc@1: 100.0000 (99.7900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:17 - INFO - train -   ------------- Evaluting stitch config 3/27 -------------
05/14/2023 02:21:18 - INFO - train -   Test: [   0/39]  Time: 0.663 (0.663)  Loss:  1.0176 (1.0176)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:22 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 02:21:22 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.105)  Loss:  1.0684 (1.0290)  Acc@1: 100.0000 (99.7400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:22 - INFO - train -   ------------- Evaluting stitch config 4/27 -------------
05/14/2023 02:21:23 - INFO - train -   Test: [   0/39]  Time: 0.651 (0.651)  Loss:  1.0557 (1.0557)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:26 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 02:21:26 - INFO - train -   Test: [  39/39]  Time: 0.025 (0.103)  Loss:  1.0088 (1.0836)  Acc@1: 100.0000 (99.1100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:26 - INFO - train -   ------------- Evaluting stitch config 5/27 -------------
05/14/2023 02:21:27 - INFO - train -   Test: [   0/39]  Time: 0.646 (0.646)  Loss:  1.0449 (1.0449)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:30 - INFO - train -   Test: [  39/39]  Time: 0.023 (0.097)  Loss:  1.0127 (1.0230)  Acc@1: 100.0000 (99.3700)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:30 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 02:21:30 - INFO - train -   ------------- Evaluting stitch config 6/27 -------------
05/14/2023 02:21:31 - INFO - train -   Test: [   0/39]  Time: 0.638 (0.638)  Loss:  0.8945 (0.8945)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:35 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 02:21:35 - INFO - train -   Test: [  39/39]  Time: 0.021 (0.095)  Loss:  1.0098 (0.9076)  Acc@1: 100.0000 (99.1100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:35 - INFO - train -   ------------- Evaluting stitch config 7/27 -------------
05/14/2023 02:21:36 - INFO - train -   Test: [   0/39]  Time: 0.826 (0.826)  Loss:  0.9648 (0.9648)  Acc@1: 97.2656 (97.2656)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:39 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 02:21:39 - INFO - train -   Test: [  39/39]  Time: 0.021 (0.098)  Loss:  0.9751 (0.9637)  Acc@1: 100.0000 (97.4800)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:39 - INFO - train -   ------------- Evaluting stitch config 8/27 -------------
05/14/2023 02:21:40 - INFO - train -   Test: [   0/39]  Time: 0.623 (0.623)  Loss:  0.9199 (0.9199)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:43 - INFO - train -   Test: [  39/39]  Time: 0.022 (0.091)  Loss:  0.8799 (0.9561)  Acc@1: 100.0000 (98.2200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:43 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 02:21:43 - INFO - train -   ------------- Evaluting stitch config 9/27 -------------
05/14/2023 02:21:44 - INFO - train -   Test: [   0/39]  Time: 0.639 (0.639)  Loss:  0.9053 (0.9053)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:47 - INFO - train -   Test: [  39/39]  Time: 0.020 (0.089)  Loss:  0.8457 (0.8819)  Acc@1: 100.0000 (99.4200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:47 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 02:21:47 - INFO - train -   ------------- Evaluting stitch config 10/27 -------------
05/14/2023 02:21:48 - INFO - train -   Test: [   0/39]  Time: 0.668 (0.668)  Loss:  0.9199 (0.9199)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:50 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 02:21:50 - INFO - train -   Test: [  39/39]  Time: 0.018 (0.087)  Loss:  0.8066 (0.9287)  Acc@1: 100.0000 (98.3900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:50 - INFO - train -   ------------- Evaluting stitch config 11/27 -------------
05/14/2023 02:21:51 - INFO - train -   Test: [   0/39]  Time: 0.593 (0.593)  Loss:  0.9014 (0.9014)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:54 - INFO - train -   Test: [  39/39]  Time: 0.019 (0.083)  Loss:  0.8735 (0.9035)  Acc@1: 100.0000 (98.8600)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:54 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 02:21:54 - INFO - train -   ------------- Evaluting stitch config 12/27 -------------
05/14/2023 02:21:55 - INFO - train -   Test: [   0/39]  Time: 0.637 (0.637)  Loss:  0.9590 (0.9590)  Acc@1: 98.0469 (98.0469)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:58 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 02:21:58 - INFO - train -   Test: [  39/39]  Time: 0.017 (0.081)  Loss:  0.8682 (0.9445)  Acc@1: 100.0000 (97.2700)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:21:58 - INFO - train -   ------------- Evaluting stitch config 13/27 -------------
05/14/2023 02:21:59 - INFO - train -   Test: [   0/39]  Time: 0.614 (0.614)  Loss:  0.8481 (0.8481)  Acc@1: 97.6562 (97.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:01 - INFO - train -   Test: [  39/39]  Time: 0.015 (0.079)  Loss:  0.8242 (0.8365)  Acc@1: 100.0000 (98.2900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:01 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 02:22:01 - INFO - train -   ------------- Evaluting stitch config 14/27 -------------
05/14/2023 02:22:02 - INFO - train -   Test: [   0/39]  Time: 0.687 (0.687)  Loss:  0.9116 (0.9116)  Acc@1: 97.6562 (97.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:04 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 02:22:04 - INFO - train -   Test: [  39/39]  Time: 0.016 (0.078)  Loss:  0.8906 (0.9177)  Acc@1: 100.0000 (97.1600)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:04 - INFO - train -   ------------- Evaluting stitch config 15/27 -------------
05/14/2023 02:22:05 - INFO - train -   Test: [   0/39]  Time: 0.626 (0.626)  Loss:  0.9751 (0.9751)  Acc@1: 97.6562 (97.6562)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:08 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 02:22:08 - INFO - train -   Test: [  39/39]  Time: 0.015 (0.075)  Loss:  0.8369 (0.9413)  Acc@1: 100.0000 (98.0400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:08 - INFO - train -   ------------- Evaluting stitch config 16/27 -------------
05/14/2023 02:22:09 - INFO - train -   Test: [   0/39]  Time: 0.576 (0.576)  Loss:  0.9463 (0.9463)  Acc@1: 98.8281 (98.8281)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:11 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 02:22:11 - INFO - train -   Test: [  39/39]  Time: 0.016 (0.073)  Loss:  1.0195 (0.9444)  Acc@1: 100.0000 (99.6300)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:11 - INFO - train -   ------------- Evaluting stitch config 17/27 -------------
05/14/2023 02:22:12 - INFO - train -   Test: [   0/39]  Time: 0.602 (0.602)  Loss:  1.0752 (1.0752)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:14 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.072)  Loss:  1.0117 (1.0761)  Acc@1: 100.0000 (99.4200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:14 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 02:22:14 - INFO - train -   ------------- Evaluting stitch config 18/27 -------------
05/14/2023 02:22:15 - INFO - train -   Test: [   0/39]  Time: 0.602 (0.602)  Loss:  1.1045 (1.1045)  Acc@1: 98.4375 (98.4375)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:17 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 02:22:17 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.071)  Loss:  1.1504 (1.1020)  Acc@1: 93.7500 (99.1100)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:17 - INFO - train -   ------------- Evaluting stitch config 19/27 -------------
05/14/2023 02:22:18 - INFO - train -   Test: [   0/39]  Time: 0.619 (0.619)  Loss:  1.1543 (1.1543)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:20 - INFO - train -   Test: [  39/39]  Time: 0.014 (0.069)  Loss:  1.2461 (1.1621)  Acc@1: 100.0000 (99.0400)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:20 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 02:22:20 - INFO - train -   ------------- Evaluting stitch config 20/27 -------------
05/14/2023 02:22:21 - INFO - train -   Test: [   0/39]  Time: 0.598 (0.598)  Loss:  1.2324 (1.2324)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:23 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 02:22:23 - INFO - train -   Test: [  39/39]  Time: 0.012 (0.067)  Loss:  1.2393 (1.2475)  Acc@1: 100.0000 (99.2500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:23 - INFO - train -   ------------- Evaluting stitch config 21/27 -------------
05/14/2023 02:22:24 - INFO - train -   Test: [   0/39]  Time: 0.695 (0.695)  Loss:  1.1943 (1.1943)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:26 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.070)  Loss:  1.2500 (1.2232)  Acc@1: 93.7500 (99.2900)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:26 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 02:22:26 - INFO - train -   ------------- Evaluting stitch config 22/27 -------------
05/14/2023 02:22:27 - INFO - train -   Test: [   0/39]  Time: 0.614 (0.614)  Loss:  1.0801 (1.0801)  Acc@1: 99.2188 (99.2188)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:29 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 02:22:29 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.066)  Loss:  1.1406 (1.1181)  Acc@1: 100.0000 (99.3200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:29 - INFO - train -   ------------- Evaluting stitch config 23/27 -------------
05/14/2023 02:22:30 - INFO - train -   Test: [   0/39]  Time: 0.627 (0.627)  Loss:  0.9395 (0.9395)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:32 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 02:22:32 - INFO - train -   Test: [  39/39]  Time: 0.011 (0.065)  Loss:  0.9561 (0.9607)  Acc@1: 100.0000 (99.8500)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:32 - INFO - train -   ------------- Evaluting stitch config 24/27 -------------
05/14/2023 02:22:33 - INFO - train -   Test: [   0/39]  Time: 0.600 (0.600)  Loss:  0.7744 (0.7744)  Acc@1: 99.6094 (99.6094)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:35 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 02:22:35 - INFO - train -   Test: [  39/39]  Time: 0.010 (0.065)  Loss:  0.7891 (0.7932)  Acc@1: 100.0000 (99.7800)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:35 - INFO - train -   ------------- Evaluting stitch config 25/27 -------------
05/14/2023 02:22:36 - INFO - train -   Test: [   0/39]  Time: 0.690 (0.690)  Loss:  0.7998 (0.7998)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:38 - INFO - train -   Test: [  39/39]  Time: 0.009 (0.065)  Loss:  0.8623 (0.8156)  Acc@1: 100.0000 (99.9200)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:38 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 02:22:38 - INFO - train -   ------------- Evaluting stitch config 26/27 -------------
05/14/2023 02:22:38 - INFO - train -   Test: [   0/39]  Time: 0.607 (0.607)  Loss:  0.8457 (0.8457)  Acc@1: 100.0000 (100.0000)  Acc@5: 100.0000 (100.0000)
05/14/2023 02:22:40 - INFO - train -   Test: [  39/39]  Time: 0.008 (0.063)  Loss:  0.8018 (0.8409)  Acc@1: 100.0000 (99.9800)  Acc@5: 100.0000 (100.0000)
