04/24/2023 13:36:58 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 2, device cuda:1.
04/24/2023 13:36:58 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 2, device cuda:0.
04/24/2023 13:36:59 - INFO - train -   Model resnet50 created, param count:37904976
04/24/2023 13:37:11 - INFO - train -   Using native Torch AMP. Training in mixed precision.
04/24/2023 13:37:11 - INFO - train -   Using native Torch DistributedDataParallel.
04/24/2023 13:37:17 - INFO - train -   Scheduled epochs: 3. LR stepped per epoch.
04/24/2023 13:37:23 - INFO - train -   Train: 0 [   0/105 (  0%)]  Loss: 8.292 (8.29)  Time: 6.240s,   41.03/s  (6.240s,   41.03/s)  LR: 5.500e-06  Data: 1.705 (1.705)
04/24/2023 13:37:39 - INFO - train -   Train: 0 [  50/105 ( 48%)]  Loss: 8.010 (8.15)  Time: 0.303s,  844.91/s  (0.436s,  587.64/s)  LR: 5.500e-06  Data: 0.013 (0.046)
04/24/2023 13:37:54 - INFO - train -   Train: 0 [ 100/105 ( 96%)]  Loss: 8.066 (8.12)  Time: 0.353s,  726.17/s  (0.367s,  697.59/s)  LR: 5.500e-06  Data: 0.012 (0.029)
04/24/2023 13:37:55 - INFO - train -   Train: 0 [ 104/105 (100%)]  Loss: 7.867 (8.06)  Time: 0.254s, 1007.13/s  (0.363s,  705.30/s)  LR: 5.500e-06  Data: 0.000 (0.029)
04/24/2023 13:37:55 - INFO - train -   Distributing BatchNorm running means and vars
04/24/2023 13:37:59 - INFO - train -   Train: 1 [   0/105 (  0%)]  Loss: 8.310 (8.31)  Time: 1.139s,  224.74/s  (1.139s,  224.74/s)  LR: 5.504e-03  Data: 0.832 (0.832)
04/24/2023 13:38:14 - INFO - train -   Train: 1 [  50/105 ( 48%)]  Loss: 5.553 (6.93)  Time: 0.291s,  880.65/s  (0.316s,  808.88/s)  LR: 5.504e-03  Data: 0.012 (0.029)
04/24/2023 13:38:29 - INFO - train -   Train: 1 [ 100/105 ( 96%)]  Loss: 4.550 (6.14)  Time: 0.302s,  849.04/s  (0.307s,  832.73/s)  LR: 5.504e-03  Data: 0.013 (0.021)
04/24/2023 13:38:30 - INFO - train -   Train: 1 [ 104/105 (100%)]  Loss: 4.694 (5.78)  Time: 0.290s,  881.54/s  (0.307s,  834.69/s)  LR: 5.504e-03  Data: 0.000 (0.020)
04/24/2023 13:38:30 - INFO - train -   Distributing BatchNorm running means and vars
04/24/2023 13:38:34 - INFO - train -   Train: 2 [   0/105 (  0%)]  Loss: 4.670 (4.67)  Time: 1.139s,  224.69/s  (1.139s,  224.69/s)  LR: 1.100e-02  Data: 0.840 (0.840)
04/24/2023 13:38:49 - INFO - train -   Train: 2 [  50/105 ( 48%)]  Loss: 4.430 (4.55)  Time: 0.354s,  722.74/s  (0.318s,  804.36/s)  LR: 1.100e-02  Data: 0.013 (0.029)
04/24/2023 13:39:03 - INFO - train -   Train: 2 [ 100/105 ( 96%)]  Loss: 4.367 (4.49)  Time: 0.356s,  719.64/s  (0.305s,  839.67/s)  LR: 1.100e-02  Data: 0.012 (0.021)
04/24/2023 13:39:04 - INFO - train -   Train: 2 [ 104/105 (100%)]  Loss: 4.341 (4.45)  Time: 0.284s,  901.02/s  (0.305s,  839.31/s)  LR: 1.100e-02  Data: 0.000 (0.020)
04/24/2023 13:39:04 - INFO - train -   Distributing BatchNorm running means and vars
