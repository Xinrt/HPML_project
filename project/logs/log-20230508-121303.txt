05/08/2023 12:13:03 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 4, device cuda:1.
05/08/2023 12:13:03 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 3, total 4, device cuda:3.
05/08/2023 12:13:03 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 2, total 4, device cuda:2.
05/08/2023 12:13:03 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 4, device cuda:0.
05/08/2023 12:13:05 - INFO - train -   Model resnet18 created, param count:33651792
05/08/2023 12:13:13 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/08/2023 12:13:13 - INFO - train -   Using native Torch DistributedDataParallel.
05/08/2023 12:13:43 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/08/2023 12:13:51 - INFO - train -   Train: 0 [   0/52 (  0%)]  Loss: 6.942 (6.94)  Time: 8.222s,   62.27/s  (8.222s,   62.27/s)  LR: 5.500e-06  Data: 1.635 (1.635)
05/08/2023 12:14:05 - INFO - train -   Train: 0 [  50/52 ( 98%)]  Loss: 6.945 (6.94)  Time: 0.261s, 1964.76/s  (0.424s, 1207.66/s)  LR: 5.500e-06  Data: 0.014 (0.046)
05/08/2023 12:14:05 - INFO - train -   Train: 0 [  51/52 (100%)]  Loss: 7.334 (7.07)  Time: 0.246s, 2077.88/s  (0.421s, 1217.47/s)  LR: 5.500e-06  Data: 0.000 (0.045)
05/08/2023 12:14:05 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:14:08 - INFO - train -   Train: 1 [   0/52 (  0%)]  Loss: 7.338 (7.34)  Time: 1.303s,  392.95/s  (1.303s,  392.95/s)  LR: 5.504e-03  Data: 1.037 (1.037)
05/08/2023 12:14:21 - INFO - train -   Train: 1 [  50/52 ( 98%)]  Loss: 4.118 (5.73)  Time: 0.274s, 1867.55/s  (0.286s, 1791.47/s)  LR: 5.504e-03  Data: 0.018 (0.034)
05/08/2023 12:14:22 - INFO - train -   Train: 1 [  51/52 (100%)]  Loss: 4.113 (5.19)  Time: 0.259s, 1980.39/s  (0.285s, 1794.76/s)  LR: 5.504e-03  Data: 0.000 (0.034)
05/08/2023 12:14:22 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:14:25 - INFO - train -   Train: 2 [   0/52 (  0%)]  Loss: 4.096 (4.10)  Time: 1.380s,  371.00/s  (1.380s,  371.00/s)  LR: 1.100e-02  Data: 1.125 (1.125)
05/08/2023 12:14:38 - INFO - train -   Train: 2 [  50/52 ( 98%)]  Loss: 3.604 (3.85)  Time: 0.257s, 1994.07/s  (0.289s, 1771.61/s)  LR: 1.100e-02  Data: 0.014 (0.036)
05/08/2023 12:14:38 - INFO - train -   Train: 2 [  51/52 (100%)]  Loss: 3.758 (3.82)  Time: 0.258s, 1982.44/s  (0.288s, 1775.24/s)  LR: 1.100e-02  Data: 0.000 (0.035)
05/08/2023 12:14:38 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:14:42 - INFO - train -   Train: 3 [   0/52 (  0%)]  Loss: 3.830 (3.83)  Time: 1.403s,  364.85/s  (1.403s,  364.85/s)  LR: 1.650e-02  Data: 1.148 (1.148)
05/08/2023 12:14:55 - INFO - train -   Train: 3 [  50/52 ( 98%)]  Loss: 3.404 (3.62)  Time: 0.261s, 1964.32/s  (0.287s, 1786.60/s)  LR: 1.650e-02  Data: 0.014 (0.036)
05/08/2023 12:14:55 - INFO - train -   Train: 3 [  51/52 (100%)]  Loss: 3.486 (3.57)  Time: 0.247s, 2073.17/s  (0.286s, 1791.37/s)  LR: 1.650e-02  Data: 0.000 (0.035)
05/08/2023 12:14:55 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:14:58 - INFO - train -   Train: 4 [   0/52 (  0%)]  Loss: 3.525 (3.53)  Time: 1.434s,  357.07/s  (1.434s,  357.07/s)  LR: 2.200e-02  Data: 1.168 (1.168)
05/08/2023 12:15:11 - INFO - train -   Train: 4 [  50/52 ( 98%)]  Loss: 3.242 (3.38)  Time: 0.249s, 2057.77/s  (0.288s, 1774.76/s)  LR: 2.200e-02  Data: 0.015 (0.036)
05/08/2023 12:15:12 - INFO - train -   Train: 4 [  51/52 (100%)]  Loss: 3.374 (3.38)  Time: 0.258s, 1984.81/s  (0.288s, 1778.38/s)  LR: 2.200e-02  Data: 0.000 (0.036)
05/08/2023 12:15:12 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:15:15 - INFO - train -   Train: 5 [   0/52 (  0%)]  Loss: 3.225 (3.22)  Time: 1.485s,  344.86/s  (1.485s,  344.86/s)  LR: 2.566e-02  Data: 1.211 (1.211)
05/08/2023 12:15:28 - INFO - train -   Train: 5 [  50/52 ( 98%)]  Loss: 3.116 (3.17)  Time: 0.275s, 1860.40/s  (0.290s, 1763.17/s)  LR: 2.566e-02  Data: 0.014 (0.038)
05/08/2023 12:15:28 - INFO - train -   Train: 5 [  51/52 (100%)]  Loss: 3.236 (3.19)  Time: 0.260s, 1966.78/s  (0.290s, 1766.69/s)  LR: 2.566e-02  Data: 0.000 (0.038)
05/08/2023 12:15:28 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:15:31 - INFO - train -   Train: 6 [   0/52 (  0%)]  Loss: 3.173 (3.17)  Time: 1.414s,  362.19/s  (1.414s,  362.19/s)  LR: 2.487e-02  Data: 1.159 (1.159)
05/08/2023 12:15:45 - INFO - train -   Train: 6 [  50/52 ( 98%)]  Loss: 3.020 (3.10)  Time: 0.273s, 1875.81/s  (0.289s, 1773.04/s)  LR: 2.487e-02  Data: 0.015 (0.036)
05/08/2023 12:15:45 - INFO - train -   Train: 6 [  51/52 (100%)]  Loss: 3.005 (3.07)  Time: 0.247s, 2072.37/s  (0.288s, 1777.98/s)  LR: 2.487e-02  Data: 0.000 (0.036)
05/08/2023 12:15:45 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:15:48 - INFO - train -   Train: 7 [   0/52 (  0%)]  Loss: 2.948 (2.95)  Time: 1.473s,  347.49/s  (1.473s,  347.49/s)  LR: 2.397e-02  Data: 1.210 (1.210)
05/08/2023 12:16:02 - INFO - train -   Train: 7 [  50/52 ( 98%)]  Loss: 3.094 (3.02)  Time: 0.262s, 1956.99/s  (0.288s, 1777.05/s)  LR: 2.397e-02  Data: 0.015 (0.037)
05/08/2023 12:16:02 - INFO - train -   Train: 7 [  51/52 (100%)]  Loss: 3.010 (3.02)  Time: 0.242s, 2112.86/s  (0.287s, 1782.50/s)  LR: 2.397e-02  Data: 0.000 (0.037)
05/08/2023 12:16:02 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:16:05 - INFO - train -   Train: 8 [   0/52 (  0%)]  Loss: 3.057 (3.06)  Time: 1.461s,  350.37/s  (1.461s,  350.37/s)  LR: 2.295e-02  Data: 1.212 (1.212)
05/08/2023 12:16:18 - INFO - train -   Train: 8 [  50/52 ( 98%)]  Loss: 2.990 (3.02)  Time: 0.263s, 1948.34/s  (0.290s, 1765.02/s)  LR: 2.295e-02  Data: 0.014 (0.037)
05/08/2023 12:16:18 - INFO - train -   Train: 8 [  51/52 (100%)]  Loss: 3.000 (3.02)  Time: 0.260s, 1971.03/s  (0.289s, 1768.57/s)  LR: 2.295e-02  Data: 0.000 (0.037)
05/08/2023 12:16:18 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:16:22 - INFO - train -   Train: 9 [   0/52 (  0%)]  Loss: 3.054 (3.05)  Time: 1.534s,  333.71/s  (1.534s,  333.71/s)  LR: 2.183e-02  Data: 1.279 (1.279)
05/08/2023 12:16:35 - INFO - train -   Train: 9 [  50/52 ( 98%)]  Loss: 2.889 (2.97)  Time: 0.272s, 1884.26/s  (0.289s, 1769.01/s)  LR: 2.183e-02  Data: 0.014 (0.038)
05/08/2023 12:16:35 - INFO - train -   Train: 9 [  51/52 (100%)]  Loss: 2.921 (2.95)  Time: 0.259s, 1976.75/s  (0.289s, 1772.60/s)  LR: 2.183e-02  Data: 0.000 (0.037)
05/08/2023 12:16:35 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:16:38 - INFO - train -   Train: 10 [   0/52 (  0%)]  Loss: 2.954 (2.95)  Time: 1.504s,  340.44/s  (1.504s,  340.44/s)  LR: 2.063e-02  Data: 1.232 (1.232)
05/08/2023 12:16:51 - INFO - train -   Train: 10 [  50/52 ( 98%)]  Loss: 2.978 (2.97)  Time: 0.271s, 1890.66/s  (0.290s, 1767.59/s)  LR: 2.063e-02  Data: 0.015 (0.038)
05/08/2023 12:16:52 - INFO - train -   Train: 10 [  51/52 (100%)]  Loss: 3.093 (3.01)  Time: 0.258s, 1986.54/s  (0.289s, 1771.35/s)  LR: 2.063e-02  Data: 0.000 (0.037)
05/08/2023 12:16:52 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:16:55 - INFO - train -   Train: 11 [   0/52 (  0%)]  Loss: 3.014 (3.01)  Time: 1.427s,  358.77/s  (1.427s,  358.77/s)  LR: 1.934e-02  Data: 1.174 (1.174)
05/08/2023 12:17:08 - INFO - train -   Train: 11 [  50/52 ( 98%)]  Loss: 2.911 (2.96)  Time: 0.261s, 1961.51/s  (0.288s, 1779.96/s)  LR: 1.934e-02  Data: 0.014 (0.037)
05/08/2023 12:17:08 - INFO - train -   Train: 11 [  51/52 (100%)]  Loss: 2.910 (2.95)  Time: 0.247s, 2075.35/s  (0.287s, 1784.85/s)  LR: 1.934e-02  Data: 0.000 (0.036)
05/08/2023 12:17:08 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:17:12 - INFO - train -   Train: 12 [   0/52 (  0%)]  Loss: 3.061 (3.06)  Time: 1.527s,  335.33/s  (1.527s,  335.33/s)  LR: 1.800e-02  Data: 1.271 (1.271)
05/08/2023 12:17:25 - INFO - train -   Train: 12 [  50/52 ( 98%)]  Loss: 3.052 (3.06)  Time: 0.263s, 1949.74/s  (0.288s, 1776.47/s)  LR: 1.800e-02  Data: 0.014 (0.038)
05/08/2023 12:17:25 - INFO - train -   Train: 12 [  51/52 (100%)]  Loss: 2.886 (3.00)  Time: 0.248s, 2065.51/s  (0.287s, 1781.27/s)  LR: 1.800e-02  Data: 0.000 (0.037)
05/08/2023 12:17:25 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:17:28 - INFO - train -   Train: 13 [   0/52 (  0%)]  Loss: 2.886 (2.89)  Time: 1.551s,  330.07/s  (1.551s,  330.07/s)  LR: 1.661e-02  Data: 1.278 (1.278)
05/08/2023 12:17:41 - INFO - train -   Train: 13 [  50/52 ( 98%)]  Loss: 2.824 (2.85)  Time: 0.258s, 1985.97/s  (0.290s, 1762.82/s)  LR: 1.661e-02  Data: 0.015 (0.039)
05/08/2023 12:17:42 - INFO - train -   Train: 13 [  51/52 (100%)]  Loss: 2.781 (2.83)  Time: 0.247s, 2072.96/s  (0.290s, 1767.90/s)  LR: 1.661e-02  Data: 0.000 (0.038)
05/08/2023 12:17:42 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:17:45 - INFO - train -   Train: 14 [   0/52 (  0%)]  Loss: 2.901 (2.90)  Time: 1.483s,  345.18/s  (1.483s,  345.18/s)  LR: 1.519e-02  Data: 1.211 (1.211)
05/08/2023 12:17:58 - INFO - train -   Train: 14 [  50/52 ( 98%)]  Loss: 2.726 (2.81)  Time: 0.273s, 1878.88/s  (0.291s, 1760.97/s)  LR: 1.519e-02  Data: 0.015 (0.037)
05/08/2023 12:17:58 - INFO - train -   Train: 14 [  51/52 (100%)]  Loss: 2.800 (2.81)  Time: 0.264s, 1938.79/s  (0.290s, 1764.08/s)  LR: 1.519e-02  Data: 0.000 (0.036)
05/08/2023 12:17:58 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:18:02 - INFO - train -   Train: 15 [   0/52 (  0%)]  Loss: 2.844 (2.84)  Time: 1.488s,  344.19/s  (1.488s,  344.19/s)  LR: 1.375e-02  Data: 1.233 (1.233)
05/08/2023 12:18:15 - INFO - train -   Train: 15 [  50/52 ( 98%)]  Loss: 2.835 (2.84)  Time: 0.249s, 2056.55/s  (0.292s, 1755.77/s)  LR: 1.375e-02  Data: 0.014 (0.038)
05/08/2023 12:18:15 - INFO - train -   Train: 15 [  51/52 (100%)]  Loss: 2.880 (2.85)  Time: 0.247s, 2072.30/s  (0.291s, 1760.95/s)  LR: 1.375e-02  Data: 0.000 (0.037)
05/08/2023 12:18:15 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:18:19 - INFO - train -   Train: 16 [   0/52 (  0%)]  Loss: 2.775 (2.78)  Time: 1.516s,  337.74/s  (1.516s,  337.74/s)  LR: 1.231e-02  Data: 1.256 (1.256)
05/08/2023 12:18:32 - INFO - train -   Train: 16 [  50/52 ( 98%)]  Loss: 2.821 (2.80)  Time: 0.272s, 1880.72/s  (0.290s, 1762.71/s)  LR: 1.231e-02  Data: 0.014 (0.038)
05/08/2023 12:18:32 - INFO - train -   Train: 16 [  51/52 (100%)]  Loss: 2.816 (2.80)  Time: 0.244s, 2101.53/s  (0.290s, 1768.19/s)  LR: 1.231e-02  Data: 0.000 (0.037)
05/08/2023 12:18:32 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:18:35 - INFO - train -   Train: 17 [   0/52 (  0%)]  Loss: 2.851 (2.85)  Time: 1.465s,  349.58/s  (1.465s,  349.58/s)  LR: 1.089e-02  Data: 1.220 (1.220)
05/08/2023 12:18:48 - INFO - train -   Train: 17 [  50/52 ( 98%)]  Loss: 2.864 (2.86)  Time: 0.248s, 2065.79/s  (0.287s, 1786.46/s)  LR: 1.089e-02  Data: 0.014 (0.037)
05/08/2023 12:18:49 - INFO - train -   Train: 17 [  51/52 (100%)]  Loss: 2.827 (2.85)  Time: 0.289s, 1769.10/s  (0.287s, 1786.12/s)  LR: 1.089e-02  Data: 0.000 (0.037)
05/08/2023 12:18:49 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:18:52 - INFO - train -   Train: 18 [   0/52 (  0%)]  Loss: 2.802 (2.80)  Time: 1.606s,  318.79/s  (1.606s,  318.79/s)  LR: 9.501e-03  Data: 1.340 (1.340)
05/08/2023 12:19:05 - INFO - train -   Train: 18 [  50/52 ( 98%)]  Loss: 2.841 (2.82)  Time: 0.245s, 2088.64/s  (0.292s, 1753.65/s)  LR: 9.501e-03  Data: 0.014 (0.040)
05/08/2023 12:19:05 - INFO - train -   Train: 18 [  51/52 (100%)]  Loss: 2.800 (2.81)  Time: 0.248s, 2061.11/s  (0.291s, 1758.70/s)  LR: 9.501e-03  Data: 0.000 (0.039)
05/08/2023 12:19:05 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:19:09 - INFO - train -   Train: 19 [   0/52 (  0%)]  Loss: 2.768 (2.77)  Time: 1.413s,  362.28/s  (1.413s,  362.28/s)  LR: 8.157e-03  Data: 1.152 (1.152)
05/08/2023 12:19:22 - INFO - train -   Train: 19 [  50/52 ( 98%)]  Loss: 2.673 (2.72)  Time: 0.262s, 1952.45/s  (0.287s, 1781.39/s)  LR: 8.157e-03  Data: 0.014 (0.036)
05/08/2023 12:19:22 - INFO - train -   Train: 19 [  51/52 (100%)]  Loss: 2.784 (2.74)  Time: 0.259s, 1979.46/s  (0.287s, 1784.82/s)  LR: 8.157e-03  Data: 0.000 (0.036)
05/08/2023 12:19:22 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:19:25 - INFO - train -   Train: 20 [   0/52 (  0%)]  Loss: 2.753 (2.75)  Time: 1.398s,  366.29/s  (1.398s,  366.29/s)  LR: 6.875e-03  Data: 1.139 (1.139)
05/08/2023 12:19:39 - INFO - train -   Train: 20 [  50/52 ( 98%)]  Loss: 2.928 (2.84)  Time: 0.272s, 1882.78/s  (0.292s, 1754.09/s)  LR: 6.875e-03  Data: 0.015 (0.036)
05/08/2023 12:19:39 - INFO - train -   Train: 20 [  51/52 (100%)]  Loss: 2.760 (2.81)  Time: 0.248s, 2067.66/s  (0.291s, 1759.22/s)  LR: 6.875e-03  Data: 0.000 (0.035)
05/08/2023 12:19:39 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:19:42 - INFO - train -   Train: 21 [   0/52 (  0%)]  Loss: 2.797 (2.80)  Time: 1.552s,  329.91/s  (1.552s,  329.91/s)  LR: 5.668e-03  Data: 1.291 (1.291)
05/08/2023 12:19:55 - INFO - train -   Train: 21 [  50/52 ( 98%)]  Loss: 2.710 (2.75)  Time: 0.257s, 1991.17/s  (0.290s, 1767.61/s)  LR: 5.668e-03  Data: 0.014 (0.039)
05/08/2023 12:19:56 - INFO - train -   Train: 21 [  51/52 (100%)]  Loss: 2.820 (2.78)  Time: 0.247s, 2072.57/s  (0.289s, 1772.63/s)  LR: 5.668e-03  Data: 0.000 (0.038)
05/08/2023 12:19:56 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:19:59 - INFO - train -   Train: 22 [   0/52 (  0%)]  Loss: 2.633 (2.63)  Time: 1.417s,  361.31/s  (1.417s,  361.31/s)  LR: 4.549e-03  Data: 1.153 (1.153)
05/08/2023 12:20:12 - INFO - train -   Train: 22 [  50/52 ( 98%)]  Loss: 2.736 (2.68)  Time: 0.262s, 1956.92/s  (0.290s, 1762.54/s)  LR: 4.549e-03  Data: 0.015 (0.036)
05/08/2023 12:20:12 - INFO - train -   Train: 22 [  51/52 (100%)]  Loss: 2.727 (2.70)  Time: 0.248s, 2061.02/s  (0.290s, 1767.46/s)  LR: 4.549e-03  Data: 0.000 (0.036)
05/08/2023 12:20:12 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:20:16 - INFO - train -   Train: 23 [   0/52 (  0%)]  Loss: 2.742 (2.74)  Time: 1.525s,  335.69/s  (1.525s,  335.69/s)  LR: 3.532e-03  Data: 1.270 (1.270)
05/08/2023 12:20:29 - INFO - train -   Train: 23 [  50/52 ( 98%)]  Loss: 2.703 (2.72)  Time: 0.260s, 1966.70/s  (0.291s, 1761.77/s)  LR: 3.532e-03  Data: 0.014 (0.039)
05/08/2023 12:20:29 - INFO - train -   Train: 23 [  51/52 (100%)]  Loss: 2.799 (2.75)  Time: 0.285s, 1793.38/s  (0.291s, 1762.37/s)  LR: 3.532e-03  Data: 0.000 (0.038)
05/08/2023 12:20:29 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:20:32 - INFO - train -   Train: 24 [   0/52 (  0%)]  Loss: 2.841 (2.84)  Time: 1.410s,  363.12/s  (1.410s,  363.12/s)  LR: 2.626e-03  Data: 1.159 (1.159)
05/08/2023 12:20:45 - INFO - train -   Train: 24 [  50/52 ( 98%)]  Loss: 2.824 (2.83)  Time: 0.262s, 1953.68/s  (0.288s, 1778.09/s)  LR: 2.626e-03  Data: 0.015 (0.036)
05/08/2023 12:20:46 - INFO - train -   Train: 24 [  51/52 (100%)]  Loss: 2.839 (2.83)  Time: 0.239s, 2146.44/s  (0.287s, 1783.98/s)  LR: 2.626e-03  Data: 0.000 (0.035)
05/08/2023 12:20:46 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:20:49 - INFO - train -   Train: 25 [   0/52 (  0%)]  Loss: 2.766 (2.77)  Time: 1.315s,  389.32/s  (1.315s,  389.32/s)  LR: 1.842e-03  Data: 1.051 (1.051)
05/08/2023 12:21:02 - INFO - train -   Train: 25 [  50/52 ( 98%)]  Loss: 2.756 (2.76)  Time: 0.273s, 1873.72/s  (0.288s, 1776.77/s)  LR: 1.842e-03  Data: 0.014 (0.034)
05/08/2023 12:21:02 - INFO - train -   Train: 25 [  51/52 (100%)]  Loss: 2.731 (2.75)  Time: 0.248s, 2066.43/s  (0.287s, 1781.57/s)  LR: 1.842e-03  Data: 0.000 (0.033)
05/08/2023 12:21:02 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:21:05 - INFO - train -   Train: 26 [   0/52 (  0%)]  Loss: 2.882 (2.88)  Time: 1.442s,  355.10/s  (1.442s,  355.10/s)  LR: 1.189e-03  Data: 1.187 (1.187)
05/08/2023 12:21:18 - INFO - train -   Train: 26 [  50/52 ( 98%)]  Loss: 2.778 (2.83)  Time: 0.262s, 1954.45/s  (0.288s, 1780.19/s)  LR: 1.189e-03  Data: 0.015 (0.036)
05/08/2023 12:21:19 - INFO - train -   Train: 26 [  51/52 (100%)]  Loss: 2.736 (2.80)  Time: 0.292s, 1752.17/s  (0.288s, 1779.64/s)  LR: 1.189e-03  Data: 0.000 (0.036)
05/08/2023 12:21:19 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:21:22 - INFO - train -   Train: 27 [   0/52 (  0%)]  Loss: 2.776 (2.78)  Time: 1.489s,  343.81/s  (1.489s,  343.81/s)  LR: 6.730e-04  Data: 1.221 (1.221)
05/08/2023 12:21:35 - INFO - train -   Train: 27 [  50/52 ( 98%)]  Loss: 2.793 (2.78)  Time: 0.262s, 1955.79/s  (0.290s, 1767.44/s)  LR: 6.730e-04  Data: 0.014 (0.038)
05/08/2023 12:21:35 - INFO - train -   Train: 27 [  51/52 (100%)]  Loss: 2.815 (2.79)  Time: 0.258s, 1986.85/s  (0.289s, 1771.21/s)  LR: 6.730e-04  Data: 0.000 (0.037)
05/08/2023 12:21:35 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:21:39 - INFO - train -   Train: 28 [   0/52 (  0%)]  Loss: 2.796 (2.80)  Time: 1.478s,  346.49/s  (1.478s,  346.49/s)  LR: 3.005e-04  Data: 1.222 (1.222)
05/08/2023 12:21:52 - INFO - train -   Train: 28 [  50/52 ( 98%)]  Loss: 2.919 (2.86)  Time: 0.253s, 2022.86/s  (0.289s, 1769.75/s)  LR: 3.005e-04  Data: 0.014 (0.038)
05/08/2023 12:21:52 - INFO - train -   Train: 28 [  51/52 (100%)]  Loss: 2.690 (2.80)  Time: 0.247s, 2075.33/s  (0.288s, 1774.77/s)  LR: 3.005e-04  Data: 0.000 (0.037)
05/08/2023 12:21:52 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:21:56 - INFO - train -   Train: 29 [   0/52 (  0%)]  Loss: 2.791 (2.79)  Time: 1.436s,  356.62/s  (1.436s,  356.62/s)  LR: 7.532e-05  Data: 1.181 (1.181)
05/08/2023 12:22:09 - INFO - train -   Train: 29 [  50/52 ( 98%)]  Loss: 2.705 (2.75)  Time: 0.273s, 1872.60/s  (0.289s, 1771.06/s)  LR: 7.532e-05  Data: 0.014 (0.037)
05/08/2023 12:22:09 - INFO - train -   Train: 29 [  51/52 (100%)]  Loss: 2.750 (2.75)  Time: 0.258s, 1982.61/s  (0.288s, 1774.71/s)  LR: 7.532e-05  Data: 0.000 (0.036)
05/08/2023 12:22:09 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 12:22:09 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 12:22:09 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 12:22:09 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 12:22:11 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 12:22:13 - INFO - train -   Test: [   0/19]  Time: 1.740 (1.740)  Loss:  1.2920 (1.2920)  Acc@1: 69.9219 (69.9219)  Acc@5: 98.6328 (98.6328)
05/08/2023 12:22:14 - INFO - train -   Test: [  19/19]  Time: 0.141 (0.166)  Loss:  1.2617 (1.2637)  Acc@1: 67.6471 (68.8600)  Acc@5: 98.8971 (98.8200)
05/08/2023 12:22:14 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 12:22:14 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 12:22:14 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 12:22:14 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 12:22:15 - INFO - train -   Test: [   0/19]  Time: 0.835 (0.835)  Loss:  0.7671 (0.7671)  Acc@1: 86.3281 (86.3281)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:16 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 12:22:16 - INFO - train -   Test: [  19/19]  Time: 0.016 (0.091)  Loss:  0.7583 (0.7619)  Acc@1: 82.7206 (84.9300)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:16 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 12:22:16 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 12:22:16 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 12:22:17 - INFO - train -   Test: [   0/19]  Time: 0.805 (0.805)  Loss:  0.8169 (0.8169)  Acc@1: 82.6172 (82.6172)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:18 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 12:22:18 - INFO - train -   Test: [  19/19]  Time: 0.029 (0.092)  Loss:  0.7778 (0.7907)  Acc@1: 85.2941 (83.3100)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:18 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 12:22:18 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 12:22:18 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 12:22:19 - INFO - train -   Test: [   0/19]  Time: 0.815 (0.815)  Loss:  0.8047 (0.8047)  Acc@1: 80.4688 (80.4688)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:20 - INFO - train -   Test: [  19/19]  Time: 0.015 (0.091)  Loss:  0.7744 (0.7865)  Acc@1: 83.8235 (83.3100)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:20 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 12:22:20 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 12:22:20 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 12:22:20 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 12:22:21 - INFO - train -   Test: [   0/19]  Time: 0.845 (0.845)  Loss:  0.8491 (0.8491)  Acc@1: 80.8594 (80.8594)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:22 - INFO - train -   Test: [  19/19]  Time: 0.014 (0.091)  Loss:  0.8423 (0.8563)  Acc@1: 80.1471 (80.7000)  Acc@5: 100.0000 (99.9900)
05/08/2023 12:22:22 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 12:22:22 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 12:22:22 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 12:22:22 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 12:22:23 - INFO - train -   Test: [   0/19]  Time: 0.828 (0.828)  Loss:  0.8623 (0.8623)  Acc@1: 85.1562 (85.1562)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:24 - INFO - train -   Test: [  19/19]  Time: 0.028 (0.091)  Loss:  0.8564 (0.8701)  Acc@1: 88.2353 (86.2100)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:24 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 12:22:24 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 12:22:24 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 12:22:24 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 12:22:25 - INFO - train -   Test: [   0/19]  Time: 0.819 (0.819)  Loss:  0.9233 (0.9233)  Acc@1: 77.1484 (77.1484)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:26 - INFO - train -   Test: [  19/19]  Time: 0.013 (0.088)  Loss:  0.9404 (0.9252)  Acc@1: 77.5735 (78.3300)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:26 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 12:22:26 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 12:22:26 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 12:22:26 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 12:22:27 - INFO - train -   Test: [   0/19]  Time: 0.785 (0.785)  Loss:  0.9644 (0.9644)  Acc@1: 76.7578 (76.7578)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:28 - INFO - train -   Test: [  19/19]  Time: 0.013 (0.089)  Loss:  0.9800 (0.9580)  Acc@1: 75.0000 (76.5900)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:28 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 12:22:28 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 12:22:28 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 12:22:28 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 12:22:29 - INFO - train -   Test: [   0/19]  Time: 0.841 (0.841)  Loss:  1.0996 (1.0996)  Acc@1: 73.4375 (73.4375)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:30 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 12:22:30 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 12:22:30 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 12:22:30 - INFO - train -   Test: [  19/19]  Time: 0.012 (0.089)  Loss:  1.1035 (1.0978)  Acc@1: 77.2059 (73.7000)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:30 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 12:22:31 - INFO - train -   Test: [   0/19]  Time: 0.761 (0.761)  Loss:  1.2568 (1.2568)  Acc@1: 70.7031 (70.7031)  Acc@5: 100.0000 (100.0000)
05/08/2023 12:22:32 - INFO - train -   Test: [  19/19]  Time: 0.011 (0.086)  Loss:  1.2656 (1.2580)  Acc@1: 70.5882 (70.8400)  Acc@5: 100.0000 (100.0000)
