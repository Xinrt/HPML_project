05/08/2023 22:17:21 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 3, total 4, device cuda:3.
05/08/2023 22:17:21 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 2, total 4, device cuda:2.
05/08/2023 22:17:21 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 1, total 4, device cuda:1.
05/08/2023 22:17:21 - INFO - train -   Training in distributed mode with multiple processes, 1 device per process.Process 0, total 4, device cuda:0.
05/08/2023 22:17:25 - INFO - train -   Model resnet18 created, param count:38448976
05/08/2023 22:18:32 - INFO - train -   Using native Torch AMP. Training in mixed precision.
05/08/2023 22:18:32 - INFO - train -   Using native Torch DistributedDataParallel.
05/08/2023 22:18:43 - INFO - train -   Scheduled epochs: 30. LR stepped per epoch.
05/08/2023 22:18:51 - INFO - train -   Train: 0 [   0/52 (  0%)]  Loss: 6.896 (6.90)  Time: 8.299s,   61.69/s  (8.299s,   61.69/s)  LR: 5.500e-06  Data: 1.820 (1.820)
05/08/2023 22:19:10 - INFO - train -   Train: 0 [  50/52 ( 98%)]  Loss: 6.965 (6.93)  Time: 0.302s, 1694.04/s  (0.527s,  971.52/s)  LR: 5.500e-06  Data: 0.014 (0.049)
05/08/2023 22:19:10 - INFO - train -   Train: 0 [  51/52 (100%)]  Loss: 6.896 (6.92)  Time: 0.287s, 1783.96/s  (0.522s,  980.11/s)  LR: 5.500e-06  Data: 0.000 (0.048)
05/08/2023 22:19:10 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:19:13 - INFO - train -   Train: 1 [   0/52 (  0%)]  Loss: 6.855 (6.86)  Time: 1.375s,  372.30/s  (1.375s,  372.30/s)  LR: 5.504e-03  Data: 1.066 (1.066)
05/08/2023 22:19:28 - INFO - train -   Train: 1 [  50/52 ( 98%)]  Loss: 4.103 (5.48)  Time: 0.309s, 1654.49/s  (0.328s, 1562.19/s)  LR: 5.504e-03  Data: 0.012 (0.033)
05/08/2023 22:19:29 - INFO - train -   Train: 1 [  51/52 (100%)]  Loss: 4.081 (5.01)  Time: 0.292s, 1754.88/s  (0.327s, 1565.49/s)  LR: 5.504e-03  Data: 0.000 (0.033)
05/08/2023 22:19:29 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:19:32 - INFO - train -   Train: 2 [   0/52 (  0%)]  Loss: 4.075 (4.07)  Time: 1.413s,  362.29/s  (1.413s,  362.29/s)  LR: 1.100e-02  Data: 1.113 (1.113)
05/08/2023 22:19:48 - INFO - train -   Train: 2 [  50/52 ( 98%)]  Loss: 3.632 (3.85)  Time: 0.303s, 1689.61/s  (0.330s, 1549.99/s)  LR: 1.100e-02  Data: 0.014 (0.035)
05/08/2023 22:19:48 - INFO - train -   Train: 2 [  51/52 (100%)]  Loss: 3.666 (3.79)  Time: 0.294s, 1742.93/s  (0.330s, 1553.29/s)  LR: 1.100e-02  Data: 0.000 (0.034)
05/08/2023 22:19:48 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:19:51 - INFO - train -   Train: 3 [   0/52 (  0%)]  Loss: 3.772 (3.77)  Time: 1.460s,  350.77/s  (1.460s,  350.77/s)  LR: 1.650e-02  Data: 1.165 (1.165)
05/08/2023 22:20:07 - INFO - train -   Train: 3 [  50/52 ( 98%)]  Loss: 3.379 (3.58)  Time: 0.312s, 1639.98/s  (0.331s, 1547.71/s)  LR: 1.650e-02  Data: 0.013 (0.036)
05/08/2023 22:20:07 - INFO - train -   Train: 3 [  51/52 (100%)]  Loss: 3.444 (3.53)  Time: 0.293s, 1745.55/s  (0.330s, 1551.09/s)  LR: 1.650e-02  Data: 0.000 (0.035)
05/08/2023 22:20:07 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:20:10 - INFO - train -   Train: 4 [   0/52 (  0%)]  Loss: 3.579 (3.58)  Time: 1.465s,  349.41/s  (1.465s,  349.41/s)  LR: 2.200e-02  Data: 1.152 (1.152)
05/08/2023 22:20:26 - INFO - train -   Train: 4 [  50/52 ( 98%)]  Loss: 3.257 (3.42)  Time: 0.301s, 1699.27/s  (0.331s, 1546.66/s)  LR: 2.200e-02  Data: 0.012 (0.035)
05/08/2023 22:20:26 - INFO - train -   Train: 4 [  51/52 (100%)]  Loss: 3.292 (3.38)  Time: 0.288s, 1777.50/s  (0.330s, 1550.53/s)  LR: 2.200e-02  Data: 0.000 (0.035)
05/08/2023 22:20:26 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:20:29 - INFO - train -   Train: 5 [   0/52 (  0%)]  Loss: 3.158 (3.16)  Time: 1.342s,  381.46/s  (1.342s,  381.46/s)  LR: 2.566e-02  Data: 1.047 (1.047)
05/08/2023 22:20:45 - INFO - train -   Train: 5 [  50/52 ( 98%)]  Loss: 3.169 (3.16)  Time: 0.304s, 1682.04/s  (0.329s, 1557.79/s)  LR: 2.566e-02  Data: 0.014 (0.033)
05/08/2023 22:20:45 - INFO - train -   Train: 5 [  51/52 (100%)]  Loss: 3.076 (3.13)  Time: 0.288s, 1777.38/s  (0.328s, 1561.50/s)  LR: 2.566e-02  Data: 0.000 (0.032)
05/08/2023 22:20:45 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:20:48 - INFO - train -   Train: 6 [   0/52 (  0%)]  Loss: 3.245 (3.24)  Time: 1.407s,  363.84/s  (1.407s,  363.84/s)  LR: 2.487e-02  Data: 1.111 (1.111)
05/08/2023 22:21:03 - INFO - train -   Train: 6 [  50/52 ( 98%)]  Loss: 2.985 (3.11)  Time: 0.307s, 1666.84/s  (0.329s, 1555.57/s)  LR: 2.487e-02  Data: 0.013 (0.034)
05/08/2023 22:21:04 - INFO - train -   Train: 6 [  51/52 (100%)]  Loss: 2.894 (3.04)  Time: 0.292s, 1755.31/s  (0.328s, 1558.98/s)  LR: 2.487e-02  Data: 0.000 (0.034)
05/08/2023 22:21:04 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:21:07 - INFO - train -   Train: 7 [   0/52 (  0%)]  Loss: 3.035 (3.04)  Time: 1.410s,  363.06/s  (1.410s,  363.06/s)  LR: 2.397e-02  Data: 1.101 (1.101)
05/08/2023 22:21:22 - INFO - train -   Train: 7 [  50/52 ( 98%)]  Loss: 3.075 (3.06)  Time: 0.306s, 1670.81/s  (0.332s, 1541.79/s)  LR: 2.397e-02  Data: 0.012 (0.034)
05/08/2023 22:21:23 - INFO - train -   Train: 7 [  51/52 (100%)]  Loss: 2.995 (3.04)  Time: 0.290s, 1764.38/s  (0.331s, 1545.54/s)  LR: 2.397e-02  Data: 0.000 (0.034)
05/08/2023 22:21:23 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:21:26 - INFO - train -   Train: 8 [   0/52 (  0%)]  Loss: 2.899 (2.90)  Time: 1.327s,  385.85/s  (1.327s,  385.85/s)  LR: 2.295e-02  Data: 1.023 (1.023)
05/08/2023 22:21:41 - INFO - train -   Train: 8 [  50/52 ( 98%)]  Loss: 2.904 (2.90)  Time: 0.308s, 1659.96/s  (0.327s, 1563.76/s)  LR: 2.295e-02  Data: 0.015 (0.033)
05/08/2023 22:21:42 - INFO - train -   Train: 8 [  51/52 (100%)]  Loss: 2.936 (2.91)  Time: 0.299s, 1715.06/s  (0.327s, 1566.42/s)  LR: 2.295e-02  Data: 0.000 (0.032)
05/08/2023 22:21:42 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:21:45 - INFO - train -   Train: 9 [   0/52 (  0%)]  Loss: 2.944 (2.94)  Time: 1.398s,  366.17/s  (1.398s,  366.17/s)  LR: 2.183e-02  Data: 1.099 (1.099)
05/08/2023 22:22:00 - INFO - train -   Train: 9 [  50/52 ( 98%)]  Loss: 2.745 (2.84)  Time: 0.305s, 1676.45/s  (0.329s, 1556.54/s)  LR: 2.183e-02  Data: 0.014 (0.034)
05/08/2023 22:22:00 - INFO - train -   Train: 9 [  51/52 (100%)]  Loss: 2.846 (2.84)  Time: 0.291s, 1759.93/s  (0.328s, 1560.01/s)  LR: 2.183e-02  Data: 0.000 (0.034)
05/08/2023 22:22:00 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:22:04 - INFO - train -   Train: 10 [   0/52 (  0%)]  Loss: 2.834 (2.83)  Time: 1.359s,  376.85/s  (1.359s,  376.85/s)  LR: 2.063e-02  Data: 1.052 (1.052)
05/08/2023 22:22:19 - INFO - train -   Train: 10 [  50/52 ( 98%)]  Loss: 2.946 (2.89)  Time: 0.302s, 1695.77/s  (0.328s, 1561.43/s)  LR: 2.063e-02  Data: 0.011 (0.033)
05/08/2023 22:22:19 - INFO - train -   Train: 10 [  51/52 (100%)]  Loss: 2.941 (2.91)  Time: 0.287s, 1784.79/s  (0.327s, 1565.20/s)  LR: 2.063e-02  Data: 0.000 (0.032)
05/08/2023 22:22:19 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:22:22 - INFO - train -   Train: 11 [   0/52 (  0%)]  Loss: 2.889 (2.89)  Time: 1.354s,  378.27/s  (1.354s,  378.27/s)  LR: 1.934e-02  Data: 1.057 (1.057)
05/08/2023 22:22:38 - INFO - train -   Train: 11 [  50/52 ( 98%)]  Loss: 2.845 (2.87)  Time: 0.310s, 1649.65/s  (0.328s, 1561.60/s)  LR: 1.934e-02  Data: 0.013 (0.033)
05/08/2023 22:22:38 - INFO - train -   Train: 11 [  51/52 (100%)]  Loss: 2.670 (2.80)  Time: 0.292s, 1752.65/s  (0.327s, 1564.88/s)  LR: 1.934e-02  Data: 0.000 (0.032)
05/08/2023 22:22:38 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:22:41 - INFO - train -   Train: 12 [   0/52 (  0%)]  Loss: 3.056 (3.06)  Time: 1.438s,  355.94/s  (1.438s,  355.94/s)  LR: 1.800e-02  Data: 1.142 (1.142)
05/08/2023 22:22:57 - INFO - train -   Train: 12 [  50/52 ( 98%)]  Loss: 2.917 (2.99)  Time: 0.305s, 1677.94/s  (0.330s, 1550.57/s)  LR: 1.800e-02  Data: 0.014 (0.035)
05/08/2023 22:22:57 - INFO - train -   Train: 12 [  51/52 (100%)]  Loss: 2.800 (2.92)  Time: 0.290s, 1765.70/s  (0.329s, 1554.21/s)  LR: 1.800e-02  Data: 0.000 (0.034)
05/08/2023 22:22:57 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:23:00 - INFO - train -   Train: 13 [   0/52 (  0%)]  Loss: 2.839 (2.84)  Time: 1.423s,  359.82/s  (1.423s,  359.82/s)  LR: 1.661e-02  Data: 1.102 (1.102)
05/08/2023 22:23:15 - INFO - train -   Train: 13 [  50/52 ( 98%)]  Loss: 2.834 (2.84)  Time: 0.309s, 1657.99/s  (0.330s, 1552.29/s)  LR: 1.661e-02  Data: 0.012 (0.034)
05/08/2023 22:23:16 - INFO - train -   Train: 13 [  51/52 (100%)]  Loss: 2.761 (2.81)  Time: 0.291s, 1759.28/s  (0.329s, 1555.81/s)  LR: 1.661e-02  Data: 0.000 (0.034)
05/08/2023 22:23:16 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:23:19 - INFO - train -   Train: 14 [   0/52 (  0%)]  Loss: 2.840 (2.84)  Time: 1.370s,  373.77/s  (1.370s,  373.77/s)  LR: 1.519e-02  Data: 1.075 (1.075)
05/08/2023 22:23:34 - INFO - train -   Train: 14 [  50/52 ( 98%)]  Loss: 2.694 (2.77)  Time: 0.309s, 1657.34/s  (0.330s, 1550.45/s)  LR: 1.519e-02  Data: 0.014 (0.034)
05/08/2023 22:23:35 - INFO - train -   Train: 14 [  51/52 (100%)]  Loss: 2.822 (2.79)  Time: 0.291s, 1759.20/s  (0.329s, 1553.99/s)  LR: 1.519e-02  Data: 0.000 (0.033)
05/08/2023 22:23:35 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:23:38 - INFO - train -   Train: 15 [   0/52 (  0%)]  Loss: 2.789 (2.79)  Time: 1.353s,  378.45/s  (1.353s,  378.45/s)  LR: 1.375e-02  Data: 1.057 (1.057)
05/08/2023 22:23:53 - INFO - train -   Train: 15 [  50/52 ( 98%)]  Loss: 2.690 (2.74)  Time: 0.300s, 1709.33/s  (0.330s, 1551.46/s)  LR: 1.375e-02  Data: 0.011 (0.033)
05/08/2023 22:23:53 - INFO - train -   Train: 15 [  51/52 (100%)]  Loss: 2.823 (2.77)  Time: 0.290s, 1767.67/s  (0.329s, 1555.12/s)  LR: 1.375e-02  Data: 0.000 (0.033)
05/08/2023 22:23:53 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:23:57 - INFO - train -   Train: 16 [   0/52 (  0%)]  Loss: 2.692 (2.69)  Time: 1.325s,  386.30/s  (1.325s,  386.30/s)  LR: 1.231e-02  Data: 1.021 (1.021)
05/08/2023 22:24:12 - INFO - train -   Train: 16 [  50/52 ( 98%)]  Loss: 2.877 (2.78)  Time: 0.306s, 1671.54/s  (0.330s, 1549.77/s)  LR: 1.231e-02  Data: 0.014 (0.033)
05/08/2023 22:24:13 - INFO - train -   Train: 16 [  51/52 (100%)]  Loss: 2.826 (2.80)  Time: 0.292s, 1753.67/s  (0.330s, 1553.24/s)  LR: 1.231e-02  Data: 0.000 (0.032)
05/08/2023 22:24:13 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:24:16 - INFO - train -   Train: 17 [   0/52 (  0%)]  Loss: 2.768 (2.77)  Time: 1.367s,  374.61/s  (1.367s,  374.61/s)  LR: 1.089e-02  Data: 1.072 (1.072)
05/08/2023 22:24:31 - INFO - train -   Train: 17 [  50/52 ( 98%)]  Loss: 2.755 (2.76)  Time: 0.301s, 1700.63/s  (0.330s, 1550.37/s)  LR: 1.089e-02  Data: 0.013 (0.034)
05/08/2023 22:24:32 - INFO - train -   Train: 17 [  51/52 (100%)]  Loss: 2.837 (2.79)  Time: 0.291s, 1761.77/s  (0.329s, 1553.95/s)  LR: 1.089e-02  Data: 0.000 (0.033)
05/08/2023 22:24:32 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:24:35 - INFO - train -   Train: 18 [   0/52 (  0%)]  Loss: 2.795 (2.80)  Time: 1.353s,  378.28/s  (1.353s,  378.28/s)  LR: 9.501e-03  Data: 1.059 (1.059)
05/08/2023 22:24:50 - INFO - train -   Train: 18 [  50/52 ( 98%)]  Loss: 2.753 (2.77)  Time: 0.300s, 1709.31/s  (0.328s, 1561.27/s)  LR: 9.501e-03  Data: 0.012 (0.033)
05/08/2023 22:24:50 - INFO - train -   Train: 18 [  51/52 (100%)]  Loss: 2.706 (2.75)  Time: 0.295s, 1733.22/s  (0.327s, 1564.26/s)  LR: 9.501e-03  Data: 0.000 (0.033)
05/08/2023 22:24:50 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:24:54 - INFO - train -   Train: 19 [   0/52 (  0%)]  Loss: 2.699 (2.70)  Time: 1.243s,  412.01/s  (1.243s,  412.01/s)  LR: 8.157e-03  Data: 0.926 (0.926)
05/08/2023 22:25:09 - INFO - train -   Train: 19 [  50/52 ( 98%)]  Loss: 2.687 (2.69)  Time: 0.303s, 1691.71/s  (0.326s, 1568.21/s)  LR: 8.157e-03  Data: 0.014 (0.031)
05/08/2023 22:25:09 - INFO - train -   Train: 19 [  51/52 (100%)]  Loss: 2.713 (2.70)  Time: 0.293s, 1750.10/s  (0.326s, 1571.35/s)  LR: 8.157e-03  Data: 0.000 (0.030)
05/08/2023 22:25:09 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:25:12 - INFO - train -   Train: 20 [   0/52 (  0%)]  Loss: 2.722 (2.72)  Time: 1.313s,  389.87/s  (1.313s,  389.87/s)  LR: 6.875e-03  Data: 1.019 (1.019)
05/08/2023 22:25:28 - INFO - train -   Train: 20 [  50/52 ( 98%)]  Loss: 2.724 (2.72)  Time: 0.306s, 1674.04/s  (0.328s, 1561.48/s)  LR: 6.875e-03  Data: 0.012 (0.032)
05/08/2023 22:25:28 - INFO - train -   Train: 20 [  51/52 (100%)]  Loss: 2.722 (2.72)  Time: 0.291s, 1758.72/s  (0.327s, 1564.86/s)  LR: 6.875e-03  Data: 0.000 (0.032)
05/08/2023 22:25:28 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:25:31 - INFO - train -   Train: 21 [   0/52 (  0%)]  Loss: 2.758 (2.76)  Time: 1.342s,  381.63/s  (1.342s,  381.63/s)  LR: 5.668e-03  Data: 1.047 (1.047)
05/08/2023 22:25:47 - INFO - train -   Train: 21 [  50/52 ( 98%)]  Loss: 2.708 (2.73)  Time: 0.305s, 1675.95/s  (0.327s, 1564.29/s)  LR: 5.668e-03  Data: 0.012 (0.033)
05/08/2023 22:25:47 - INFO - train -   Train: 21 [  51/52 (100%)]  Loss: 2.658 (2.71)  Time: 0.291s, 1761.90/s  (0.327s, 1567.67/s)  LR: 5.668e-03  Data: 0.000 (0.032)
05/08/2023 22:25:47 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:25:50 - INFO - train -   Train: 22 [   0/52 (  0%)]  Loss: 2.621 (2.62)  Time: 1.329s,  385.27/s  (1.329s,  385.27/s)  LR: 4.549e-03  Data: 1.022 (1.022)
05/08/2023 22:26:06 - INFO - train -   Train: 22 [  50/52 ( 98%)]  Loss: 2.617 (2.62)  Time: 0.306s, 1671.80/s  (0.328s, 1562.43/s)  LR: 4.549e-03  Data: 0.014 (0.033)
05/08/2023 22:26:06 - INFO - train -   Train: 22 [  51/52 (100%)]  Loss: 2.683 (2.64)  Time: 0.288s, 1778.14/s  (0.327s, 1566.08/s)  LR: 4.549e-03  Data: 0.000 (0.032)
05/08/2023 22:26:06 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:26:09 - INFO - train -   Train: 23 [   0/52 (  0%)]  Loss: 2.722 (2.72)  Time: 1.377s,  371.80/s  (1.377s,  371.80/s)  LR: 3.532e-03  Data: 1.079 (1.079)
05/08/2023 22:26:24 - INFO - train -   Train: 23 [  50/52 ( 98%)]  Loss: 2.732 (2.73)  Time: 0.306s, 1671.92/s  (0.328s, 1560.13/s)  LR: 3.532e-03  Data: 0.012 (0.034)
05/08/2023 22:26:25 - INFO - train -   Train: 23 [  51/52 (100%)]  Loss: 2.756 (2.74)  Time: 0.295s, 1738.32/s  (0.328s, 1563.21/s)  LR: 3.532e-03  Data: 0.000 (0.033)
05/08/2023 22:26:25 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:26:28 - INFO - train -   Train: 24 [   0/52 (  0%)]  Loss: 2.739 (2.74)  Time: 1.333s,  384.13/s  (1.333s,  384.13/s)  LR: 2.626e-03  Data: 1.029 (1.029)
05/08/2023 22:26:43 - INFO - train -   Train: 24 [  50/52 ( 98%)]  Loss: 2.816 (2.78)  Time: 0.310s, 1654.12/s  (0.327s, 1564.89/s)  LR: 2.626e-03  Data: 0.012 (0.033)
05/08/2023 22:26:44 - INFO - train -   Train: 24 [  51/52 (100%)]  Loss: 2.844 (2.80)  Time: 0.287s, 1783.74/s  (0.326s, 1568.59/s)  LR: 2.626e-03  Data: 0.000 (0.032)
05/08/2023 22:26:44 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:26:47 - INFO - train -   Train: 25 [   0/52 (  0%)]  Loss: 2.784 (2.78)  Time: 1.345s,  380.71/s  (1.345s,  380.71/s)  LR: 1.842e-03  Data: 1.037 (1.037)
05/08/2023 22:27:02 - INFO - train -   Train: 25 [  50/52 ( 98%)]  Loss: 2.750 (2.77)  Time: 0.302s, 1693.28/s  (0.328s, 1559.21/s)  LR: 1.842e-03  Data: 0.011 (0.033)
05/08/2023 22:27:03 - INFO - train -   Train: 25 [  51/52 (100%)]  Loss: 2.662 (2.73)  Time: 0.290s, 1768.03/s  (0.328s, 1562.76/s)  LR: 1.842e-03  Data: 0.000 (0.032)
05/08/2023 22:27:03 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:27:06 - INFO - train -   Train: 26 [   0/52 (  0%)]  Loss: 2.862 (2.86)  Time: 1.428s,  358.46/s  (1.428s,  358.46/s)  LR: 1.189e-03  Data: 1.123 (1.123)
05/08/2023 22:27:21 - INFO - train -   Train: 26 [  50/52 ( 98%)]  Loss: 2.690 (2.78)  Time: 0.306s, 1674.56/s  (0.330s, 1551.13/s)  LR: 1.189e-03  Data: 0.015 (0.035)
05/08/2023 22:27:21 - INFO - train -   Train: 26 [  51/52 (100%)]  Loss: 2.692 (2.75)  Time: 0.289s, 1769.77/s  (0.329s, 1554.82/s)  LR: 1.189e-03  Data: 0.000 (0.034)
05/08/2023 22:27:21 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:27:25 - INFO - train -   Train: 27 [   0/52 (  0%)]  Loss: 2.642 (2.64)  Time: 1.486s,  344.48/s  (1.486s,  344.48/s)  LR: 6.730e-04  Data: 1.186 (1.186)
05/08/2023 22:27:40 - INFO - train -   Train: 27 [  50/52 ( 98%)]  Loss: 2.657 (2.65)  Time: 0.304s, 1684.74/s  (0.333s, 1538.54/s)  LR: 6.730e-04  Data: 0.014 (0.037)
05/08/2023 22:27:41 - INFO - train -   Train: 27 [  51/52 (100%)]  Loss: 2.752 (2.68)  Time: 0.293s, 1745.17/s  (0.332s, 1542.05/s)  LR: 6.730e-04  Data: 0.000 (0.036)
05/08/2023 22:27:41 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:27:44 - INFO - train -   Train: 28 [   0/52 (  0%)]  Loss: 2.738 (2.74)  Time: 1.473s,  347.68/s  (1.473s,  347.68/s)  LR: 3.005e-04  Data: 1.165 (1.165)
05/08/2023 22:27:59 - INFO - train -   Train: 28 [  50/52 ( 98%)]  Loss: 2.858 (2.80)  Time: 0.303s, 1689.42/s  (0.330s, 1550.05/s)  LR: 3.005e-04  Data: 0.011 (0.035)
05/08/2023 22:28:00 - INFO - train -   Train: 28 [  51/52 (100%)]  Loss: 2.682 (2.76)  Time: 0.292s, 1751.32/s  (0.330s, 1553.49/s)  LR: 3.005e-04  Data: 0.000 (0.035)
05/08/2023 22:28:00 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:28:03 - INFO - train -   Train: 29 [   0/52 (  0%)]  Loss: 2.702 (2.70)  Time: 1.152s,  444.48/s  (1.152s,  444.48/s)  LR: 7.532e-05  Data: 0.853 (0.853)
05/08/2023 22:28:18 - INFO - train -   Train: 29 [  50/52 ( 98%)]  Loss: 2.707 (2.70)  Time: 0.308s, 1659.78/s  (0.324s, 1582.00/s)  LR: 7.532e-05  Data: 0.014 (0.030)
05/08/2023 22:28:19 - INFO - train -   Train: 29 [  51/52 (100%)]  Loss: 2.743 (2.72)  Time: 0.297s, 1722.32/s  (0.323s, 1584.48/s)  LR: 7.532e-05  Data: 0.000 (0.030)
05/08/2023 22:28:19 - INFO - train -   Distributing BatchNorm running means and vars
05/08/2023 22:28:19 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:28:19 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:28:19 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:28:21 - INFO - train -   ------------- Evaluting stitch config 0/10 -------------
05/08/2023 22:28:23 - INFO - train -   Test: [   0/19]  Time: 2.008 (2.008)  Loss:  1.0527 (1.0527)  Acc@1: 74.4141 (74.4141)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:25 - INFO - train -   Test: [  19/19]  Time: 0.508 (0.205)  Loss:  1.1133 (1.0967)  Acc@1: 72.4265 (73.3200)  Acc@5: 100.0000 (99.9300)
05/08/2023 22:28:25 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:28:25 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:28:25 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:28:25 - INFO - train -   ------------- Evaluting stitch config 1/10 -------------
05/08/2023 22:28:26 - INFO - train -   Test: [   0/19]  Time: 0.830 (0.830)  Loss:  0.7544 (0.7544)  Acc@1: 87.8906 (87.8906)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:27 - INFO - train -   Test: [  19/19]  Time: 0.058 (0.090)  Loss:  0.7373 (0.7545)  Acc@1: 85.6618 (86.6500)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:27 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:28:27 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:28:27 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:28:27 - INFO - train -   ------------- Evaluting stitch config 2/10 -------------
05/08/2023 22:28:28 - INFO - train -   Test: [   0/19]  Time: 0.795 (0.795)  Loss:  0.7998 (0.7998)  Acc@1: 81.8359 (81.8359)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:29 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:28:29 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:28:29 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:28:29 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.093)  Loss:  0.8008 (0.7770)  Acc@1: 82.3529 (83.3900)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:29 - INFO - train -   ------------- Evaluting stitch config 3/10 -------------
05/08/2023 22:28:30 - INFO - train -   Test: [   0/19]  Time: 0.760 (0.760)  Loss:  0.8115 (0.8115)  Acc@1: 83.2031 (83.2031)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:31 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:28:31 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:28:31 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:28:31 - INFO - train -   Test: [  19/19]  Time: 0.020 (0.093)  Loss:  0.8110 (0.8085)  Acc@1: 81.2500 (83.1400)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:31 - INFO - train -   ------------- Evaluting stitch config 4/10 -------------
05/08/2023 22:28:32 - INFO - train -   Test: [   0/19]  Time: 0.764 (0.764)  Loss:  0.8359 (0.8359)  Acc@1: 82.8125 (82.8125)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:33 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.092)  Loss:  0.8379 (0.8317)  Acc@1: 82.3529 (81.3800)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:33 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:28:33 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:28:33 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:28:33 - INFO - train -   ------------- Evaluting stitch config 5/10 -------------
05/08/2023 22:28:34 - INFO - train -   Test: [   0/19]  Time: 0.811 (0.811)  Loss:  0.7783 (0.7783)  Acc@1: 87.3047 (87.3047)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:35 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.095)  Loss:  0.7690 (0.7850)  Acc@1: 88.2353 (86.2500)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:35 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:28:35 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:28:35 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:28:35 - INFO - train -   ------------- Evaluting stitch config 6/10 -------------
05/08/2023 22:28:36 - INFO - train -   Test: [   0/19]  Time: 0.821 (0.821)  Loss:  0.7153 (0.7153)  Acc@1: 85.9375 (85.9375)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:37 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:28:37 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:28:37 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.093)  Loss:  0.7339 (0.7056)  Acc@1: 87.5000 (86.7900)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:37 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:28:37 - INFO - train -   ------------- Evaluting stitch config 7/10 -------------
05/08/2023 22:28:38 - INFO - train -   Test: [   0/19]  Time: 0.847 (0.847)  Loss:  1.1074 (1.1074)  Acc@1: 74.6094 (74.6094)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:39 - INFO - train -   Test: [  19/19]  Time: 0.019 (0.096)  Loss:  1.1572 (1.1095)  Acc@1: 73.5294 (72.8000)  Acc@5: 100.0000 (99.9700)
05/08/2023 22:28:39 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:28:39 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:28:39 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:28:39 - INFO - train -   ------------- Evaluting stitch config 8/10 -------------
05/08/2023 22:28:40 - INFO - train -   Test: [   0/19]  Time: 0.817 (0.817)  Loss:  1.1445 (1.1445)  Acc@1: 70.5078 (70.5078)  Acc@5: 100.0000 (100.0000)
05/08/2023 22:28:41 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:28:41 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:28:41 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:28:41 - INFO - train -   Test: [  19/19]  Time: 0.018 (0.096)  Loss:  1.1855 (1.1462)  Acc@1: 69.8529 (69.9300)  Acc@5: 100.0000 (99.9900)
05/08/2023 22:28:41 - INFO - train -   ------------- Evaluting stitch config 9/10 -------------
05/08/2023 22:28:42 - INFO - train -   Test: [   0/19]  Time: 0.789 (0.789)  Loss:  1.2715 (1.2715)  Acc@1: 70.7031 (70.7031)  Acc@5: 99.8047 (99.8047)
05/08/2023 22:28:43 - INFO - train -   Test: [  19/19]  Time: 0.018 (0.090)  Loss:  1.3184 (1.2832)  Acc@1: 65.8088 (68.6200)  Acc@5: 100.0000 (99.9900)
